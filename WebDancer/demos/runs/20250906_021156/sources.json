[
  {
    "title": "arXiv [2508.05748] WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent",
    "url": "https://arxiv.org/abs/2508.05748",
    "snippet": "5 days ago - View a PDF of the paper titled WebWatcher : Breaking New Frontier of Vision-Language Deep Research Agent , by Xinyu Geng and 13 other authors ... arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.",
    "content": "Computer Science > Information Retrieval\n[Submitted on 7 Aug 2025 (\n[v1](https://arxiv.org/abs/2508.05748v1)), last revised 1 Sep 2025 (this version, v3)]Title:WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent\n[View PDF](/pdf/2508.05748)\nAbstract:Web agents such as Deep Research have demonstrated superhuman cognitive abilities, capable of solving highly challenging information-seeking problems. However, most research remains primarily text-centric, overlooking visual information in the real world. This makes multimodal Deep Research highly challenging, as such agents require much stronger reasoning abilities in perception, logic, knowledge, and the use of more sophisticated tools compared to text-based agents. To address this limitation, we introduce WebWatcher, a multi-modal Agent for Deep Research equipped with enhanced visual-language reasoning capabilities. It leverages high-quality synthetic multimodal trajectories for efficient cold start training, utilizes various tools for deep reasoning, and further enhances generalization through reinforcement learning. To better evaluate the capabilities of multimodal agents, we propose BrowseComp-VL, a benchmark with BrowseComp-style that requires complex information retrieval involving both visual and textual information. Experimental results show that WebWatcher significantly outperforms proprietary baseline, RAG workflow and open-source agents in four challenging VQA benchmarks, which paves the way for solving complex multimodal information-seeking tasks.\nSubmission history\nFrom: Xinyu Geng [[view email](/show-email/3ec4256f/2508.05748)]\n[[v1]](/abs/2508.05748v1)Thu, 7 Aug 2025 18:03:50 UTC (2,822 KB)\n[[v2]](/abs/2508.05748v2)Mon, 11 Aug 2025 15:09:49 UTC (2,491 KB)\n[v3] Mon, 1 Sep 2025 03:21:53 UTC (2,485 KB)\nReferences & Citations\nexport BibTeX citation\nLoading...\nBibliographic and Citation Tools\nBibliographic Explorer (\n[What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))\nConnected Papers (\n[What is Connected Papers?](https://www.connectedpapers.com/about))\nLitmaps (\n[What is Litmaps?](https://www.litmaps.co/))\nscite Smart Citations (\n[What are Smart Citations?](https://www.scite.ai/))Code, Data and Media Associated with this Article\nalphaXiv (\n[What is alphaXiv?](https://alphaxiv.org/))\nCatalyzeX Code Finder for Papers (\n[What is CatalyzeX?](https://www.catalyzex.com))\nDagsHub (\n[What is DagsHub?](https://dagshub.com/))\nGotit.pub (\n[What is GotitPub?](http://gotit.pub/faq))\nHugging Face (\n[What is Huggingface?](https://huggingface.co/huggingface))\nPapers with Code (\n[What is Papers with Code?](https://paperswithcode.com/))\nScienceCast (\n[What is ScienceCast?](https://sciencecast.org/welcome))Demos\nRecommenders and Search Tools\nInfluence Flower (\n[What are Influence Flowers?](https://influencemap.cmlab.dev/))\nCORE Recommender (\n[What is CORE?](https://core.ac.uk/services/recommender))arXivLabs: experimental projects with communit",
    "extraction_method": "direct",
    "timestamp": "2025-09-06T02:11:57.707501",
    "artifact": "1757139117_b8dabe89fa8d30ab_arxiv.org_abs_2508.05748.html"
  },
  {
    "title": "Hugging Face Paper page - WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent",
    "url": "https://huggingface.co/papers/2508.05748",
    "snippet": "WebWatcher, a multimodal agent with enhanced visual-language reasoning , outperforms existing agents in complex visual and textual information retrieval tasks using synthetic trajectories and reinforcement learning. ... Web agents such as Deep Research have demonstrated superhuman cognitive ...",
    "content": "[Papers](/papers)\nWebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent\nAbstract\nWebWatcher, a multimodal agent with enhanced visual-language reasoning, outperforms existing agents in complex visual and textual information retrieval tasks using synthetic trajectories and reinforcement learning.\nWeb agents such as Deep Research have demonstrated superhuman cognitive\nabilities, capable of solving highly challenging information-seeking problems.\nHowever, most research remains primarily text-centric, overlooking visual\ninformation in the real world. This makes [multimodal](/papers?q=multimodal) Deep Research highly\nchallenging, as such agents require much stronger reasoning abilities in\nperception, logic, knowledge, and the use of more sophisticated tools compared\nto text-based agents. To address this limitation, we introduce WebWatcher, a\nmulti-modal Agent for Deep Research equipped with enhanced visual-language\nreasoning capabilities. It leverages high-quality synthetic [multimodal](/papers?q=multimodal)\ntrajectories for efficient cold start training, utilizes various tools for deep\nreasoning, and further enhances generalization through [reinforcement learning](/papers?q=reinforcement%20learning).\nTo better evaluate the capabilities of [multimodal](/papers?q=multimodal) agents, we propose\n[BrowseComp-VL](/papers?q=BrowseComp-VL), a benchmark with BrowseComp-style that requires complex\ninformation retrieval involving both visual and textual information.\nExperimental results show that WebWatcher significantly outperforms proprietary\nbaseline, RAG workflow and open-source agents in four challenging VQA\nbenchmarks, which paves the way for solving complex [multimodal](/papers?q=multimodal)\ninformation-seeking tasks.\nCommunity\nüéâ In this paper, we introduce WebWatcher, a multimodal agent for deep research that possesses enhanced visual-language reasoning capabilities. Our work presents a unified framework that combines complex vision-language reasoning with multi-tool interaction.\nKey features of our approach include:\n- BrowseComp-VL Benchmark: We propose a new benchmark, BrowseComp-VL, to evaluate the capabilities of multimodal agents. This challenging dataset is designed for in-depth multimodal reasoning and strategic planning, mirroring the complexity of BrowseComp but extending it into the visual domain. It emphasizes tasks that require both visual perception and advanced information-gathering abilities.\n- Automated Trajectory Generation: To provide robust tool-use capabilities, we developed an automated pipeline to generate high-quality, multi-step reasoning trajectories. These trajectories, which are grounded in actual tool-use behavior and reflect procedural decision-making, are used for efficient cold-start training and further optimization via reinforcement learning. The agent is equipped with several tools, including Web Image Search, Web Text Search, Webpage Visit, Code Interpreter, and an internal OCR tool.\n- Superior Perfor",
    "extraction_method": "direct",
    "timestamp": "2025-09-06T02:11:58.223223",
    "artifact": "1757139118_689806273c87760e_huggingface.co_papers_2508.05748.html"
  },
  {
    "title": "Rohan-paul üá®üá≥ Alibaba's Tongyi Lab Open-Sources WebWatcher: A Breakthrough in Vision-Language AI Agents",
    "url": "https://www.rohan-paul.com/p/alibabas-tongyi-lab-open-sources",
    "snippet": "2 days ago - WebWatcher integrates 5 tools, web text search, web image search, webpage visit for focused reading, code execution, and OCR . It learns from synthetic tasks built by crawling real sites, masking entities so the model must infer them, then pairing questions with real images, with strict filtering.",
    "content": "üá®üá≥ Alibaba's Tongyi Lab Open-Sources WebWatcher: A Breakthrough in Vision-Language AI Agents\nAlibaba open-sources WebWatcher, Tencent launches 3D world model HunyuanWorld-Voyager, plus an opinion on why LLMs hit limits between deep and shallow reasoning.\nRead time: 10 min\nüìö Browse [past editions here](https://rohanpaul.substack.com/s/daily-ai-newsletter/archive?sort=new).\n([ ](https://x.com/rohanpaul_ai)[I publish this newletter](https://x.com/rohanpaul_ai) daily. Noise-free, actionable, applied-AI developments only).\n‚ö°In today‚Äôs Edition (3-Sept-2025):\nüá®üá≥ Alibaba's Tongyi Lab Open-Sources WebWatcher: A Breakthrough in Vision-Language AI Agents\nüèÜ Tencent's Hunyuan AI team has unveiled HunyuanWorld-Voyager, the world's first open-source ultra-long-range world model featuring native 3D reconstruction.\nüß† OPINION: Deep vs. Shallow: Why today‚Äôs LLMs hit a wall\nüá®üá≥ Alibaba's Tongyi Lab Open-Sources WebWatcher: A Breakthrough in Vision-Language AI Agents\n[Alibaba's Tongyi Lab announced the open-sourcing of WebWatcher](https://github.com/Alibaba-NLP/WebAgent/tree/main/WebWatcher), a cutting-edge vision-language deep research agent developed by their NLP team. Available in 7B and 32B parameter scales, WebWatcher sets new state-of-the-art (SOTA) performance on challenging visual question-answering (VQA) benchmarks, outperforming models like GPT-4o, Gemini-1.5-Flash, Qwen2.5-VL-72B, and Claude-3.7.\nKey highlights from the benchmarks (based on WebWatcher-32B):\nHumanity's Last Exam (HLE)-VL: 13.6% pass rate, surpassing GPT-4o's 9.8%.\nBrowseComp-VL (Average): 27.0% pass rate, nearly double GPT-4o's 13.4%.\nLiveVQA: 58.7% accuracy, leading over Gemini-1.5-Flash's 41.3%.\nMMSearch: 55.3% pass rate, ahead of Gemini-1.5-Flash's 43.9%.\nWhat sets WebWatcher apart is its unified framework for multimodal reasoning, combining visual and textual analysis with multi-tool interactions (e.g., web search, image processing, OCR, and code interpretation). Unlike template-based systems, it uses an automated trajectory generation pipeline for high-quality, multi-step reasoning.\n[And this is the paper for WebWatcher.](https://arxiv.org/pdf/2508.05748)\nWebWatcher integrates 5 tools, web text search, web image search, webpage visit for focused reading, code execution, and OCR.\nIt learns from synthetic tasks built by crawling real sites, masking entities so the model must infer them, then pairing questions with real images, with strict filtering. The team also builds BrowseComp-VL, where Level 1 names entities and Level 2 hides them, so the agent must plan multi hop steps.\nTraining starts with supervised fine tuning on ReAct traces, think then act then observe, to teach tool use. Then reinforcement learning with group relative policy optimization ranks multiple tool sequences and pushes the policy toward higher scoring ones, which works only after an SFT cold start.\nThe big idea is simple, combine seeing, reading, searching, coding, and clicking under one planner, and train on data and",
    "extraction_method": "direct",
    "timestamp": "2025-09-06T02:11:58.711225",
    "artifact": "1757139118_89d8bd3434cce2f0_rohan-paul.com_p_alibabas-tongyi-lab-open-sources.html"
  },
  {
    "title": "AI Models WebWatcher: Breaking New Frontiers of Vision-Language Deep Research Agent | AI Research Paper Details",
    "url": "https://www.aimodels.fyi/papers/arxiv/webwatcher-breaking-new-frontiers-vision-language-deep",
    "snippet": "Demonstrated enhanced reasoning ... agents ¬∑ Introduced BrowseComp-VL, a novel benchmark for multimodal research ¬∑ The WebWatcher architecture represents a significant leap in multimodal AI research . By combining high-quality synthetic multimodal trajectories with reinforcement learning, the system develops sophisticated ...",
    "content": "0\n0\nCan AI agents truly conquer research if they're blind to the visual world?\nWebWatcher: Breaking New Frontiers of Vision-Language Deep Research Agent\n[Xinyu Geng](/author-profile/xinyu-geng-c02a30f9-3970-4e7f-bd41-02644b87d6a3),\n[Zhen Zhang](/author-profile/zhen-zhang-d60e4f27-a831-407f-b3d1-1b53e927b867),\n[Xinyu Wang](/author-profile/xinyu-wang-f63b7889-9b88-4e24-a550-797774e8cbab),\n[Qiuchen Wang](/author-profile/qiuchen-wang-56798426-1aa3-4395-b894-6c26cc66a27a),\n[Ruixue Ding](/author-profile/ruixue-ding-2fbed346-7839-4195-bf59-a08355c4b669),\n[Chenxi Wang](/author-profile/chenxi-wang-ea0860cd-8d47-4622-b638-1eb9fc2bfc2e),\n[Jialong Wu](/author-profile/jialong-wu-83a2e5a5-f8b8-4550-bf28-7edfa63bdff9)and 3 more...\nGet notified when new papers like this one come out!\nOverview\n- WebWatcher introduces a multi-modal research agent with advanced visual-language reasoning\n- Addresses limitations of text-centric web research agents\n- Leverages synthetic multimodal trajectories for training\n- Proposes a new benchmark called BrowseComp-VL for evaluating complex information retrieval\n- Demonstrates superior performance across visual question answering (VQA) benchmarks\nPlain English Explanation\nImagine a digital detective that doesn't just read text, but can actually see and understand images while searching the web. That's essentially what [WebWatcher](https://aimodels.fyi/papers/arxiv/webwatcher-breaking-new-frontiers-vision-language-deep) aims to be.\nTraditional web research tools are like reading-glasses-only investigators - they can process text, but miss crucial visual context. WebWatcher is more like a fully equipped detective with enhanced perception. It can look at images, understand text, and connect the dots in ways previous systems couldn't.\nThink of it like teaching an AI to not just read a restaurant review, but actually look at the menu pictures, read the text, and make nuanced recommendations. The system uses synthetic training data to rapidly learn complex information-seeking strategies, almost like training a rookie detective with simulated case scenarios.\nKey Findings\n- Outperformed proprietary baselines in four challenging VQA benchmarks\n- Successfully integrated visual and textual information retrieval\n- Demonstrated enhanced reasoning capabilities beyond text-based agents\n- Introduced\n[BrowseComp-VL](https://aimodels.fyi/papers/arxiv/visualwebarena-evaluating-multimodal-agents-realistic-visual-web), a novel benchmark for multimodal research\nTechnical Explanation\nThe WebWatcher architecture represents a significant leap in [multimodal AI research](https://aimodels.fyi/papers/arxiv/agent-x-evaluating-deep-multimodal-reasoning-vision). By combining high-quality synthetic multimodal trajectories with reinforcement learning, the system develops sophisticated reasoning capabilities.\nKey technical innovations include:\n- Advanced visual-language processing\n- Multi-tool reasoning framework\n- Reinforcement learning for generalization\n- Synthet",
    "extraction_method": "direct",
    "timestamp": "2025-09-06T02:11:59.148870",
    "artifact": "1757139119_d52c003bd55508fe_aimodels.fyi_papers_arxiv_webwatcher-breaking-new-frontiers-vision-language-deep.html"
  },
  {
    "title": "YouTube WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent - YouTube",
    "url": "https://www.youtube.com/watch?v=YcKFqqmfE5Y",
    "snippet": "WebWatcher : Breaking New Frontier of Vision-Language Deep Research AgentAbstract: Web agents such as Deep Research have demonstrated superhuman cognitive abi...",
    "content": "About\nPress\nCopyright\nContact us\nCreators\nAdvertise\nDevelopers\nTerms\nPrivacy\nPolicy & Safety\nHow YouTube works\nTest new features\nNFL Sunday Ticket\n¬© 2025 Google LLC",
    "extraction_method": "direct",
    "timestamp": "2025-09-06T02:11:59.949731",
    "artifact": "1757139119_98e784f0c3e4905d_youtube.com_watch.html"
  }
]