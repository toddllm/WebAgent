[
  {
    "title": "",
    "url": "https://arxiv.org/pdf/2508.05748",
    "snippet": "Aug 11, 2025 Â· To better evaluate the capabilities of multimodal agents, we propose BrowseComp-VL, a benchmark â€¦",
    "content": "%PDF-1.5\n%ï¿½ï¿½ï¿½ï¿½\n1 0 obj\n<< /Metadata 3 0 R /Names 4 0 R /OpenAction 5 0 R /PageMode /UseOutlines /Pages 6 0 R /Type /Catalog >>\nendobj\n2 0 obj\n<< /Author (Xinyu Geng; Peng Xia; Zhen Zhang; Xinyu Wang; Qiuchen Wang; Ruixue Ding; Chenxi Wang; Jialong Wu; Yida Zhao; Kuan Li; Yong Jiang; Pengjun Xie; Fei Huang; Jingren Zhou) /Creator (arXiv GenPDF \\(tex2pdf:\\)) /DOI (https://doi.org/10.48550/arXiv.2508.05748) /License (http://arxiv.org/licenses/nonexclusive-distrib/1.0/) /PTEX.Fullbanner (This is pdfTeX, Version 3.141592653-2.6-1.40.25 \\(TeX Live 2023\\) kpathsea version 6.3.5) /Producer (pikepdf 8.15.1) /Title (WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent) /Trapped /False /arXivID (https://arxiv.org/abs/2508.05748v3) >>\nendobj\n3 0 obj\n<< /Subtype /XML /Type /Metadata /Length 1921 >>\nstream\nWebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent\nXinyu Geng\nPeng Xia\nZhen Zhang\nXinyu Wang\nQiuchen Wang\nRuixue Ding\nChenxi Wang\nJialong Wu\nYida Zhao\nKuan Li\nYong Jiang\nPengjun Xie\nFei Huang\nJingren Zhou\nhttp://arxiv.org/licenses/nonexclusive-distrib/1.0/\ncs.IR\nendstream\nendobj\n4 0 obj\n<< /Dests 7 0 R >>\nendobj\n5 0 obj\n<< /D [ 8 0 R /Fit ] /S /GoTo >>\nendobj\n6 0 obj\n<< /Count 27 /Kids [ 9 0 R 10 0 R 11 0 R 12 0 R 13 0 R ] /Type /Pages >>\nendobj\n7 0 obj\n<< /Kids [ 14 0 R 15 0 R 16 0 R 17 0 R 18 0 R ] /Limits [ (Doc-Start) (table.caption.20) ] >>\nendobj\n8 0 obj\n<< /Annots [ 19 0 R 20 0 R 21 0 R ] /Contents [ 22 0 R 23 0 R 24 0 R 25 0 R ] /MediaBox [ 0 0 595.276 841.89 ] /Parent 9 0 R /Resources 26 0 R /Type /Page >>\nendobj\n9 0 obj\n<< /Count 6 /Kids [ 8 0 R 27 0 R 28 0 R 29 0 R 30 0 R 31 0 R ] /Parent 6 0 R /Type /Pages >>\nendobj\n10 0 obj\n<< /Count 6 /Kids [ 32 0 R 33 0 R 34 0 R 35 0 R 36 0 R 37 0 R ] /Parent 6 0 R /Type /Pages >>\nendobj\n11 0 obj\n<< /Count 6 /Kids [ 38 0 R 39 0 R 40 0 R 41 0 R 42 0 R 43 0 R ] /Parent 6 0 R /Type /Pages >>\nendobj\n12 0 obj\n<< /Count 6 /Kids [ 44 0 R 45 0 R 46 0 R 47 0 R 48 0 R 49 0 R ] /Parent 6 0 R /Type /Pages >>\nendobj\n13 0 obj\n<< /Count 3 /Kids [ 50 0 R 51 0 R 52 0 R ] /Parent 6 0 R /Type /Pages >>\nendobj\n14 0 obj\n<< /Kids [ 53 0 R 54 0 R 55 0 R 56 0 R 57 0 R 58 0 R ] /Limits [ (Doc-Start) (cite.gu2025toward) ] >>\nendobj\n15 0 obj\n<< /Kids [ 59 0 R 60 0 R 61 0 R 62 0 R 63 0 R 64 0 R ] /Limits [ (cite.guo2025deepseek) (cite.zhang2025open3dvqa) ] >>\nendobj\n16 0 obj\n<< /Kids [ 65 0 R 66 0 R 67 0 R 68 0 R 69 0 R 70 0 R ] /Limits [ (cite.zhao2025pyvision) (page.27) ] >>\nendobj\n17 0 obj\n<< /Kids [ 71 0 R 72 0 R 73 0 R 74 0 R 75 0 R 76 0 R ] /Limits [ (page.3) (subsection.D.2) ] >>\nendobj\n18 0 obj\n<< /Kids [ 77 0 R 78 0 R ] /Limits [ (subsection.D.3) (table.caption.20) ] >>\nendobj\n19 0 obj\n<< /A << /D (Hfootnote.1) /S /GoTo >> /Border [ 0 0 0 ] /C [ 1 0 0 ] /H /I /Rect [ 261.42 712.836 263.413 726.065 ] /Subtype /Link /Type /Annot >>\nendobj\n20 0 obj\n<< /A << /S /URI /Type /Action /URI (https://github.com/Alibaba-NLP/WebAgent) >> /Border [ 0 0 0 ] /C [ 0 1 1 ] /H /I /Rect [ 205.742 648.212 41",
    "extraction_method": "direct",
    "timestamp": "2025-09-06T01:50:48.551028",
    "artifact": "1757137848_1c703e446e7bedeb_arxiv.org_pdf_2508.05748.html"
  },
  {
    "title": "GitHub - Alibaba-NLP/WebAgent: WebAgent for Information Seekinâ€¦",
    "url": "https://github.com/Alibaba-NLP/WebAgent",
    "snippet": "Jan 14, 2025 Â· We conduct extensive experiments across multiple benchmarks to evaluate the effectiveness of â€¦",
    "content": "ðŸ¤— [HuggingFace WebAgent](https://huggingface.co/collections/Alibaba-NLP/webagent-6878a9947443234bc6cbf9f4) ï½œ\n[ ](/Alibaba-NLP/WebAgent/blob/main/assets/tongyi.png)[ModelScope WebAgent](https://modelscope.cn/collections/WebAgent-3f21405837f646) ï½œ\nYou can check the paper of\n[WebDancer]and[WebWalker]and[WebSailor]and[WebShaper]and[WebWatcher].\nðŸ’¥ ðŸ’¥ ðŸ’¥ Stay tuned for more updates! We are working on building native agentic model based on the Browser and more open-domain environments!\n[WebWatcher](/Alibaba-NLP/WebAgent/blob/main/WebWatcher)(Preprint 2025) - WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent[WebShaper](/Alibaba-NLP/WebAgent/blob/main/WebShaper)(Preprint 2025) - WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization[WebSailor](/Alibaba-NLP/WebAgent/blob/main/WebSailor)(Preprint 2025) - WebSailor: Navigating Super-human Reasoning for Web Agent[WebDancer](/Alibaba-NLP/WebAgent/blob/main/WebDancer)(Preprint 2025) - WebDancer: Towards Autonomous Information Seeking Agency[WebWalker](/Alibaba-NLP/WebAgent/blob/main/WebWalker)(ACL 2025) - WebWalker: Benchmarking LLMs in Web Traversal\n2025.08.28\nðŸ”¥ðŸ”¥ðŸ”¥[WebShaper-32B](https://huggingface.co/Alibaba-NLP/WebShaper-32B)is released.2025.08.26\nðŸ”¥ðŸ”¥ðŸ”¥[WebSailor-32B](https://huggingface.co/Alibaba-NLP/WebSailor-32B)and[WebSailor-7B](https://huggingface.co/Alibaba-NLP/WebSailor-7B)is released.2025.08.07\nðŸ”¥ðŸ”¥ðŸ”¥We release WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent.2025.07.22\nðŸ”¥ðŸ”¥ðŸ”¥We release WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization.2025.07.11\nðŸ”¥ðŸ”¥ðŸ”¥WebSailor-3B is[released](https://huggingface.co/Alibaba-NLP/WebSailor-3B). You can deploy it with one click using[Alibaba Cloud's FunctionAI](https://functionai.console.aliyun.com/template-detail?template=Alibaba-NLP-WebSailor-3B)in ten minutes!2025.07.03\nðŸ”¥ðŸ”¥ðŸ”¥We release WebSailor, an agentic search model specialized in performing extremely complex information seeking tasks, achieving open-source SOTA on some of the most difficult browsing benchmarks. WebSailor topped the HuggingFace[daily papers](https://huggingface.co/papers/2507.02592).2025.06.23\nðŸ”¥ðŸ”¥ðŸ”¥The model, interactive demo, and some of the data of WebDancer have been open-sourced. You're welcome to try them out! You can deploy it with one click using[Alibaba Cloud's FunctionAI](https://functionai.console.aliyun.com/template-detail?template=Alibaba-NLP-WebDancer-32B)in ten minutes!2025.05.29\nðŸ”¥ðŸ”¥ðŸ”¥We release WebDancer, a native agentic search model towards autonomous information seeking agency and Deep Research-like model.2025.05.15\nWebWalker is accepted by ACL 2025 main conference.2025.01.14\nWe release WebWalker, a benchmark for LLMs in web traversal and a multi-agent framework for information seeking.\n| Model | Release Date | Max Length | Tool List |\n|---|---|---|---|\n|\n[WebSailor-3B](https://huggingface.co/Alibaba-NLP/WebSailor-3B)[WebSailor-7B](https://huggingface.co/Alibaba-NLP/WebSailor-7B)[WebSailor-32B](ht",
    "extraction_method": "direct",
    "timestamp": "2025-09-06T01:50:49.189338",
    "artifact": "1757137849_cdd37f9543f01960_github.com_Alibaba-NLP_WebAgent.html"
  },
  {
    "title": "WebWatcher Research Agent | Alibaba-NLP/WebAgent | DeepWiki",
    "url": "https://deepwiki.com/Alibaba-NLP/WebAgent/5.1-usage-examples",
    "snippet": "Aug 22, 2025 Â· This document covers WebWatcher, the vision-language deep research agent component of the â€¦",
    "content": "Loading...",
    "extraction_method": "direct",
    "timestamp": "2025-09-06T01:50:49.592045",
    "artifact": "1757137849_69c0555d24043026_deepwiki.com_Alibaba-NLP_WebAgent_5.1-usage-examples.html"
  },
  {
    "title": "WebWatcher: How Alibabaâ€™s New AI Agent Finally Teaches Computâ€¦",
    "url": "https://towardsdev.com/webwatcher-how-alibabas-new-ai-agent-finally-teaches-computers-to-see-and-think-like-a-researcher-83cbec57ee97",
    "snippet": "Discover WebWatcher, a new multimodal AI agent from Alibaba that breaks the text-only barrier. Learn how it uses vision â€¦",
    "content": "Member-only story\nWebWatcher: How Alibabaâ€™s New AI Agent Finally Teaches Computers to See and Think Like a Researcher\nDiscover WebWatcher, a new multimodal AI agent from Alibaba that breaks the text-only barrier. Learn how it uses vision-language reasoning and a sophisticated toolset to perform deep research, outperforming models like GPT-4 on complex, real-world information-seeking tasks.\nWe stand at a remarkable moment in technological history. AI assistants, powered by Large Language Models (LLMs), are weaving themselves into the fabric of our digital lives. They write code, draft emails, and summarize dense documents with astonishing fluency. Weâ€™ve given them access to the worldâ€™s text, and in return, theyâ€™ve given us superhuman abilities in processing it.\nBut for all their power, these brilliant minds have a profound, almost tragic, disability: they are blind.\nThey operate in a world devoid of images, charts, diagrams, and the rich visual context that makes up over 80% of the information on the internet. Ask an AI to analyze a sales report, and it will read the text. But ask it to interpret the accompanying sales trend chart, and it hits a wall. This is the Achillesâ€™ heel of modern AI research â€” a deep, cognitive gap between its linguistic prowess and its perceptual understanding of the world.\nThis isnâ€™t just about recognizing cats in photos. Itâ€™s about the fundamental nature of knowledge. A scientific paper is a blend of text and diagrams. A financial analysis combines numbersâ€¦",
    "extraction_method": "direct",
    "timestamp": "2025-09-06T01:50:50.489071",
    "artifact": "1757137850_c1513c1294678382_towardsdev.com_webwatcher-how-alibabas-new-ai-agent-finally-teaches-computers-to-see-and-think-like-.html"
  },
  {
    "title": "Alibaba Launches WebWatcher, a Multimodal Deep Research Intelliâ€¦",
    "url": "https://news.aibase.com/news/20567",
    "snippet": "Its core goal is to enable multimodal agents to have flexible reasoning and multi-tool collaboration capabilities in high â€¦",
    "content": "The Alibaba Natural Language Processing team announced the release of WebWatcher, an open-source multimodal deep research intelligent agent designed to overcome the limitations of existing closed-source systems and open-source agents in the field of multimodal deep research. WebWatcher integrates various tools such as web browsing, image search, code interpreter, and internal OCR, enabling it to handle complex multimodal tasks like human researchers, demonstrating strong visual understanding, logical reasoning, knowledge retrieval, tool scheduling, and self-verification capabilities.\nThe development team of WebWatcher pointed out that although existing closed-source systems such as OpenAI's DeepResearch perform well in text-based deep research, they are mostly limited to pure text environments and struggle to handle complex images, charts, and mixed content in the real world. Existing open-source agents also face two major bottlenecks: one type focuses on text retrieval agents, which can integrate information but cannot process images; the other type is visual agents, which can recognize images but lack cross-modal reasoning and multi-tool collaboration capabilities. WebWatcher was specifically designed to address these bottlenecks.\nWebWatcher's technical solution covers the entire workflow from data construction to training optimization. Its core goal is to enable multimodal agents to have flexible reasoning and multi-tool collaboration capabilities in high-difficulty multimodal deep research tasks. To achieve this, the research team designed an automated multimodal data generation process, collecting cross-modal knowledge chains through random walks and introducing information blurring technology to increase the uncertainty and complexity of tasks. All complex problem samples are expanded into multimodal versions through a QA-to-VQA conversion module, further enhancing the model's cross-modal understanding capabilities.\nIn terms of building high-quality reasoning trajectories and post-training, WebWatcher adopted an Action-Observation-driven trajectory generation method. By collecting real-world multi-tool interaction trajectories and performing supervised fine-tuning (SFT), the model quickly grasps the basic patterns of multimodal ReAct-style reasoning and tool calling in the early stages of training. Subsequently, the model enters the reinforcement learning phase, further improving the decision-making capabilities of the multimodal agent in complex environments through GRPO.\nTo comprehensively verify the capabilities of WebWatcher, the research team proposed BrowseComp-VL, an extension version of BrowseComp for vision-language tasks, aiming to approach the difficulty of cross-modal research tasks performed by human experts. In multi-round rigorous evaluations, WebWatcher significantly outperformed current mainstream open-source and closed-source multimodal large models in complex reasoning, information retrieval, knowledge integration, and in",
    "extraction_method": "direct",
    "timestamp": "2025-09-06T01:50:51.866506",
    "artifact": "1757137851_47b5ea2be60f5696_news.aibase.com_news_20567.html"
  }
]