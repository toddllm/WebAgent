<!DOCTYPE html><html  lang="en" data-capo=""><head><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Alibaba Launches WebWatcher, a Multimodal Deep Research Intelligent Agent</title>
<link rel="stylesheet" href="https://csstools.aibase.com/iconfont/iconfont.css">
<link rel="stylesheet" href="/_nuxt/entry.CvZSfuUW.css" crossorigin>
<link rel="stylesheet" href="/_nuxt/NewHeader.C3q_dfEw.css" crossorigin>
<link rel="stylesheet" href="/_nuxt/ArticleRelate.Du3LtZTt.css" crossorigin>
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/B743cgoS.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BKHFVBTU.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DCwZfihu.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/BqA000Jy.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/YLQKUWD1.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/B7vminhQ.js">
<link rel="modulepreload" as="script" crossorigin href="/_nuxt/DkLnu7o1.js">
<link rel="prefetch" as="image" type="image/png" href="/_nuxt/title_pc.DaZ8du0S.png">
<link rel="prefetch" as="image" type="image/png" href="/_nuxt/title_mb.CJ8YhvTp.png">
<link rel="prefetch" as="image" type="image/jpeg" href="/_nuxt/bigBg.LP-pnBku.jpg">
<link rel="prefetch" as="image" type="image/jpeg" href="/_nuxt/Mobile1000.wFrcSM6R.jpg">
<link rel="prefetch" as="image" type="image/jpeg" href="/_nuxt/detail3200.XNAzJjN3.jpg">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/A7GRMxZh.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/Cu0BndaH.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/BG1DUPlN.js">
<link rel="prefetch" as="script" crossorigin href="/_nuxt/BiQqFiFA.js">
<link rel="prefetch" as="image" type="image/png" href="/_nuxt/userlogo.q1jFctRw.png">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<link rel="icon" href="/favicon.ico" sizes="any">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<script>var _hmt = _hmt || [];
          (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?20171d7bc84390037a47470dd0e59957";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
          })();</script>
<script>
(function() {
  var hm = document.createElement("script");
  hm.src = "https://www.googletagmanager.com/gtag/js?id=GT-5RMLLX6V";
  hm.async = true;
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  window.dataLayer = window.dataLayer || [];
})();</script>
<script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-VYDTW9D9YP');</script>
<script>(function(c,l,a,r,i,t,y){
         c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
         t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
         y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
     })(window, document, "clarity", "script", "kq7u45ainz");</script>
<link rel="alternate" hreflang="en" href="https://news.aibase.com/news/20567">
<link rel="alternate" hreflang="zh-CN" href="https://news.aibase.com/zh/news/20567">
<link rel="alternate" hreflang="zh-TW" href="https://news.aibase.com/tw/news/20567">
<link rel="alternate" hreflang="ja" href="https://news.aibase.com/ja/news/20567">
<link rel="alternate" hreflang="x-default" href="https://news.aibase.com/news/20567">
<link rel="canonical" href="https://news.aibase.com/news/20567">
<meta name="description" content="The Alibaba Natural Language Processing team announced the launch of WebWatcher, an open-source multimodal deep research intelligent agent designed to break thr">
<script type="module" src="/_nuxt/B743cgoS.js" crossorigin></script>
<link rel="preload" as="fetch" fetchpriority="low" crossorigin="anonymous" href="/_nuxt/builds/meta/7aab3073-b06c-4b8b-a0b1-424d9dad5915.json"><script>"use strict";(()=>{const t=window,e=document.documentElement,c=["dark","light"],n=getStorageValue("localStorage","nuxt-color-mode")||"light";let i=n==="system"?u():n;const r=e.getAttribute("data-color-mode-forced");r&&(i=r),l(i),t["__NUXT_COLOR_MODE__"]={preference:n,value:i,getColorScheme:u,addColorScheme:l,removeColorScheme:d};function l(o){const s=""+o+"",a="";e.classList?e.classList.add(s):e.className+=" "+s,a&&e.setAttribute("data-"+a,o)}function d(o){const s=""+o+"",a="";e.classList?e.classList.remove(s):e.className=e.className.replace(new RegExp(s,"g"),""),a&&e.removeAttribute("data-"+a)}function f(o){return t.matchMedia("(prefers-color-scheme"+o+")")}function u(){if(t.matchMedia&&f("").media!=="not all"){for(const o of c)if(f(":"+o).matches)return o}return"light"}})();function getStorageValue(t,e){switch(t){case"localStorage":return window.localStorage.getItem(e);case"sessionStorage":return window.sessionStorage.getItem(e);case"cookie":return getCookie(e);default:return null}}function getCookie(t){const c=("; "+window.document.cookie).split("; "+t+"=");if(c.length===2)return c.pop()?.split(";").shift()}</script></head><body><div id="__nuxt"><!--[--><div class="nuxt-loading-indicator" style="position:fixed;top:0;right:0;left:0;pointer-events:none;width:auto;height:3px;opacity:0;background:repeating-linear-gradient(to right,#00dc82 0%,#34cdfe 50%,#0047e1 100%);background-size:Infinity% auto;transform:scaleX(0%);transform-origin:left;transition:transform 0.1s, height 0.4s, opacity 0.4s;z-index:999999;"></div><main class="min-h-[100%] w-auto relative"><div class="w-full md:min-w-[1400px] overflow-auto"><header class="header fixed" data-v-6dae6635 data-v-3d2290a7><!--[--><div class="nav-container commContainer" data-v-3d2290a7><div class="nav-left" data-v-3d2290a7><div class="logo-section" data-v-3d2290a7><!--[--><a href="https://www.aibase.com" class="logo-link" data-v-3d2290a7><img src="data:image/svg+xml,%3c?xml%20version=&#39;1.0&#39;%20encoding=&#39;UTF-8&#39;?%3e%3csvg%20id=&#39;_图层_1&#39;%20data-name=&#39;图层_1&#39;%20xmlns=&#39;http://www.w3.org/2000/svg&#39;%20version=&#39;1.1&#39;%20viewBox=&#39;0%200%2095%2024&#39;%3e%3c!--%20Generator:%20Adobe%20Illustrator%2029.2.1,%20SVG%20Export%20Plug-In%20.%20SVG%20Version:%202.1.0%20Build%20116)%20--%3e%3cdefs%3e%3cstyle%3e%20.st0%20{%20fill:%20%23061b40;%20}%20.st1%20{%20fill:%20%23306af1;%20}%20.st2%20{%20fill:%20%235ce5cf;%20}%20%3c/style%3e%3c/defs%3e%3cg%3e%3cpath%20class=&#39;st0&#39;%20d=&#39;M55,10.5h9v3h-9v9h12V7.5h-12v3ZM64,19.5h-6v-3h6v3Z&#39;/%3e%3cpolygon%20class=&#39;st0&#39;%20points=&#39;69%2016.5%2078%2016.5%2078%2019.5%2069%2019.5%2069%2022.5%2081%2022.5%2081%2013.5%2072%2013.5%2072%2010.5%2081%2010.5%2081%207.5%2069%207.5%2069%2016.5&#39;/%3e%3cpolygon%20class=&#39;st0&#39;%20points=&#39;95%2010.5%2095%207.5%2083%207.5%2083%2022.5%2095%2022.5%2095%2019.5%2086%2019.5%2086%2016.5%2095%2016.5%2095%2013.5%2086%2013.5%2086%2010.5%2095%2010.5&#39;/%3e%3cpath%20class=&#39;st0&#39;%20d=&#39;M40,1.5v21h11.6l1.4-1.4v-7.6h0c0,0-1.4-1.5-1.4-1.5l1.4-1.4V2.9l-1.4-1.4h-11.6ZM50,19.5h-7v-6h7v6ZM50,10.5h-7v-6h7v6Z&#39;/%3e%3c/g%3e%3cpath%20class=&#39;st1&#39;%20d=&#39;M23.1,24L14.7,4.8l-4.9,11.2h3.8l-1.8,4H3.3L12.1,0H2C.9,0,0,.9,0,2v20c0,1.1.9,2,2,2h21.1Z&#39;/%3e%3cpath%20class=&#39;st2&#39;%20d=&#39;M34,0h-16.8l10.6,24h6.2c1.1,0,2-.9,2-2V2C36,.9,35.1,0,34,0ZM32.5,20h-4V4h4v16Z&#39;/%3e%3c/svg%3e" alt="AIbase" class="logo-default" data-v-3d2290a7></a><!--]--></div><div class="desktop-menu" data-v-3d2290a7><nav class="nav-menu" data-v-3d2290a7><!--[--><div class="nav-menu-item" data-v-7144d3b0 data-v-3d2290a7><a href="//www.aibase.com" class="menu-link menu-trigger" data-v-7144d3b0>Home</a><!----></div><div class="nav-menu-item" data-v-7144d3b0 data-v-3d2290a7><a href="//news.aibase.com" class="menu-link menu-trigger" data-v-7144d3b0><span data-v-7144d3b0>AI NEWS</span><i class="iconfont icon-xia1 dropdown-icon" data-v-7144d3b0></i></a><!----></div><div class="nav-menu-item" data-v-7144d3b0 data-v-3d2290a7><a href="//app.aibase.com" class="menu-link menu-trigger" data-v-7144d3b0><span data-v-7144d3b0>AI Tools</span><i class="iconfont icon-xia1 dropdown-icon" data-v-7144d3b0></i></a><!----></div><div class="nav-menu-item" data-v-7144d3b0 data-v-3d2290a7><a href="//model.aibase.com" class="menu-link menu-trigger" data-v-7144d3b0><span data-v-7144d3b0>AI Models</span><i class="iconfont icon-xia1 dropdown-icon" data-v-7144d3b0></i></a><!----></div><div class="nav-menu-item" data-v-7144d3b0 data-v-3d2290a7><a href="//mcp.aibase.com" class="menu-link menu-trigger" data-v-7144d3b0><span data-v-7144d3b0>MCP</span><i class="iconfont icon-xia1 dropdown-icon" data-v-7144d3b0></i></a><!----></div><div class="nav-menu-item" data-v-7144d3b0 data-v-3d2290a7><a href class="menu-link menu-trigger" data-v-7144d3b0><span data-v-7144d3b0>AI Services</span><i class="iconfont icon-xia1 dropdown-icon" data-v-7144d3b0></i></a><!----></div><div class="nav-menu-item" data-v-7144d3b0 data-v-3d2290a7><a href class="menu-link menu-trigger menu-link-disabled" data-v-7144d3b0>Datasets</a><!----></div><div class="nav-menu-item" data-v-7144d3b0 data-v-3d2290a7><a href class="menu-link menu-trigger menu-link-disabled" data-v-7144d3b0>AI Compute</a><!----></div><div class="nav-menu-item" data-v-7144d3b0 data-v-3d2290a7><a href class="menu-link menu-trigger menu-link-disabled" data-v-7144d3b0>AI Tutorial</a><!----></div><!--]--></nav></div></div><div class="nav-right" data-v-3d2290a7><!--[--><div class="flex"><div class="md:hidden"><div class="mobile-search-container" data-v-996d21c7><button class="search-trigger-button" data-v-996d21c7><i class="iconfont icon-sousuo1 trigger-icon" data-v-996d21c7></i></button><!----></div></div><div class="hidden md:block"><div class="expandable-search-container" data-v-a83a61cd><div class="search-input-wrapper" data-v-a83a61cd><button class="search-button" data-v-a83a61cd><i class="iconfont icon-sousuo1 search-icon" data-v-a83a61cd></i></button><input type="text" placeholder="Search" class="search-input" value data-v-a83a61cd><!----></div></div></div></div><!--]--><div class="desktop-language" data-v-3d2290a7><!--[--><div class="relative mb-[32px] md:mb-[0]" data-v-80fd35d2><div class="flex items-center gap-[4px] text-[14px] leading-[22px] h-[24px] cursor-pointer mainColor" data-v-80fd35d2><i class="iconfont icon-yuyan md:text-[16px] text-[24px]" data-v-80fd35d2></i><div class="font-bold md:font-normal text-[18px] leading-[28px] md:text-[14px] md:leading-[22px]" data-v-80fd35d2>EN</div><i class="iconfont icon-xia md:text-[12px] text-[16px] tipColor" data-v-80fd35d2></i></div><!----><!----></div><!--]--></div><div class="desktop-user" data-v-3d2290a7><!--[--><span></span><!--]--></div><div class="mobile-menu-button" data-v-3d2290a7><div class="modal-wrapper" data-v-56e8e3c1 data-v-3d2290a7><div style="visibility:visible;" data-v-56e8e3c1><!--[--><!--[--><div class="mobile-trigger-default" data-v-3d2290a7><i class="iconfont icon-gengduo" data-v-3d2290a7></i></div><!--]--><!--]--></div><!----></div></div></div></div><!--]--></header><div class="md:pt-[56px] pt-[50px] md:min-w-[1400px] min-h-[calc(100vh-361px)]"><!--[--><div class="bigBgDetails md:bg-[#F7F8FA] bg-[#fff] md:pt-[24px] pt-[0px]"><div class="commContainer1090 flex gap-[16px] md:flex-row flex-col"><div class="flex-1"><article class="md:pb-[80px] pb-0 max-w-[728px] mx-auto"><div class="max-w-[728px] mx-auto flex flex-col"><div><div class="mdBorderMain mainRadius md:px-[24px] pb-[24px] bg-[#fff]"><div class="pt-[20px] pb-[30px]"><div class="inline-block"><div class="inline font250 text-[14px] leading-[22px]"><!--[--><!--[--><div class="inline-flex items-center gap-[8px] mr-[8px]"><a href="/" class="inline-block text-[#061B40]">AI NEWS</a><i class="iconfont icon-you text-[#C3C7CD] text-[12px]"></i></div><!--]--><!--[--><div class="inline-flex items-center gap-[8px] mr-[8px]"><a href="/news" class="inline-block text-[#061B40]">Latest AI News</a><i class="iconfont icon-you text-[#C3C7CD] text-[12px]"></i></div><!--]--><!--]--><div class="text-[#061B40] flex-1 inline leading-[22px]">Article Content</div></div></div></div><div class=""><h1 class="text-[24px] leading-[36px] font600 mainColor">Alibaba Launches WebWatcher, a Multimodal Deep Research Intelligent Agent</h1><div class="flex items-center gap-x-[40px] gap-y-[12px] text-[15px] leading-[25px] mt-[14px] flex-wrap mainColor"><div class="flex items-center"><div class="overflow-hidden flex-shrink-0 mr-[8px] w-[32px] h-[32px] rounded-full"><img alt="aibase" src="/_nuxt/userlogo.q1jFctRw.png" class="w-[32px] h-[32px] rounded-full"></div><div class="flex flex-col"><div class="flex flex-wrap items-center"><span class="">Published in Latest AI News</span></div></div></div><div class="flex gap-x-[40px]"><div class="flex items-center gap-[5px]"><i class="iconfont icon-rili"></i><span class="hidden md:inline">Time :</span><span>Aug 18, 2025</span></div><div class="flex items-center gap-[5px]"><i class="iconfont icon-meiripingjun"></i><span class="hidden md:inline">Read :</span><span><span>5</span><span>minute</span></span></div></div></div></div><div class="borderTop mt-[20px] mb-[19px]"></div><div class="articleContent"><div class="overflow-hidden space-y-[20px] text-[15px] leading-[25px] break-words mainColor post-content text-wrap" style="word-break:break-word;overflow-wrap:break-word;hyphens:auto;"><p>The Alibaba Natural Language Processing team announced the release of WebWatcher, an open-source multimodal deep research intelligent agent designed to overcome the limitations of existing closed-source systems and open-source agents in the field of multimodal deep research. WebWatcher integrates various tools such as web browsing, image search, code interpreter, and internal OCR, enabling it to handle complex multimodal tasks like human researchers, demonstrating strong visual understanding, logical reasoning, knowledge retrieval, tool scheduling, and self-verification capabilities.</p><p>The development team of WebWatcher pointed out that although existing closed-source systems such as OpenAI's DeepResearch perform well in text-based deep research, they are mostly limited to pure text environments and struggle to handle complex images, charts, and mixed content in the real world. Existing open-source agents also face two major bottlenecks: one type focuses on text retrieval agents, which can integrate information but cannot process images; the other type is visual agents, which can recognize images but lack cross-modal reasoning and multi-tool collaboration capabilities. WebWatcher was specifically designed to address these bottlenecks.</p><p>WebWatcher's technical solution covers the entire workflow from data construction to training optimization. Its core goal is to enable multimodal agents to have flexible reasoning and multi-tool collaboration capabilities in high-difficulty multimodal deep research tasks. To achieve this, the research team designed an automated multimodal data generation process, collecting cross-modal knowledge chains through random walks and introducing information blurring technology to increase the uncertainty and complexity of tasks. All complex problem samples are expanded into multimodal versions through a QA-to-VQA conversion module, further enhancing the model's cross-modal understanding capabilities.</p><p style="text-align:center"><img src="https://upload.chinaz.com/2025/0818/6389110600218927649884538.png" title="WeChat screenshot_20250818092634.png" alt="WeChat screenshot_20250818092634.png"/></p><p>In terms of building high-quality reasoning trajectories and post-training, WebWatcher adopted an Action-Observation-driven trajectory generation method. By collecting real-world multi-tool interaction trajectories and performing supervised fine-tuning (SFT), the model quickly grasps the basic patterns of multimodal ReAct-style reasoning and tool calling in the early stages of training. Subsequently, the model enters the reinforcement learning phase, further improving the decision-making capabilities of the multimodal agent in complex environments through GRPO.</p><p>To comprehensively verify the capabilities of WebWatcher, the research team proposed BrowseComp-VL, an extension version of BrowseComp for vision-language tasks, aiming to approach the difficulty of cross-modal research tasks performed by human experts. In multi-round rigorous evaluations, WebWatcher significantly outperformed current mainstream open-source and closed-source multimodal large models in complex reasoning, information retrieval, knowledge integration, and information optimization tasks.</p><p>Specifically, on the Humanity’s Last Exam (HLE-VL) benchmark, a multi-step complex reasoning benchmark, WebWatcher achieved a Pass@1 score of 13.6%, far surpassing representative models such as GPT-4o (9.8%), Gemini 2.5-flash (9.2%), and Qwen2.5-VL-72B (8.6%). In the MMSearch evaluation, which is more closely aligned with real-world multimodal search, WebWatcher achieved a Pass@1 score of 55.3%, significantly outperforming Gemini 2.5-flash (43.9%) and GPT-4o (24.1%). In the LiveVQA evaluation, WebWatcher achieved a Pass@1 score of 58.7%, leading other mainstream models. On the most comprehensive and challenging BrowseComp-VL benchmark, WebWatcher achieved an average score (Pass@1) of 27.0%, leading by more than double.</p><p>Repository: https://github.com/Alibaba-NLP/WebAgent</p></div></div></div></div></div></article></div><div class="md:w-[336px]"><div class="max-w-[728px] tracking-wider mb-[48px] mx-auto flex flex-col flex-1"><div class="borderTop mb-[48px] mt-[4px] md:hidden"></div><div class=""><div class="grid grid-cols-1 gap-[16px]"><!--[--><a href="https://news.aibase.com/news/19465" rel="noopener noreferrer" target="_blank" class="bg-white mainRadius overflow-hidden border border-solid border-[#EBEDF0] px-[16px] py-[12px] group/detail transition-all duration-200 cursor-pointer"><div class=""><div class="block group"><h2 class="mb-[8px] text-[16px] leading-[24px] font600 mainColor line-clamp-2">Open Source Revolution! Kyutai TTS Launches: Ultra-Low Latency Speech Synthesis, the New Era of AI Voice is Here!</h2><p class="text-[14px] leading-[22px] tipColor line-clamp-2">Recently, the French AI laboratory Kyutai announced the official open source of its new text-to-speech model, Kyutai TTS, providing global developers and researchers with a high-performance, low-latency speech synthesis solution. This breakthrough release not only promotes the development of open-source AI technology but also opens up new possibilities for multilingual voice interaction applications. AIbase provides an exclusive analysis of this technological highlight and its potential impact. Ultra-low latency, a new experience in real-time interaction. Kyutai TTS has become an industry standout with its exceptional performance.</p></div></div><div class="borderTop my-[12px]"></div><div class="flex justify-between items-center mainColor"><div class="inline-flex items-center mainColor gap-[4px]"><i class="iconfont icon-rili text-[14px]"></i><span>Jul 4, 2025</span></div><div class="inline-flex items-center mainColor gap-[4px]"><i class="iconfont icon-fangwenliang1 text-[14px]"></i><span>56.2k</span></div></div></a><a href="https://news.aibase.com/news/19339" rel="noopener noreferrer" target="_blank" class="bg-white mainRadius overflow-hidden border border-solid border-[#EBEDF0] px-[16px] py-[12px] group/detail transition-all duration-200 cursor-pointer"><div class=""><div class="block group"><h2 class="mb-[8px] text-[16px] leading-[24px] font600 mainColor line-clamp-2">Baidu Launches the WENXIN Large Model 4.5 Series Open Source, Sparking a New Wave in the Domestic Large Model Market!</h2><p class="text-[14px] leading-[22px] tipColor line-clamp-2">Recently, Baidu officially announced the open-source release of its WENXIN Large Model 4.5 series, launching a total of ten models, including mixed expert (MoE) models with 47B and 3B activated parameters, as well as dense models with 0.3B parameters. This open-source initiative not only fully publicizes the pre-trained weights but also provides inference code, marking a significant advancement for Baidu in the field of large models. These newly released models can be downloaded and deployed on platforms such as PaddlePaddle Starry Sky Community and Hugging Face. Additionally, Baidu Intelligent Cloud&#39;s Qianfan Large Model Platform also provides</p></div></div><div class="borderTop my-[12px]"></div><div class="flex justify-between items-center mainColor"><div class="inline-flex items-center mainColor gap-[4px]"><i class="iconfont icon-rili text-[14px]"></i><span>Jun 30, 2025</span></div><div class="inline-flex items-center mainColor gap-[4px]"><i class="iconfont icon-fangwenliang1 text-[14px]"></i><span>19.3k</span></div></div></a><a href="https://news.aibase.com/news/19259" rel="noopener noreferrer" target="_blank" class="bg-white mainRadius overflow-hidden border border-solid border-[#EBEDF0] px-[16px] py-[12px] group/detail transition-all duration-200 cursor-pointer"><div class=""><div class="block group"><h2 class="mb-[8px] text-[16px] leading-[24px] font600 mainColor line-clamp-2">Google Unveils a Major Move! Gemini CLI Open Source Release, Free AI Coding Assistant Challenges Cursor</h2><p class="text-[14px] leading-[22px] tipColor line-clamp-2">Google has officially launched Gemini CLI today, an open-source terminal AI agent tool, directly challenging commercial AI coding tools. The project immediately received over 9,000 stars on GitHub, showing strong community interest. The free strategy disrupts the market: Zero cost usage: Simply use a personal Google account to obtain a Gemini Code Assist license. Top model: Free access to the Gemini 2.5 Pro model.</p></div></div><div class="borderTop my-[12px]"></div><div class="flex justify-between items-center mainColor"><div class="inline-flex items-center mainColor gap-[4px]"><i class="iconfont icon-rili text-[14px]"></i><span>Jun 26, 2025</span></div><div class="inline-flex items-center mainColor gap-[4px]"><i class="iconfont icon-fangwenliang1 text-[14px]"></i><span>69.3k</span></div></div></a><a href="https://news.aibase.com/news/15990" rel="noopener noreferrer" target="_blank" class="bg-white mainRadius overflow-hidden border border-solid border-[#EBEDF0] px-[16px] py-[12px] group/detail transition-all duration-200 cursor-pointer"><div class=""><div class="block group"><h2 class="mb-[8px] text-[16px] leading-[24px] font600 mainColor line-clamp-2">Alibaba Open-Sources New Inference Large Model QwQ-32B, Rivaling DeepSeek-R1 with Lower VRAM Requirements</h2><p class="text-[14px] leading-[22px] tipColor line-clamp-2">Alibaba has open-sourced a new inference large language model, QwQ-32B.  Benchmarks show performance comparable to DeepSeek-R1, but with significantly reduced VRAM requirements.</p></div></div><div class="borderTop my-[12px]"></div><div class="flex justify-between items-center mainColor"><div class="inline-flex items-center mainColor gap-[4px]"><i class="iconfont icon-rili text-[14px]"></i><span>Mar 6, 2025</span></div><div class="inline-flex items-center mainColor gap-[4px]"><i class="iconfont icon-fangwenliang1 text-[14px]"></i><span>160.2k</span></div></div></a><a href="https://news.aibase.com/news/15713" rel="noopener noreferrer" target="_blank" class="bg-white mainRadius overflow-hidden border border-solid border-[#EBEDF0] px-[16px] py-[12px] group/detail transition-all duration-200 cursor-pointer"><div class=""><div class="block group"><h2 class="mb-[8px] text-[16px] leading-[24px] font600 mainColor line-clamp-2">Tongyi Wanxiang Open-Source Video Generation Model Wan2.1: Generate 480P Videos with Only 8.2GB VRAM</h2><p class="text-[14px] leading-[22px] tipColor line-clamp-2">Recently, Tongyi announced the open-sourcing of its latest Tongyi Wanxiang large model, Wan2.1. Wan2.1 is an AI model focused on high-quality video generation.  With its excellent performance in handling complex movements, reproducing realistic physics, enhancing cinematic quality, and improving instruction following, it has become the preferred tool for creators, developers, and businesses embracing the AI era.</p></div></div><div class="borderTop my-[12px]"></div><div class="flex justify-between items-center mainColor"><div class="inline-flex items-center mainColor gap-[4px]"><i class="iconfont icon-rili text-[14px]"></i><span>Feb 26, 2025</span></div><div class="inline-flex items-center mainColor gap-[4px]"><i class="iconfont icon-fangwenliang1 text-[14px]"></i><span>438.4k</span></div></div></a><!--]--></div></div></div></div></div></div><!--]--></div><div id="Foot-box" class="relative bottom-0 w-full"><div class="w-full bg-white Foot border-t-[1px] border-t-[#ebedf0] border-t-solid"><div class="commContainer mx-auto pt-[32px] pb-[28px] md:flex flex-col justify-between"><div class="flex-1 md:flex md:justify-between 2xl:gap-[160px] xl:gap-[80px] lg:gap-[20px]"><div class="md:w-1/3 flex flex-col items-start justify-center md:justify-start md:mb-0 mb-[24px]"><img src="data:image/svg+xml,%3c?xml%20version=&#39;1.0&#39;%20encoding=&#39;UTF-8&#39;?%3e%3csvg%20id=&#39;_图层_1&#39;%20data-name=&#39;图层_1&#39;%20xmlns=&#39;http://www.w3.org/2000/svg&#39;%20version=&#39;1.1&#39;%20viewBox=&#39;0%200%2095%2024&#39;%3e%3c!--%20Generator:%20Adobe%20Illustrator%2029.2.1,%20SVG%20Export%20Plug-In%20.%20SVG%20Version:%202.1.0%20Build%20116)%20--%3e%3cdefs%3e%3cstyle%3e%20.st0%20{%20fill:%20%23061b40;%20}%20.st1%20{%20fill:%20%23306af1;%20}%20.st2%20{%20fill:%20%235ce5cf;%20}%20%3c/style%3e%3c/defs%3e%3cg%3e%3cpath%20class=&#39;st0&#39;%20d=&#39;M55,10.5h9v3h-9v9h12V7.5h-12v3ZM64,19.5h-6v-3h6v3Z&#39;/%3e%3cpolygon%20class=&#39;st0&#39;%20points=&#39;69%2016.5%2078%2016.5%2078%2019.5%2069%2019.5%2069%2022.5%2081%2022.5%2081%2013.5%2072%2013.5%2072%2010.5%2081%2010.5%2081%207.5%2069%207.5%2069%2016.5&#39;/%3e%3cpolygon%20class=&#39;st0&#39;%20points=&#39;95%2010.5%2095%207.5%2083%207.5%2083%2022.5%2095%2022.5%2095%2019.5%2086%2019.5%2086%2016.5%2095%2016.5%2095%2013.5%2086%2013.5%2086%2010.5%2095%2010.5&#39;/%3e%3cpath%20class=&#39;st0&#39;%20d=&#39;M40,1.5v21h11.6l1.4-1.4v-7.6h0c0,0-1.4-1.5-1.4-1.5l1.4-1.4V2.9l-1.4-1.4h-11.6ZM50,19.5h-7v-6h7v6ZM50,10.5h-7v-6h7v6Z&#39;/%3e%3c/g%3e%3cpath%20class=&#39;st1&#39;%20d=&#39;M23.1,24L14.7,4.8l-4.9,11.2h3.8l-1.8,4H3.3L12.1,0H2C.9,0,0,.9,0,2v20c0,1.1.9,2,2,2h21.1Z&#39;/%3e%3cpath%20class=&#39;st2&#39;%20d=&#39;M34,0h-16.8l10.6,24h6.2c1.1,0,2-.9,2-2V2C36,.9,35.1,0,34,0ZM32.5,20h-4V4h4v16Z&#39;/%3e%3c/svg%3e" alt="AIbase" class="h-[24px]" loading="lazy"><div class="mainColor text-[14px] leading-[22px] mt-[12px]">Intelligent Future, Your Artificial Intelligence Solution Think Tank</div><div class="flex items-center gap-[40px] mt-[24px] leading-[22px]"><!--[--><!--[--><!--nuxt-i18n-slp-[en]--><a aria-current="page" href="/news/20567" class="router-link-active router-link-exact-active activeHover">English</a><!--/nuxt-i18n-slp--><!--]--><!--[--><!--nuxt-i18n-slp-[zh]--><a href="/zh/news/20567" class="activeHover">简体中文</a><!--/nuxt-i18n-slp--><!--]--><!--[--><!--nuxt-i18n-slp-[tw]--><a href="/tw/news/20567" class="activeHover">繁體中文</a><!--/nuxt-i18n-slp--><!--]--><!--[--><!--nuxt-i18n-slp-[ja]--><a href="/ja/news/20567" class="activeHover">にほんご</a><!--/nuxt-i18n-slp--><!--]--><!--]--></div></div></div><div class="mt-[24px] tipColor text-[14px] flex gap-[12px] justify-start leading-[24px]"><span class="text-nowrap">© 2025</span><span>AIbase</span><!----></div></div></div></div></div></main><!----><div data-v-3f5dacdb></div><span></span><!--]--></div><div id="teleports"></div><script type="application/json" data-nuxt-data="nuxt-app" data-ssr="true" id="__NUXT_DATA__">[["ShallowReactive",1],{"data":2,"state":72,"once":265,"_errors":266,"serverRendered":77,"path":268,"pinia":269},["ShallowReactive",3],{"getAIDetail":4,"getSimilarAIIArticles":25},{"code":5,"msg":6,"data":7,"timeStamp":24},200,"Success",{"title":8,"subtitle":9,"thumb":10,"sourceName":11,"author":9,"tags":12,"description":17,"summary":18,"className":19,"status":20,"type":21,"createTime":22,"pv":23},"Alibaba Launches WebWatcher, a Multimodal Deep Research Intelligent Agent","","https://upload.chinaz.com/2025/0818/6389110600218927649884538.png","AIbase基地",[13,14,15,16],"WebWatcher","Multimodal Deep Research Intelligent Agent","Open Source","Natural Language Processing","The Alibaba Natural Language Processing team announced the launch of WebWatcher, an open-source multimodal deep research intelligent agent designed to break through the limitations of existing closed-source systems and open-source agents in the field of multimodal deep research. WebWatcher integrates various tools such as web browsing, image search, code interpreter, and internal OCR, enabling it to handle complex multimodal tasks like human researchers, demonstrating strong visual understanding, logical reasoning, knowledge retrieval, tool scheduling, and self-validation capabilities. WebWatcher","\u003Cp>The Alibaba Natural Language Processing team announced the release of WebWatcher, an open-source multimodal deep research intelligent agent designed to overcome the limitations of existing closed-source systems and open-source agents in the field of multimodal deep research. WebWatcher integrates various tools such as web browsing, image search, code interpreter, and internal OCR, enabling it to handle complex multimodal tasks like human researchers, demonstrating strong visual understanding, logical reasoning, knowledge retrieval, tool scheduling, and self-verification capabilities.\u003C/p>\u003Cp>The development team of WebWatcher pointed out that although existing closed-source systems such as OpenAI's DeepResearch perform well in text-based deep research, they are mostly limited to pure text environments and struggle to handle complex images, charts, and mixed content in the real world. Existing open-source agents also face two major bottlenecks: one type focuses on text retrieval agents, which can integrate information but cannot process images; the other type is visual agents, which can recognize images but lack cross-modal reasoning and multi-tool collaboration capabilities. WebWatcher was specifically designed to address these bottlenecks.\u003C/p>\u003Cp>WebWatcher's technical solution covers the entire workflow from data construction to training optimization. Its core goal is to enable multimodal agents to have flexible reasoning and multi-tool collaboration capabilities in high-difficulty multimodal deep research tasks. To achieve this, the research team designed an automated multimodal data generation process, collecting cross-modal knowledge chains through random walks and introducing information blurring technology to increase the uncertainty and complexity of tasks. All complex problem samples are expanded into multimodal versions through a QA-to-VQA conversion module, further enhancing the model's cross-modal understanding capabilities.\u003C/p>\u003Cp style=\"text-align:center\">\u003Cimg src=\"https://upload.chinaz.com/2025/0818/6389110600218927649884538.png\" title=\"WeChat screenshot_20250818092634.png\" alt=\"WeChat screenshot_20250818092634.png\"/>\u003C/p>\u003Cp>In terms of building high-quality reasoning trajectories and post-training, WebWatcher adopted an Action-Observation-driven trajectory generation method. By collecting real-world multi-tool interaction trajectories and performing supervised fine-tuning (SFT), the model quickly grasps the basic patterns of multimodal ReAct-style reasoning and tool calling in the early stages of training. Subsequently, the model enters the reinforcement learning phase, further improving the decision-making capabilities of the multimodal agent in complex environments through GRPO.\u003C/p>\u003Cp>To comprehensively verify the capabilities of WebWatcher, the research team proposed BrowseComp-VL, an extension version of BrowseComp for vision-language tasks, aiming to approach the difficulty of cross-modal research tasks performed by human experts. In multi-round rigorous evaluations, WebWatcher significantly outperformed current mainstream open-source and closed-source multimodal large models in complex reasoning, information retrieval, knowledge integration, and information optimization tasks.\u003C/p>\u003Cp>Specifically, on the Humanity’s Last Exam (HLE-VL) benchmark, a multi-step complex reasoning benchmark, WebWatcher achieved a Pass@1 score of 13.6%, far surpassing representative models such as GPT-4o (9.8%), Gemini 2.5-flash (9.2%), and Qwen2.5-VL-72B (8.6%). In the MMSearch evaluation, which is more closely aligned with real-world multimodal search, WebWatcher achieved a Pass@1 score of 55.3%, significantly outperforming Gemini 2.5-flash (43.9%) and GPT-4o (24.1%). In the LiveVQA evaluation, WebWatcher achieved a Pass@1 score of 58.7%, leading other mainstream models. On the most comprehensive and challenging BrowseComp-VL benchmark, WebWatcher achieved an average score (Pass@1) of 27.0%, leading by more than double.\u003C/p>\u003Cp>Repository: https://github.com/Alibaba-NLP/WebAgent\u003C/p>",[9],2,1,"2025-08-18 09:26:52",6117,1757137851505,[26,34,42,50,58,65],{"title":27,"subtitle":28,"thumb":29,"sourceName":11,"author":9,"description":30,"oid":31,"createTime":32,"pv":33},"Open Source Revolution! Kyutai TTS Launches: Ultra-Low Latency Speech Synthesis, the New Era of AI Voice is Here!","Kyutai TTS发布：超低延迟语音合成，AI语音新纪元来袭！","https://pic.chinaz.com/thumb/2025/0704/25070411133655595874.jpg","Recently, the French AI laboratory Kyutai announced the official open source of its new text-to-speech model, Kyutai TTS, providing global developers and researchers with a high-performance, low-latency speech synthesis solution. This breakthrough release not only promotes the development of open-source AI technology but also opens up new possibilities for multilingual voice interaction applications. AIbase provides an exclusive analysis of this technological highlight and its potential impact. Ultra-low latency, a new experience in real-time interaction. Kyutai TTS has become an industry standout with its exceptional performance.",19465,"2025-07-04 11:13:59",3747,{"title":35,"subtitle":36,"thumb":37,"sourceName":11,"author":9,"description":38,"oid":39,"createTime":40,"pv":41},"Baidu Launches the WENXIN Large Model 4.5 Series Open Source, Sparking a New Wave in the Domestic Large Model Market!","百度重磅开源文心大模型 4.5 系列，国内大模型市场再掀波澜！","https://pic.chinaz.com/picmap/202305091556165277_9.jpg","Recently, Baidu officially announced the open-source release of its WENXIN Large Model 4.5 series, launching a total of ten models, including mixed expert (MoE) models with 47B and 3B activated parameters, as well as dense models with 0.3B parameters. This open-source initiative not only fully publicizes the pre-trained weights but also provides inference code, marking a significant advancement for Baidu in the field of large models. These newly released models can be downloaded and deployed on platforms such as PaddlePaddle Starry Sky Community and Hugging Face. Additionally, Baidu Intelligent Cloud's Qianfan Large Model Platform also provides",19339,"2025-06-30 14:23:59",0,{"title":43,"subtitle":44,"thumb":45,"sourceName":11,"author":9,"description":46,"oid":47,"createTime":48,"pv":49},"Google Unveils a Major Move! Gemini CLI Open Source Release, Free AI Coding Assistant Challenges Cursor","谷歌放大招!Gemini CLI开源发布，免费提供AI编程助手挑战Cursor","https://upload.chinaz.com/2025/0626/6388652849669547717086763.png","Google has officially launched Gemini CLI today, an open-source terminal AI agent tool, directly challenging commercial AI coding tools. The project immediately received over 9,000 stars on GitHub, showing strong community interest. The free strategy disrupts the market: Zero cost usage: Simply use a personal Google account to obtain a Gemini Code Assist license. Top model: Free access to the Gemini 2.5 Pro model.",19259,"2025-06-26 09:55:09",4620,{"title":51,"subtitle":52,"thumb":53,"sourceName":11,"author":9,"description":54,"oid":55,"createTime":56,"pv":57},"Alibaba Open-Sources New Inference Large Model QwQ-32B, Rivaling DeepSeek-R1 with Lower VRAM Requirements","​阿里新开源推理大模型QwQ-32B，性能媲美DeepSeek-R1、显存需求更小","https://upload.chinaz.com/2025/0306/6387684942934592679291323.png","Alibaba has open-sourced a new inference large language model, QwQ-32B.  Benchmarks show performance comparable to DeepSeek-R1, but with significantly reduced VRAM requirements.",15990,"2025-03-06 09:17:43",10680,{"title":59,"subtitle":9,"thumb":60,"sourceName":11,"author":9,"description":61,"oid":62,"createTime":63,"pv":64},"Tongyi Wanxiang Open-Source Video Generation Model Wan2.1: Generate 480P Videos with Only 8.2GB VRAM","https://pic.chinaz.com/picmap/thumb/202310311416147098_0.jpg","Recently, Tongyi announced the open-sourcing of its latest Tongyi Wanxiang large model, Wan2.1. Wan2.1 is an AI model focused on high-quality video generation.  With its excellent performance in handling complex movements, reproducing realistic physics, enhancing cinematic quality, and improving instruction following, it has become the preferred tool for creators, developers, and businesses embracing the AI era.",15713,"2025-02-26 07:58:44",29229,{"title":66,"subtitle":9,"thumb":67,"sourceName":11,"author":9,"description":68,"oid":69,"createTime":70,"pv":71},"Ali International Open Source Ovis2 Series Multimodal Large Language Model with Six Versions","https://upload.chinaz.com/2025/0221/6387575538624479013259076.png","Ovis2 is the latest version of the Ovis series models proposed by Alibaba's international team. Compared to the previous version 1.6, Ovis2 has significant improvements in data construction and training methods. It not only enhances the capacity density of small models but also greatly improves chain of thought (CoT) reasoning capabilities through instruction fine-tuning and preference learning. Additionally, Ovis2 introduces video and multi-image processing capabilities, and enhances multilingual abilities and OCR capabilities in complex scenarios, significantly increasing the model's practicality.",15615,"2025-02-21 17:23:48",15113,["Reactive",73],{"$snuxt-i18n-meta":74,"$scolor-mode":75,"$snavigation-menu-en-2":79},{},{"preference":76,"value":76,"unknown":77,"forced":78},"light",true,false,[80,86,106,143,190,243,253,257,261],{"menuName":81,"menuOtherName":82,"menuDescription":83,"menuUrl":84,"menuIcon":9,"menuType":21,"sort":21,"status":21,"children":85},"Home",null,"Homepage, AI services overview","//www.aibase.com",[],{"menuName":87,"menuOtherName":82,"menuDescription":88,"menuUrl":89,"menuIcon":9,"menuType":21,"sort":20,"status":21,"children":90},"AI NEWS","AI industry news and trend analysis platform","//news.aibase.com",[91],{"menuName":92,"menuOtherName":82,"menuDescription":82,"menuUrl":9,"menuIcon":82,"menuType":21,"sort":21,"status":21,"children":93},"Information",[94,100],{"menuName":95,"menuOtherName":82,"menuDescription":96,"menuUrl":97,"menuIcon":98,"menuType":20,"sort":21,"status":21,"children":99},"Latest AI News","Explore AI Frontiers, Master Industry Trends","//news.aibase.com/news","icon-xinwen",[],{"menuName":101,"menuOtherName":82,"menuDescription":102,"menuUrl":103,"menuIcon":104,"menuType":20,"sort":20,"status":21,"children":105},"AI Daily Brief","Your Daily AI Brief - Never Miss What's Next","//news.aibase.com/daily","icon-shijian3",[],{"menuName":107,"menuOtherName":82,"menuDescription":108,"menuUrl":109,"menuIcon":9,"menuType":21,"sort":110,"status":21,"children":111},"AI Tools","AI product one-stop service platform","//app.aibase.com",3,[112,133],{"menuName":92,"menuOtherName":82,"menuDescription":113,"menuUrl":9,"menuIcon":9,"menuType":21,"sort":21,"status":21,"children":114},"AI product information and resource center",[115,121,127],{"menuName":116,"menuOtherName":82,"menuDescription":117,"menuUrl":118,"menuIcon":119,"menuType":20,"sort":21,"status":21,"children":120},"AI Product Finder","Smart Product Discovery - Comprehensive Market Intelligence","//app.aibase.com/discover","icon-jiqiren1",[],{"menuName":122,"menuOtherName":82,"menuDescription":123,"menuUrl":124,"menuIcon":125,"menuType":20,"sort":20,"status":21,"children":126},"AI Product Rankings","AI Product Power Rankings - Performance, Buzz & Trends","//app.aibase.com/best-ai-tools/","icon-bangdan3",[],{"menuName":128,"menuOtherName":82,"menuDescription":129,"menuUrl":130,"menuIcon":131,"menuType":20,"sort":110,"status":21,"children":132},"AI Product Submit","Submit Your AI Product - Amplify Reach & Drive Growth","//app.aibase.com/submission","icon-shangchuan1",[],{"menuName":134,"menuOtherName":82,"menuDescription":135,"menuUrl":82,"menuIcon":9,"menuType":21,"sort":20,"status":21,"children":136},"Tools","AI product practical tools",[137],{"menuName":138,"menuOtherName":82,"menuDescription":139,"menuUrl":140,"menuIcon":141,"menuType":20,"sort":21,"status":21,"children":142},"AI Tools Directory","Discover The Best AI Websites & Tools","//app.aibase.com/tools","icon-daohangye",[],{"menuName":144,"menuOtherName":82,"menuDescription":145,"menuUrl":146,"menuIcon":9,"menuType":21,"sort":147,"status":21,"children":148},"AI Models","AI model one-stop service platform","//model.aibase.com",4,[149,169],{"menuName":92,"menuOtherName":82,"menuDescription":150,"menuUrl":82,"menuIcon":9,"menuType":21,"sort":21,"status":21,"children":151},"Model information and resource center",[152,158,164],{"menuName":153,"menuOtherName":82,"menuDescription":154,"menuUrl":155,"menuIcon":156,"menuType":20,"sort":21,"status":21,"children":157},"AI Models Finder","Comprehensive AI Models Collection for All Your Development & Research Needs","//model.aibase.com/llm","icon-duomoxing",[],{"menuName":159,"menuOtherName":82,"menuDescription":160,"menuUrl":161,"menuIcon":162,"menuType":20,"sort":110,"status":21,"children":163},"Model Providers","Discover Trusted AI Model Partners - Guaranteed Reliable Support","//model.aibase.com/providers","icon-gongyingshang",[],{"menuName":165,"menuOtherName":82,"menuDescription":166,"menuUrl":167,"menuIcon":131,"menuType":20,"sort":147,"status":21,"children":168},"Submit Your Model","Submit Your Model Info & Services - Precision Marketing & User Targeting","//model.aibase.com/submit",[],{"menuName":134,"menuOtherName":82,"menuDescription":170,"menuUrl":82,"menuIcon":9,"menuType":21,"sort":20,"status":21,"children":171},"AI model practical tools",[172,178,184],{"menuName":173,"menuOtherName":82,"menuDescription":174,"menuUrl":175,"menuIcon":176,"menuType":20,"sort":21,"status":21,"children":177},"Compare LLMs","Multi-Dimensional Large Model Comparison - Find Your Perfect Match","//model.aibase.com/compare","icon-duibi",[],{"menuName":179,"menuOtherName":82,"menuDescription":180,"menuUrl":181,"menuIcon":182,"menuType":20,"sort":20,"status":21,"children":183},"LLM Cost Calculator","Calculate AI Model Costs Accurately - Optimize Your Budget","//model.aibase.com/calculator","icon-jisuanqi",[],{"menuName":185,"menuOtherName":82,"menuDescription":186,"menuUrl":187,"menuIcon":188,"menuType":20,"sort":110,"status":21,"children":189},"LLM Arena","Multi-Model Real-Time Evaluation & Quick Output Comparison","//model.aibase.com/arena","icon-VS",[],{"menuName":191,"menuOtherName":82,"menuDescription":192,"menuUrl":193,"menuIcon":9,"menuType":21,"sort":194,"status":21,"children":195},"MCP","MCP service one-stop platform","//mcp.aibase.com",5,[196,228],{"menuName":92,"menuOtherName":82,"menuDescription":197,"menuUrl":82,"menuIcon":9,"menuType":21,"sort":21,"status":21,"children":198},"MCP service information and resource center",[199,205,211,217,223],{"menuName":200,"menuOtherName":82,"menuDescription":201,"menuUrl":202,"menuIcon":203,"menuType":20,"sort":21,"status":21,"children":204},"MCP Servers","Discover Popular AI-MCP Services - Find Your Perfect Match Instantly","//mcp.aibase.com/explore","icon-fuwuqi",[],{"menuName":206,"menuOtherName":82,"menuDescription":207,"menuUrl":208,"menuIcon":209,"menuType":20,"sort":20,"status":21,"children":210},"MCP Client","Easy MCP Client Integration - Access Powerful AI Capabilities","//mcp.aibase.com/client","icon-zhuomian",[],{"menuName":212,"menuOtherName":82,"menuDescription":213,"menuUrl":214,"menuIcon":215,"menuType":20,"sort":110,"status":21,"children":216},"MCP Case Tutorials","Master MCP Usage - From Beginner to Expert","//mcp.aibase.com/server/doc","icon-gonglve",[],{"menuName":218,"menuOtherName":82,"menuDescription":219,"menuUrl":220,"menuIcon":221,"menuType":20,"sort":147,"status":21,"children":222},"MCP Ranking","Top MCP Service Performance Rankings - Find Your Best Choice","//mcp.aibase.com/ranking","icon-bangdan1",[],{"menuName":224,"menuOtherName":82,"menuDescription":225,"menuUrl":226,"menuIcon":131,"menuType":20,"sort":194,"status":21,"children":227},"MCP Service Submission","Publish & Promote Your MCP Services","//mcp.aibase.com/submit",[],{"menuName":134,"menuOtherName":82,"menuDescription":229,"menuUrl":82,"menuIcon":9,"menuType":21,"sort":20,"status":21,"children":230},"MCP service practical tools",[231,237],{"menuName":232,"menuOtherName":82,"menuDescription":233,"menuUrl":234,"menuIcon":235,"menuType":20,"sort":21,"status":21,"children":236},"MCP Playground","Test MCP Services Freely - Quick Online Experience","//mcp.aibase.com/playground","icon-huojian",[],{"menuName":238,"menuOtherName":82,"menuDescription":239,"menuUrl":240,"menuIcon":241,"menuType":20,"sort":20,"status":21,"children":242},"MCP Inspector","Quick MCP Service Testing - Fast Deployment","//mcp.aibase.com/inspector","icon-daima",[],{"menuName":244,"menuOtherName":82,"menuDescription":82,"menuUrl":9,"menuIcon":82,"menuType":21,"sort":245,"status":21,"children":246},"AI Services",6,[247],{"menuName":248,"menuOtherName":82,"menuDescription":249,"menuUrl":250,"menuIcon":251,"menuType":20,"sort":21,"status":21,"children":252},"GEO Services​","Achieve Dominant Visibility in AI Search for Your Business or Brand with GEO Services​","//app.aibase.com/geo","icon-GEO",[],{"menuName":254,"menuOtherName":82,"menuDescription":82,"menuUrl":9,"menuIcon":9,"menuType":21,"sort":255,"status":20,"children":256},"Datasets",7,[],{"menuName":258,"menuOtherName":82,"menuDescription":82,"menuUrl":9,"menuIcon":9,"menuType":21,"sort":259,"status":20,"children":260},"AI Compute",8,[],{"menuName":262,"menuOtherName":82,"menuDescription":82,"menuUrl":9,"menuIcon":9,"menuType":21,"sort":263,"status":20,"children":264},"AI Tutorial",9,[],["Set"],["ShallowReactive",267],{"getAIDetail":82,"getSimilarAIIArticles":82},"/news/20567",["Reactive",270],{"user":271},{"points":272,"code":274,"loginShow":276,"token":278,"loginType":279,"userInfo":280},["EmptyRef",273],"0",["EmptyRef",275],"\"\"",["EmptyRef",277],"false",["EmptyRef",275],["Ref",21],["Ref",281],["Reactive",282],{}]</script>
<script>window.__NUXT__={};window.__NUXT__.config={public:{BASE_URL:"https://mcpapi.aibase.cn",URL_LOGIN:"https://userapi.aibase.cn",ucLoginUrl:"https://uc.chinaz.com",test:"test value",googleClientId:"640171740860-s08v4omrlpm1jspf8mnp2352hjh1ttij.apps.googleusercontent.com",googleRedirectURI:"https://news.aibase.com/login",isCN:"false",defaultLangCode:"en",ucf:"AIbase-mcp",inspectorUrl:"https://test-inspector.aibase.com",cookieDomain:".aibase.com",loginSource:2,allDomain:{mcpUrl:"https://mcp.aibase.com",appUrl:"https://app.aibase.com",newsUrl:"https://news.aibase.com",wwwUrl:"https://www.aibase.com",modelUrl:"https://model.aibase.com"},i18n:{baseUrl:"",defaultLocale:"en",defaultDirection:"ltr",strategy:"prefix_and_default",lazy:false,rootRedirect:"",routesNameSeparator:"___",defaultLocaleRouteNameSuffix:"default",skipSettingLocaleOnNavigate:false,differentDomains:false,trailingSlash:false,locales:[{code:"en",name:"English",showName:"EN",apiKey:"en",hreflang:"en",descriptionLength:160,files:["/usr/src/app/locales/en.json"]},{code:"zh",name:"简体中文",showName:"ZH",apiKey:"zh_cn",hreflang:"zh-CN",descriptionLength:80,files:["/usr/src/app/locales/zh-cn.json"]},{code:"tw",name:"繁體中文",showName:"TW",apiKey:"zh_tw",hreflang:"zh-TW",descriptionLength:75,files:["/usr/src/app/locales/zh-tw.json"]},{code:"ja",name:"にほんご",showName:"JA",apiKey:"jp",hreflang:"ja",descriptionLength:80,files:["/usr/src/app/locales/ja.json"]}],detectBrowserLanguage:{alwaysRedirect:true,cookieCrossOrigin:false,cookieDomain:".aibase.com",cookieKey:"i18n_name",cookieSecure:true,fallbackLocale:"en",redirectOn:"root",useCookie:true},experimental:{localeDetector:"",switchLocalePathLinkSSR:false,autoImportTranslationFunctions:false,typedPages:true,typedOptionsAndMessages:false,generatedLocaleFilePathFormat:"absolute",alternateLinkCanonicalQueries:false},multiDomainLocales:false}},app:{baseURL:"/",buildId:"7aab3073-b06c-4b8b-a0b1-424d9dad5915",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body></html>