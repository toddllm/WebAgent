<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/569ce4b8f30dc480-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/93f479601ee12b01-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/f30152c0704fba31.css?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/a972318adfb35d83.css?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-acbbbb548492d4a6.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T"/><script src="/_next/static/chunks/4bd1b696-cebf68b71ed1e85d.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/1684-03d4b82d0b83989d.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/main-app-8ab5af3d6b81086e.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/b1298b8d-549c141f97a3b262.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/378e5a93-3b0f971d3611a8a5.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/f7f68e2d-40290491c524df5c.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/5009-cf1c1739f4eccbfa.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/3377-d302682beb4206f6.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/6671-9def8ce641ff6e98.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/6967-c4b815e71e97ed18.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/app/layout-a564f3995b363596.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/7bf36345-06f80506190927ed.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/c16f53c3-1a60b9b77b3e1d4b.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/537-d9d5e9261de3e69c.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/8039-e53433e94d896525.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/2136-947db7298d61ada7.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/8674-a5b7b13d963b8a5d.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/3977-e03adae34a519f5e.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/9783-722d48d9df1b9359.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/420-e2b3ea79d67e5a73.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/6032-6ee8e996cc9e1f96.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/6746-6bf545a776fba8b7.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/640-aa28fb220205f1f1.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/app/%5Borg%5D/%5Brepo%5D/%5B%5B...wikiRoutes%5D%5D/page-b1e5c22f5611f67e.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/3449-a5c2556a0924beaf.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/7202-534888446eb5d98d.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/736-84cb3005f3f1502e.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script src="/_next/static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-cfddf83b83fc940f.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><meta name="next-size-adjust" content=""/><title>WebWatcher Research Agent | Alibaba-NLP/WebAgent | DeepWiki</title><meta name="description" content="This document covers WebWatcher, the vision-language deep research agent component of the WebAgent system. WebWatcher is designed to provide advanced research capabilities that leverage both visual an"/><meta property="og:title" content="WebWatcher Research Agent | Alibaba-NLP/WebAgent | DeepWiki"/><meta property="og:description" content="This document covers WebWatcher, the vision-language deep research agent component of the WebAgent system. WebWatcher is designed to provide advanced research capabilities that leverage both visual an"/><meta property="og:url" content="https://deepwiki.com/Alibaba-NLP/WebAgent/5.1-webwatcher-research-agent"/><meta property="og:site_name" content="DeepWiki"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="WebWatcher Research Agent | Alibaba-NLP/WebAgent | DeepWiki"/><meta name="twitter:description" content="This document covers WebWatcher, the vision-language deep research agent component of the WebAgent system. WebWatcher is designed to provide advanced research capabilities that leverage both visual an"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="48x48"/><link rel="icon" href="/icon.png?66aaf51e0e68c818" type="image/png" sizes="16x16"/><link rel="apple-touch-icon" href="/apple-icon.png?a4f658907db0ab87" type="image/png" sizes="180x180"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" noModule=""></script></head><body class="__variable_5cfdac font-geist-sans relative min-h-screen __variable_9a8899 bg-background antialiased"><section aria-label="Notifications alt+T" tabindex="-1" aria-live="polite" aria-relevant="additions text" aria-atomic="false"></section><script>((e,t,r,n,a,o,i,s)=>{let u=document.documentElement,l=["light","dark"];function c(t){var r;(Array.isArray(e)?e:[e]).forEach(e=>{let r="class"===e,n=r&&o?a.map(e=>o[e]||e):a;r?(u.classList.remove(...n),u.classList.add(o&&o[t]?o[t]:t)):u.setAttribute(e,t)}),r=t,s&&l.includes(r)&&(u.style.colorScheme=r)}if(n)c(n);else try{let e=localStorage.getItem(t)||r,n=i&&"system"===e?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":e;c(n)}catch(e){}})("class","theme","light",null,["light","dark"],null,true,true)</script><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><div class="flex min-h-screen w-full flex-col text-white"><div class="container-wrapper flex flex-1 items-center justify-center px-4"><div class="inline-block bg-clip-text text-[#b5b5b5a4] animate-shine text-center text-lg" style="background-image:linear-gradient(120deg, rgba(255, 255, 255, 0) 40%, rgba(255, 255, 255, 0.8) 50%, rgba(255, 255, 255, 0) 60%);background-size:200% 100%;-webkit-background-clip:text;animation-duration:1s">Loading...</div></div></div><!--/$--><script src="/_next/static/chunks/webpack-acbbbb548492d4a6.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[51709,[\"9453\",\"static/chunks/b1298b8d-549c141f97a3b262.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"8970\",\"static/chunks/378e5a93-3b0f971d3611a8a5.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"1585\",\"static/chunks/f7f68e2d-40290491c524df5c.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"5009\",\"static/chunks/5009-cf1c1739f4eccbfa.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"3377\",\"static/chunks/3377-d302682beb4206f6.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"6671\",\"static/chunks/6671-9def8ce641ff6e98.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"6967\",\"static/chunks/6967-c4b815e71e97ed18.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"7177\",\"static/chunks/app/layout-a564f3995b363596.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\"],\"RootProvider\"]\n3:I[87555,[],\"\"]\n4:I[31295,[],\"\"]\n6:I[90894,[],\"ClientPageRoot\"]\n7:I[87667,[\"9453\",\"static/chunks/b1298b8d-549c141f97a3b262.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"8970\",\"static/chunks/378e5a93-3b0f971d3611a8a5.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"1585\",\"static/chunks/f7f68e2d-40290491c524df5c.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"4129\",\"static/chunks/7bf36345-06f80506190927ed.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"2545\",\"static/chunks/c16f53c3-1a60b9b77b3e1d4b.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"537\",\"static/chunks/537-d9d5e9261de3e69c.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"5009\",\"static/chunks/5009-cf1c1739f4eccbfa.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"3377\",\"static/chunks/3377-d302682beb4206f6.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"6671\",\"static/chunks/6671-9def8ce641ff6e98.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"8039\",\"static/chunks/8039-e53433e94d896525.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"2136\",\"static/chunks/2136-947db7298d61ada7.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"6967\",\"static/chunks/6967-c4b815e71e97ed18.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"8674\",\"static/chunks/8674-a5b7b13d963b8a5d.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"3977\",\"static/chunks/3977-e03adae34a519f5e.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"9783\",\"static/ch"])</script><script>self.__next_f.push([1,"unks/9783-722d48d9df1b9359.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"420\",\"static/chunks/420-e2b3ea79d67e5a73.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"6032\",\"static/chunks/6032-6ee8e996cc9e1f96.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"6746\",\"static/chunks/6746-6bf545a776fba8b7.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"640\",\"static/chunks/640-aa28fb220205f1f1.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"3285\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/%5B%5B...wikiRoutes%5D%5D/page-b1e5c22f5611f67e.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\"],\"default\"]\na:I[59665,[],\"OutletBoundary\"]\nd:I[59665,[],\"ViewportBoundary\"]\nf:I[59665,[],\"MetadataBoundary\"]\n11:I[26614,[],\"\"]\n:HL[\"/_next/static/media/569ce4b8f30dc480-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/93f479601ee12b01-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/f30152c0704fba31.css?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"style\"]\n:HL[\"/_next/static/css/a972318adfb35d83.css?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"SXuQ6LyYNQyvXU4dbaOCk\",\"p\":\"\",\"c\":[\"\",\"Alibaba-NLP\",\"WebAgent\",\"5.1-usage-examples\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"org\",\"Alibaba-NLP\",\"d\"],{\"children\":[[\"repo\",\"WebAgent\",\"d\"],{\"children\":[[\"wikiRoutes\",\"5.1-usage-examples\",\"oc\"],{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/f30152c0704fba31.css?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/a972318adfb35d83.css?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{}],[\"$\",\"body\",null,{\"className\":\"__variable_5cfdac font-geist-sans relative min-h-screen __variable_9a8899 bg-background antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}]]}],{\"children\":[[\"org\",\"Alibaba-NLP\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"repo\",\"WebAgent\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,\"$L5\"]}],{\"children\":[[\"wikiRoutes\",\"5.1-usage-examples\",\"oc\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L6\",null,{\"Component\":\"$7\",\"searchParams\":{},\"params\":{\"org\":\"Alibaba-NLP\",\"repo\":\"WebAgent\",\"wikiRoutes\":[\"5.1-usage-examples\"]},\"promises\":[\"$@8\",\"$@9\"]}],\"$undefined\",null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",null]}]]}],{},null,false]},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"klBRi1QVREpGbdBYEd9CL\",{\"children\":[[\"$\",\"$Ld\",null,{\"children\":\"$Le\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"8:{}\n9:{\"org\":\"Alibaba-NLP\",\"repo\":\"WebAgent\",\"wikiRoutes\":\"$0:f:0:1:2:children:2:children:2:children:2:children:1:props:children:0:props:params:wikiRoutes\"}\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"12:I[27393,[\"9453\",\"static/chunks/b1298b8d-549c141f97a3b262.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"8970\",\"static/chunks/378e5a93-3b0f971d3611a8a5.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"1585\",\"static/chunks/f7f68e2d-40290491c524df5c.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"537\",\"static/chunks/537-d9d5e9261de3e69c.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"5009\",\"static/chunks/5009-cf1c1739f4eccbfa.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"3377\",\"static/chunks/3377-d302682beb4206f6.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"8039\",\"static/chunks/8039-e53433e94d896525.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"2136\",\"static/chunks/2136-947db7298d61ada7.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"3449\",\"static/chunks/3449-a5c2556a0924beaf.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"3977\",\"static/chunks/3977-e03adae34a519f5e.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"9783\",\"static/chunks/9783-722d48d9df1b9359.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"7202\",\"static/chunks/7202-534888446eb5d98d.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"736\",\"static/chunks/736-84cb3005f3f1502e.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"2933\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-cfddf83b83fc940f.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\"],\"HeaderWrapperWithSuspense\"]\n13:I[93403,[\"9453\",\"static/chunks/b1298b8d-549c141f97a3b262.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"8970\",\"static/chunks/378e5a93-3b0f971d3611a8a5.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"1585\",\"static/chunks/f7f68e2d-40290491c524df5c.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"537\",\"static/chunks/537-d9d5e9261de3e69c.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"5009\",\"static/chunks/5009-cf1c1739f4eccbfa.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"3377\",\"static/chunks/3377-d302682beb4206f6.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"8039\",\"static/chunks/8039-e53433e94d896525.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"2136\",\"static/chunks/2136-947db7298d61ada7.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"3449\",\"static/chunks/3449-a5c2556a0924beaf.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xn"])</script><script>self.__next_f.push([1,"Ky1T\",\"3977\",\"static/chunks/3977-e03adae34a519f5e.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"9783\",\"static/chunks/9783-722d48d9df1b9359.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"7202\",\"static/chunks/7202-534888446eb5d98d.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"736\",\"static/chunks/736-84cb3005f3f1502e.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\",\"2933\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-cfddf83b83fc940f.js?dpl=dpl_41ugcEfV6nkXKBScP1DcX4xnKy1T\"],\"WikiContextProvider\"]\n14:T22ba,"])</script><script>self.__next_f.push([1,"# Overview\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [assets/aliyun.png](assets/aliyun.png)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides a comprehensive overview of the WebAgent system, a sophisticated multi-component framework for autonomous web information seeking developed by Tongyi Lab, Alibaba Group. WebAgent consists of five core components that work together to enable advanced web navigation, data synthesis, reasoning, and benchmarking capabilities.\n\nFor detailed information about individual components, see [WebShaper](#2), [WebDancer](#3), [WebSailor](#4), and [WebWatcher and WebWalker](#5). For installation and usage instructions, see [Getting Started](#6).\n\n## System Architecture\n\nWebAgent is built around five specialized components that address different aspects of web information seeking:\n\n### Core Components Overview\n\n```mermaid\ngraph TB\n    WA[\"WebAgent System\u003cbr/\u003eTongyi Lab, Alibaba Group\"]\n    \n    WA --\u003e WW[\"WebWatcher\u003cbr/\u003eVision-Language Deep Research Agent\"]\n    WA --\u003e WS[\"WebShaper\u003cbr/\u003eAgentic Data Synthesizing\"]\n    WA --\u003e WSA[\"WebSailor\u003cbr/\u003eSuper-human Reasoning for Web Agent\"]\n    WA --\u003e WD[\"WebDancer\u003cbr/\u003eAutonomous Information Seeking Agency\"]\n    WA --\u003e WWA[\"WebWalker\u003cbr/\u003eBenchmarking LLMs in Web Traversal\"]\n    \n    subgraph \"Generated Assets\"\n        WSQA[\"WebShaperQA Dataset\"]\n        SFQA[\"SailorFog-QA Dataset\"] \n        WWQA[\"WebWalkerQA Dataset\"]\n        WSA7B[\"WebSailor-7B Model\"]\n        WSA3B[\"WebSailor-3B Model\"]\n        WD32B[\"WebDancer-32B Model\"]\n    end\n    \n    WS --\u003e WSQA\n    WSA --\u003e SFQA\n    WSA --\u003e WSA7B\n    WSA --\u003e WSA3B\n    WWA --\u003e WWQA\n    WD --\u003e WD32B\n    \n    subgraph \"Evaluation Benchmarks\"\n        GAIA[\"GAIA Benchmark\"]\n        BROWSE[\"BrowseComp Benchmarks\"]\n        SIMPLE[\"SimpleQA\"]\n    end\n    \n    WSQA --\u003e GAIA\n    WWQA --\u003e GAIA\n    SFQA --\u003e BROWSE\n    WD32B --\u003e GAIA\n    WSA7B --\u003e BROWSE\n    WSA7B --\u003e SIMPLE\n```\n\n**System Component Architecture**\n\nSources: [README.md:35-41]()\n\n| Component | Purpose | Key Outputs | Primary Use Case |\n|-----------|---------|-------------|------------------|\n| **WebWatcher** | Vision-language deep research agent | Research demonstrations | Multi-modal VQA problems |\n| **WebShaper** | Agentic data synthesis via formalization | WebShaperQA dataset | Training data generation |\n| **WebSailor** | Super-human reasoning for web navigation | WebSailor models, SailorFog-QA | Complex information seeking |\n| **WebDancer** | Autonomous information seeking agency | WebDancer-32B model | Native agentic search |\n| **WebWalker** | Benchmarking framework for web traversal | WebWalkerQA benchmark | LLM evaluation |\n\n### Component Integration and Code Structure\n\n```mermaid\ngraph LR\n    subgraph \"WebAgent Repository Structure\"\n        ROOT[\"/WebAgent\"]\n        ROOT --\u003e WS_DIR[\"/WebShaper\"]\n        ROOT --\u003e WD_DIR[\"/WebDancer\"] \n        ROOT --\u003e WSA_DIR[\"/WebSailor\"]\n        ROOT --\u003e WW_DIR[\"/WebWatcher\"]\n        ROOT --\u003e WWA_DIR[\"/WebWalker\"]\n        ROOT --\u003e ASSETS[\"/assets\"]\n        ROOT --\u003e README[\"README.md\"]\n    end\n    \n    subgraph \"Core Interfaces\"\n        SEARCH_TOOL[\"Search Tool\u003cbr/\u003eGoogle Serper API\"]\n        VISIT_TOOL[\"Visit Tool\u003cbr/\u003eJina Reader API\"]\n        SGLANG[\"SGLang Server\u003cbr/\u003eModel Inference\"]\n        GRADIO[\"Gradio Demo Interface\"]\n    end\n    \n    subgraph \"External Services\"\n        GOOGLE_API[\"serper.dev\"]\n        JINA_API[\"jina.ai\"]\n        DASHSCOPE[\"dashscope.aliyun.com\"]\n    end\n    \n    WD_DIR --\u003e SEARCH_TOOL\n    WD_DIR --\u003e VISIT_TOOL\n    WSA_DIR --\u003e SEARCH_TOOL\n    WSA_DIR --\u003e VISIT_TOOL\n    \n    SEARCH_TOOL --\u003e GOOGLE_API\n    VISIT_TOOL --\u003e JINA_API\n    WD_DIR --\u003e DASHSCOPE\n    \n    WD_DIR --\u003e SGLANG\n    WSA_DIR --\u003e SGLANG\n    SGLANG --\u003e GRADIO\n```\n\n**WebAgent Code Structure and Service Integration**\n\nSources: [README.md:89-124]()\n\n## Core Capabilities\n\n### Data Synthesis and Training\n\nWebAgent employs three distinct data synthesis methodologies:\n\n1. **Formalization-driven synthesis** (WebShaper): Systematic generation of information-seeking instances through task formalization\n2. **Graph sampling and obfuscation** (WebSailor): High-uncertainty QA benchmarks using knowledge graphs  \n3. **Multi-agent framework** (WebWalker): Web traversal benchmarking through collaborative agents\n\n### Training Paradigms\n\n```mermaid\nflowchart TD\n    subgraph \"WebShaper Pipeline\"\n        RAW[\"Raw Web Content\"] --\u003e FORM[\"Formalization Process\"]\n        FORM --\u003e EXP[\"Agentic Expander\"]\n        EXP --\u003e WSQA[\"WebShaperQA Dataset\"]\n    end\n    \n    subgraph \"WebSailor Training\"\n        KG[\"Knowledge Graphs\"] --\u003e SAMPLE[\"Graph Sampling\"]\n        SAMPLE --\u003e OBFUS[\"Information Obfuscation\"] \n        OBFUS --\u003e SFQA[\"SailorFog-QA Dataset\"]\n        SFQA --\u003e RFT[\"RFT Cold Start\"]\n        RFT --\u003e DUPO[\"DUPO Reinforcement Learning\"]\n        DUPO --\u003e WSA_MODELS[\"WebSailor Models\"]\n    end\n    \n    subgraph \"WebDancer Training\" \n        BROWSE[\"Browsing Data Construction\"] --\u003e TRAJ[\"Trajectory Sampling\"]\n        TRAJ --\u003e SFT[\"Supervised Fine-tuning\"]\n        SFT --\u003e DAPO[\"DAPO Reinforcement Learning\"]\n        DAPO --\u003e WD_MODEL[\"WebDancer-32B\"]\n    end\n    \n    WSQA --\u003e BENCH[\"Benchmark Evaluation\"]\n    WSA_MODELS --\u003e BENCH\n    WD_MODEL --\u003e BENCH\n```\n\n**Training Pipeline Architecture**\n\nSources: [README.md:68-88]()\n\n### Autonomous Information Seeking\n\nThe system implements autonomous web information seeking through:\n\n- **ReAct Framework**: Used by WebDancer for iterative reasoning and acting\n- **Multi-step Reasoning**: Extended thinking processes for complex task completion\n- **Tool Integration**: Standardized Search and Visit tools for web interaction\n- **Trajectory-level Supervision**: Training paradigm enabling autonomous decision making\n\n## Performance Highlights\n\n### Benchmark Results\n\n| Model | GAIA | WebWalkerQA | BrowseComp-en | BrowseComp-zh | SimpleQA |\n|-------|------|-------------|---------------|---------------|----------|\n| **WebDancer-32B** | 64.1% (Pass@3) | 62.0% (Pass@3) | - | - | - |\n| **WebSailor-7B** | 55.4% | - | 12.0% | 30.1% | âœ“ |\n| **WebShaper** | 60.19% | 52.50% | - | - | - |\n\n### Key Achievements\n\n- **State-of-the-art results** on GAIA and WebWalkerQA benchmarks through WebShaper's formalization-driven approach\n- **Superior reasoning capabilities** demonstrated by WebSailor's performance on complex browsing tasks\n- **Native agentic capabilities** enabling autonomous multi-step information seeking through WebDancer\n- **Comprehensive benchmarking framework** for evaluating LLM web traversal abilities via WebWalker\n\nSources: [README.md:54-88]()\n\n## System Integration\n\n### Deployment Architecture\n\n```mermaid\ngraph TB\n    subgraph \"User Interface Layer\"\n        DEMO[\"Gradio Demo Interface\"]\n        CLI[\"Command Line Interface\"]\n    end\n    \n    subgraph \"Model Serving Layer\"\n        SGL[\"SGLang Server\"]\n        OPENAI_API[\"OpenAI-Compatible API\"]\n    end\n    \n    subgraph \"Tool Layer\"\n        SEARCH[\"SearchTool\"]\n        VISIT[\"VisitTool\"]\n    end\n    \n    subgraph \"Agent Layer\"\n        WD_AGENT[\"WebDancer Agent\"]\n        WS_AGENT[\"WebSailor Agent\"]\n    end\n    \n    subgraph \"External APIs\"\n        SERPER[\"Google Serper API\"]\n        JINA[\"Jina Reader API\"]\n        DASH[\"DashScope API\"]\n    end\n    \n    DEMO --\u003e SGL\n    CLI --\u003e SGL\n    SGL --\u003e WD_AGENT\n    SGL --\u003e WS_AGENT\n    \n    WD_AGENT --\u003e SEARCH\n    WD_AGENT --\u003e VISIT\n    WS_AGENT --\u003e SEARCH\n    WS_AGENT --\u003e VISIT\n    \n    SEARCH --\u003e SERPER\n    VISIT --\u003e JINA\n    WD_AGENT --\u003e DASH\n    VISIT --\u003e OPENAI_API\n```\n\n**WebAgent Deployment and Service Architecture**\n\nSources: [README.md:113-124]()\n\n### Quick Start Overview\n\nThe system can be deployed following these key steps:\n\n1. **Environment Setup**: Python 3.12 with required dependencies from `requirements.txt`\n2. **Model Deployment**: Download models from HuggingFace/ModelScope and deploy using SGLang\n3. **API Configuration**: Configure keys for Google Serper, Jina Reader, and DashScope APIs\n4. **Demo Launch**: Run Gradio interface for interactive demonstrations\n\nFor complete setup instructions, see [Installation and Setup](#6.1).\n\n## Research Impact\n\nWebAgent represents a comprehensive approach to autonomous web information seeking, combining:\n\n- **Advanced data synthesis** techniques for scalable training data generation\n- **Sophisticated training paradigms** including reinforcement learning for agentic capabilities  \n- **Robust evaluation frameworks** for measuring web navigation and reasoning performance\n- **Production-ready deployment** architectures supporting real-world applications\n\nThe system has demonstrated significant improvements over existing approaches across multiple benchmark evaluations and provides a foundation for developing next-generation web-based AI agents.\n\nSources: [README.md:1-238]()"])</script><script>self.__next_f.push([1,"15:T242b,"])</script><script>self.__next_f.push([1,"# System Architecture\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [assets/aliyun.png](assets/aliyun.png)\n- [assets/tongyi.png](assets/tongyi.png)\n- [assets/webagent.png](assets/webagent.png)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the high-level system architecture of the WebAgent ecosystem, which consists of four integrated components for autonomous information seeking and web agent development. The architecture encompasses data synthesis pipelines, training methodologies, runtime deployment systems, and evaluation frameworks.\n\nFor detailed implementation specifics of individual components, see [WebShaper](#2), [WebDancer](#3), [WebSailor](#4), and [WebWalker](#5). For deployment and usage instructions, see [Getting Started](#6). For performance metrics and benchmarking details, see [Evaluation and Performance](#7).\n\n## High-Level Component Overview\n\nThe WebAgent ecosystem consists of five main components that address different aspects of the information seeking pipeline:\n\n### WebAgent System Overview\n\n```mermaid\ngraph TB\n    WA[\"WebAgent for Information Seeking\u003cbr/\u003eTongyi Lab, Alibaba Group\"]\n    \n    WA --\u003e WW[\"WebWatcher\u003cbr/\u003eVision-Language Deep Research Agent\"]\n    WA --\u003e WS[\"WebShaper\u003cbr/\u003eAgentic Data Synthesizing\"]\n    WA --\u003e WSA[\"WebSailor\u003cbr/\u003eSuper-human Reasoning for Web Agent\"]\n    WA --\u003e WD[\"WebDancer\u003cbr/\u003eAutonomous Information Seeking Agency\"]\n    WA --\u003e WWA[\"WebWalker\u003cbr/\u003eBenchmarking LLMs in Web Traversal\"]\n    \n    subgraph \"Data Generation \u0026 Synthesis\"\n        WS --\u003e WSQA[\"WebShaperQA Dataset\"]\n        WSA --\u003e SFQA[\"SailorFog-QA Dataset\"]\n        WWA --\u003e WWQA[\"WebWalkerQA Dataset\"]\n    end\n    \n    subgraph \"Model Training \u0026 Deployment\"\n        WSA --\u003e WSA7B[\"WebSailor-7B Model\"]\n        WSA --\u003e WSA3B[\"WebSailor-3B Model\"]\n        WD --\u003e WD32B[\"WebDancer-32B Model\"]\n    end\n    \n    subgraph \"Evaluation Benchmarks\"\n        GAIA[\"GAIA Benchmark\"]\n        BROWSE[\"BrowseComp Benchmarks\"]\n        SIMPLE[\"SimpleQA\"]\n    end\n    \n    WSQA --\u003e GAIA\n    WWQA --\u003e GAIA\n    SFQA --\u003e BROWSE\n    WD32B --\u003e GAIA\n    WSA7B --\u003e BROWSE\n    WSA7B --\u003e SIMPLE\n```\n\n**Sources:** [README.md:35-41](), [README.md:69](), [README.md:76](), [README.md:84]()\n\n## Training and Deployment Pipeline Architecture\n\nThe system implements a comprehensive pipeline from raw data processing through model deployment:\n\n### Training Pipeline Architecture\n\n```mermaid\ngraph TD\n    subgraph \"Data Construction Phase\"\n        A[\"Raw Web Content\"] --\u003e B[\"WebShaper Formalization\"]\n        B --\u003e C[\"Agentic Expander\"]\n        C --\u003e D[\"WebShaperQA Dataset\"]\n        \n        E[\"Knowledge Graphs\"] --\u003e F[\"SailorFog-QA Pipeline\"]\n        F --\u003e G[\"Information Obfuscation\"]\n        G --\u003e H[\"SailorFog-QA Dataset\"]\n        \n        I[\"Multi-agent Framework\"] --\u003e J[\"WebWalkerQA Dataset\"]\n    end\n    \n    subgraph \"Training Pipeline\"\n        D --\u003e K[\"WebSailor Training\"]\n        H --\u003e K\n        K --\u003e L[\"Expert Trajectories\"]\n        L --\u003e M[\"RFT Cold Start\"]\n        M --\u003e N[\"DUPO Reinforcement Learning\"]\n        N --\u003e O[\"WebSailor Models\"]\n        \n        D --\u003e P[\"WebDancer Training\"]\n        P --\u003e Q[\"Browsing Data Construction\"]\n        Q --\u003e R[\"Trajectory Sampling\"]\n        R --\u003e S[\"Supervised Fine-tuning\"]\n        S --\u003e T[\"Reinforcement Learning DAPO\"]\n        T --\u003e U[\"WebDancer Models\"]\n    end\n    \n    subgraph \"Deployment \u0026 Evaluation\"\n        O --\u003e V[\"SGLang Servers\"]\n        U --\u003e V\n        V --\u003e W[\"Gradio Demo Interface\"]\n        V --\u003e X[\"Benchmark Evaluation\"]\n        \n        W --\u003e Y[\"User Interaction\"]\n        X --\u003e Z[\"Performance Results\"]\n    end\n```\n\n**Sources:** [README.md:67-70](), [README.md:75-78](), [README.md:82-87](), [README.md:99-124]()\n\n## Runtime Architecture\n\nThe deployed WebAgent system uses a multi-tier architecture with local model servers and external API integrations:\n\n### Tool and Service Integration Architecture\n\n```mermaid\ngraph LR\n    subgraph \"Core Agents\"\n        WD[\"WebDancer Agent\"]\n        WS_Agent[\"WebSailor Agent\"]\n    end\n    \n    subgraph \"Tool Layer\"\n        SEARCH[\"Search Tool\u003cbr/\u003eGoogle Serper API\"]\n        VISIT[\"Visit Tool\u003cbr/\u003eJina Reader API\"]\n    end\n    \n    subgraph \"External APIs\"\n        GOOGLE[\"Google Search\u003cbr/\u003eserper.dev\"]\n        JINA[\"Jina Reader\u003cbr/\u003ejina.ai\"]\n        DASH[\"DashScope\u003cbr/\u003edashscope.aliyun.com\"]\n    end\n    \n    subgraph \"LLM Services\"\n        SGLANG[\"SGLang Server\u003cbr/\u003eModel Inference\"]\n        OPENAI[\"OpenAI-Compatible API\u003cbr/\u003eSummary Models\"]\n    end\n    \n    subgraph \"User Interface\"\n        GRADIO[\"Gradio Web Interface\"]\n        CLI[\"Command Line Interface\"]\n    end\n    \n    WD --\u003e SEARCH\n    WD --\u003e VISIT\n    WS_Agent --\u003e SEARCH\n    WS_Agent --\u003e VISIT\n    \n    SEARCH --\u003e GOOGLE\n    VISIT --\u003e JINA\n    WD --\u003e DASH\n    \n    WD --\u003e SGLANG\n    WS_Agent --\u003e SGLANG\n    VISIT --\u003e OPENAI\n    \n    SGLANG --\u003e GRADIO\n    SGLANG --\u003e CLI\n    \n    GRADIO --\u003e WD\n    CLI --\u003e WS_Agent\n```\n\n**Sources:** [README.md:99-124](), [README.md:110-118]()\n\n### Component Interaction and Communication Patterns\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant WebDancer as \"WebDancer Agent\"\n    participant Tools as \"Tool Layer\"\n    participant APIs as \"External APIs\"\n    participant LLM as \"SGLang Server\"\n    \n    User-\u003e\u003eWebDancer: \"Submit information seeking query\"\n    WebDancer-\u003e\u003eLLM: \"Generate action plan (ReAct)\"\n    LLM-\u003e\u003eWebDancer: \"Reasoning + planned actions\"\n    \n    loop Multi-step Information Seeking\n        WebDancer-\u003e\u003eTools: \"Execute search/visit tools\"\n        Tools-\u003e\u003eAPIs: \"API calls (Google, Jina)\"\n        APIs-\u003e\u003eTools: \"Results\"\n        Tools-\u003e\u003eWebDancer: \"Processed results\"\n        WebDancer-\u003e\u003eLLM: \"Update context + continue reasoning\"\n        LLM-\u003e\u003eWebDancer: \"Next actions or final answer\"\n    end\n    \n    WebDancer-\u003e\u003eUser: \"Comprehensive answer with sources\"\n```\n\n**Sources:** [README.md:82-87](), [README.md:154-172]()\n\n## Data Flow and Evaluation Ecosystem\n\nThe system implements comprehensive benchmarking across multiple evaluation datasets:\n\n### Data Flow and Evaluation Ecosystem\n\n```mermaid\ngraph TB\n    subgraph \"Data Synthesis Methods\"\n        FORM[\"Formalization-driven\u003cbr/\u003eWebShaper Method\"]\n        OBFUS[\"Graph Sampling \u0026\u003cbr/\u003eInformation Obfuscation\u003cbr/\u003eWebSailor Method\"]\n        MULTI[\"Multi-agent Framework\u003cbr/\u003eWebWalker Method\"]\n    end\n    \n    subgraph \"Generated Datasets\"\n        WSQA[\"WebShaperQA\u003cbr/\u003e60.19% GAIA\u003cbr/\u003e52.50% WebWalkerQA\"]\n        SFQA[\"SailorFog-QA\u003cbr/\u003eHigh Uncertainty Tasks\"]\n        WWQA[\"WebWalkerQA\u003cbr/\u003eWeb Traversal Benchmark\"]\n    end\n    \n    subgraph \"Trained Models\"\n        WS_MODEL[\"WebSailor Models\u003cbr/\u003e3B, 7B variants\"]\n        WD_MODEL[\"WebDancer Models\u003cbr/\u003e32B variant\"]\n    end\n    \n    subgraph \"Evaluation Results\"\n        GAIA_EVAL[\"GAIA Benchmark\u003cbr/\u003eWebDancer: 64.1% Pass@3\"]\n        BROWSE_EVAL[\"BrowseComp Benchmarks\u003cbr/\u003eWebSailor: 12.0% en, 30.1% zh\"]\n        WW_EVAL[\"WebWalkerQA Benchmark\u003cbr/\u003eWebDancer: 62.0% Pass@3\"]\n    end\n    \n    FORM --\u003e WSQA\n    OBFUS --\u003e SFQA\n    MULTI --\u003e WWQA\n    \n    WSQA --\u003e WS_MODEL\n    SFQA --\u003e WS_MODEL\n    WWQA --\u003e WD_MODEL\n    \n    WS_MODEL --\u003e BROWSE_EVAL\n    WD_MODEL --\u003e GAIA_EVAL\n    WD_MODEL --\u003e WW_EVAL\n    \n    WSQA -.-\u003e GAIA_EVAL\n    WWQA -.-\u003e WW_EVAL\n```\n\n**Sources:** [README.md:69-72](), [README.md:76-79](), [README.md:84-87]()\n\n## Code Structure Mapping\n\nThe following table maps high-level components to their corresponding code entities:\n\n| Component | Primary Directory | Key Files | Main Classes/Functions |\n|-----------|-------------------|-----------|------------------------|\n| WebShaper | `WebShaper/` | Data synthesis pipeline | Agentic Expander, Formalization Engine |\n| WebSailor | `WebSailor/` | Post-training methodology | `MultiTurnReactAgent`, RFT, DUPO |\n| WebDancer | `WebDancer/` | `gradio_demo.py`, `deploy_model.sh` | ReAct Agent, Tool Registry |\n| WebWalker | `WebWalker/` | Benchmark framework | WebWalkerQA evaluation |\n\n### Deployment Scripts and Configuration\n\n```mermaid\ngraph TB\n    subgraph \"WebDancer Deployment\"\n        DEPLOY_SH[\"scripts/deploy_model.sh\u003cbr/\u003e(Model Deployment)\"]\n        RUN_DEMO[\"scripts/run_demo.sh\u003cbr/\u003e(Demo Launch)\"]\n        GRADIO_PY[\"gradio_demo.py\u003cbr/\u003e(Web Interface)\"]\n        REQ_TXT[\"requirements.txt\u003cbr/\u003e(Dependencies)\"]\n    end\n    \n    subgraph \"Configuration\"\n        ENV_VARS[\"Environment Variables\u003cbr/\u003e(API Keys)\"]\n        SGLANG_CONFIG[\"sglang Configuration\u003cbr/\u003e(Port 8004)\"]\n    end\n    \n    DEPLOY_SH --\u003e SGLANG_CONFIG\n    RUN_DEMO --\u003e GRADIO_PY\n    RUN_DEMO --\u003e ENV_VARS\n    ENV_VARS --\u003e GRADIO_PY\n```\n\n**Sources:** [README.md:99-104](), [README.md:110-121]()\n\n### API Integration Points\n\nThe system integrates with three main external services through environment variable configuration:\n\n- **GOOGLE_SEARCH_KEY**: Serper API for web search functionality\n- **JINA_API_KEY**: Jina API for content processing and extraction  \n- **DASHSCOPE_API_KEY**: Alibaba Cloud DashScope for additional LLM services\n\n**Sources:** [README.md:112-114]()\n\nThis architecture enables scalable deployment of autonomous information seeking agents with comprehensive evaluation capabilities across multiple benchmark datasets."])</script><script>self.__next_f.push([1,"16:T1cac,"])</script><script>self.__next_f.push([1,"# WebShaper\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [WebShaper/assets/case_study.png](WebShaper/assets/case_study.png)\n- [WebShaper/readme.md](WebShaper/readme.md)\n- [assets/aliyun.png](assets/aliyun.png)\n\n\u003c/details\u003e\n\n\n\nWebShaper is a formalization-driven data synthesis component within the WebAgent ecosystem that generates high-quality training data for information-seeking agents. It introduces a novel approach to synthetic data generation by establishing formal task structures before collecting information, enabling systematic and scalable creation of information-seeking question-answer pairs.\n\nFor information about WebShaper's detailed data synthesis pipeline, see [Data Synthesis Pipeline](#2.1). For architectural implementation details, see [Architecture and Implementation](#2.2). For deep technical details on the formalization methodology, see [Formalization Method](#2.3).\n\n## Overview and Purpose\n\nWebShaper addresses the fundamental challenge of creating diverse, high-quality training data for information-seeking agents through its **formalization-driven** approach. Unlike traditional methods that collect information first and then synthesize questions, WebShaper establishes formal task structures and then systematically generates data aligned with these formalizations.\n\nThe system produces the **WebShaperQA dataset**, containing 500 carefully synthesized question-answer pairs that achieve state-of-the-art performance on multiple benchmarks, including 60.19% on GAIA and 52.50% on WebWalkerQA.\n\nSources: [README.md:68-72](), [WebShaper/readme.md:9-13]()\n\n## Core Formalization Approach\n\nWebShaper's key innovation lies in its **information-seeking task formalization** that defines structured representations of complex reasoning tasks before data synthesis begins.\n\n### Knowledge Projections and Operations\n\nThe formalization uses **Knowledge Projections (KP)** as fundamental building blocks, representing sets of entities with two core operations:\n\n| Operation | Description | Purpose |\n|-----------|-------------|---------|\n| **R-Union** | Combines distributed web content | Creates challenging information aggregation tasks |\n| **Intersection** | Finds common elements across sources | Enables multi-step reasoning requirements |\n\n### Expansion Paradigms\n\nWebShaper implements three distinct structural approaches for data generation:\n\n```mermaid\ngraph TD\n    subgraph \"Expansion Paradigms\"\n        A[\"Random Structure\"] --\u003e A1[\"Random constant addition\"]\n        B[\"Sequential Structure\"] --\u003e B1[\"Chain-based reasoning sequence\"]\n        C[\"Layer-wise Structure\"] --\u003e C1[\"Hierarchical variable replacement\"]\n    end\n    \n    subgraph \"Core Components\"\n        D[\"Target Variable (T)\"] --\u003e E[\"Intermediate Variables\"]\n        E --\u003e F[\"Constants\"]\n        F --\u003e G[\"Knowledge Projections (KP)\"]\n    end\n    \n    C1 --\u003e D\n    B1 --\u003e E\n    A1 --\u003e F\n```\n\nSources: [WebShaper/readme.md:53-60](), [WebShaper/readme.md:45-51]()\n\n## WebShaper Data Synthesis Pipeline\n\nThe following diagram illustrates how WebShaper integrates formalization with agentic data generation:\n\n```mermaid\ngraph TB\n    subgraph \"Formalization Layer\"\n        FORM[\"Task Formalization\"] --\u003e KP[\"Knowledge Projections\"]\n        KP --\u003e STRUCT[\"Layer-wise Structure\"]\n    end\n    \n    subgraph \"Agentic Generation\"\n        STRUCT --\u003e EXPAND[\"Agentic Expander\"]\n        EXPAND --\u003e VALID[\"Question Validation\"]\n        VALID --\u003e ITER[\"Iterative Generation\"]\n    end\n    \n    subgraph \"Data Output\"\n        ITER --\u003e DATASET[\"webshaper.500.jsonl\"]\n        DATASET --\u003e FIELDS[\"Data Fields:\u003cbr/\u003e- id\u003cbr/\u003e- question\u003cbr/\u003e- formalization\u003cbr/\u003e- answer\u003cbr/\u003e- urls\"]\n    end\n    \n    subgraph \"Information Collection\"\n        WEB[\"Web Information\"] --\u003e RUNION[\"R-Union Operations\"]\n        RUNION --\u003e INTERSECT[\"Intersection Operations\"]\n        INTERSECT --\u003e EXPAND\n    end\n```\n\nSources: [WebShaper/readme.md:15-27](), [WebShaper/readme.md:36-44]()\n\n## Integration with WebAgent Ecosystem\n\nWebShaper serves as a foundational data synthesis component that supports multiple WebAgent subsystems:\n\n```mermaid\ngraph LR\n    subgraph \"WebShaper Output\"\n        WS[\"WebShaperQA Dataset\"] --\u003e GAIA[\"GAIA Benchmark\u003cbr/\u003e60.19% Performance\"]\n        WS --\u003e WWQA[\"WebWalkerQA\u003cbr/\u003e52.50% Performance\"]\n    end\n    \n    subgraph \"Training Pipeline Integration\"\n        WS --\u003e WST[\"WebSailor Training\"]\n        WS --\u003e WDT[\"WebDancer Training\"]\n        WST --\u003e WSM[\"WebSailor Models\"]\n        WDT --\u003e WDM[\"WebDancer Models\"]\n    end\n    \n    subgraph \"Evaluation Integration\"\n        WSM --\u003e BROWSE[\"BrowseComp Benchmarks\"]\n        WDM --\u003e GAIA2[\"GAIA Evaluation\u003cbr/\u003e64.1% Pass@3\"]\n        WDM --\u003e WWQA2[\"WebWalkerQA\u003cbr/\u003e62.0% Pass@3\"]\n    end\n```\n\nSources: [README.md:68-72](), [README.md:82-87]()\n\n## Data Structure and Quality Assurance\n\n### WebShaperQA Dataset Structure\n\nThe dataset produced by WebShaper follows a structured format designed for information-seeking agent training:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `id` | String | Unique identifier for each data instance |\n| `question` | String | Synthesized natural language question |\n| `formalization` | Object | Formal representation using KP structures |\n| `answer` | String | Ground truth answer for the question |\n| `urls` | Array | All URLs used for information retrieval |\n\n### Quality Control Mechanisms\n\nWebShaper implements several mechanisms to ensure data quality:\n\n1. **Formalization Validation**: Each generated question must align with established formal structures\n2. **Agentic Validation**: The Agentic Expander iteratively validates question coherence\n3. **Reasoning Complexity**: Layer-wise structure prevents reasoning shortcuts and ensures multi-step requirements\n4. **Information Distribution**: R-Union operations create challenging aggregation tasks across multiple sources\n\nSources: [WebShaper/readme.md:20-26](), [WebShaper/readme.md:62-69]()\n\n## Performance and Benchmarking\n\nWebShaper demonstrates significant improvements in information-seeking agent capabilities:\n\n### Benchmark Results\n\n| Benchmark | WebShaper Performance | Improvement |\n|-----------|----------------------|-------------|\n| **GAIA** | 60.19% | State-of-the-art |\n| **WebWalkerQA** | 52.50% | State-of-the-art |\n\n### Key Advantages\n\n1. **No Redundancy**: Layer-wise structure eliminates reasoning shortcuts\n2. **Diverse IS Forms**: Formalization enables varied information-seeking patterns\n3. **Distributed Reasoning**: R-Union operations require aggregation across multiple sources\n4. **Systematic Generation**: Formalization-first approach ensures consistency and quality\n\nSources: [README.md:68-72](), [WebShaper/readme.md:62-69]()\n\n## Technical Implementation\n\nWebShaper is implemented as a standalone component within the WebAgent framework, with dataset access available through multiple channels:\n\n- **HuggingFace**: `Alibaba-NLP/WebShaper` dataset\n- **ModelScope**: `iic/WebShaper` dataset  \n- **Local Access**: [WebShaper/data/webshaper.500.jsonl]()\n\nThe system integrates with the broader WebAgent training pipeline to support multiple downstream models including WebSailor and WebDancer variants.\n\nSources: [WebShaper/readme.md:15-18](), [README.md:12-16]()"])</script><script>self.__next_f.push([1,"17:T2068,"])</script><script>self.__next_f.push([1,"# Data Synthesis Pipeline\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebShaper/assets/case_study.png](WebShaper/assets/case_study.png)\n- [WebShaper/data/webshaper.500.jsonl](WebShaper/data/webshaper.500.jsonl)\n- [WebShaper/readme.md](WebShaper/readme.md)\n- [assets/roadmap.png](assets/roadmap.png)\n\n\u003c/details\u003e\n\n\n\nThis document details WebShaper's formalization-driven data synthesis methodology and agentic expander system. It covers the technical implementation of knowledge projections, the iterative generation and validation process, and the structured output format used for training information-seeking agents.\n\nFor overall WebShaper architecture details, see [2.2](#2.2). For the theoretical foundations of the formalization method, see [2.3](#2.3).\n\n## Formalization-Driven Methodology\n\nWebShaper employs a fundamentally different approach from traditional data synthesis methods. Rather than retrieving and organizing information first, WebShaper establishes task formalization before information collection, then synthesizes question-answer pairs based on this structured representation.\n\n```mermaid\ngraph TD\n    subgraph \"Traditional Methods\"\n        A1[\"Information Retrieval\"] --\u003e A2[\"Information Organization\"] \n        A2 --\u003e A3[\"Data Synthesis\"]\n    end\n    \n    subgraph \"WebShaper Approach\"\n        B1[\"Task Formalization\"] --\u003e B2[\"Information Collection\"]\n        B2 --\u003e B3[\"QA Data Synthesis\"]\n    end\n    \n    B1 --\u003e B4[\"Knowledge Projections\"]\n    B4 --\u003e B5[\"R-Union Operations\"]\n    B4 --\u003e B6[\"Intersection Operations\"]\n```\n\nThe core innovation lies in using knowledge projections (KPs) as the foundation for data synthesis. These projections represent sets of entities with two primary operations: R-Union for combining distributed information and intersection for finding common elements across knowledge domains.\n\nSources: [WebShaper/readme.md:36-44]()\n\n## Knowledge Projection System\n\nKnowledge projections form the structural backbone of WebShaper's formalization system. Each projection represents a set of entities that can be manipulated through formal operations.\n\n```mermaid\ngraph LR\n    subgraph \"Knowledge Projection Components\"\n        KP[\"Knowledge Projection\"] --\u003e VAR[\"Variables (V@X, V@Y, V@Z)\"]\n        KP --\u003e CONST[\"Constants (C@entity_name)\"]\n        KP --\u003e REL[\"Relationships (predicate connections)\"]\n    end\n    \n    subgraph \"Operations\"\n        RUNION[\"R-Union\"] --\u003e COMBINE[\"Combine distributed web content\"]\n        INTERSECT[\"Intersection\"] --\u003e COMMON[\"Find common entities\"]\n    end\n    \n    VAR --\u003e OPS[\"Operations\"]\n    CONST --\u003e OPS\n    REL --\u003e OPS\n    OPS --\u003e RUNION\n    OPS --\u003e INTERSECT\n```\n\nThe formalization uses a structured representation where variables are prefixed with `V@` and constants with `C@`. Relationships are expressed as predicates connecting these entities, creating a formal logical structure that can be systematically processed.\n\nSources: [WebShaper/readme.md:45-52](), [WebShaper/data/webshaper.500.jsonl:1-10]()\n\n## Agentic Expander Architecture\n\nThe agentic expander implements an iterative generation and validation process that ensures alignment with the established formalization. This component systematically generates questions while maintaining consistency with the knowledge projection structure.\n\n```mermaid\nflowchart TD\n    FORM[\"formalization_structure\"] --\u003e EXPAND[\"agentic_expander\"]\n    EXPAND --\u003e GEN[\"question_generation\"]\n    EXPAND --\u003e VAL[\"validation_process\"]\n    \n    GEN --\u003e ITER[\"iterative_refinement\"]\n    VAL --\u003e ITER\n    \n    ITER --\u003e CHECK{\"alignment_check\"}\n    CHECK --\u003e|\"not_aligned\"| GEN\n    CHECK --\u003e|\"aligned\"| OUTPUT[\"webshaper_dataset\"]\n    \n    subgraph \"Layer-wise Structure\"\n        LAYER1[\"Random Structure\"]\n        LAYER2[\"Sequential Structure\"] \n        LAYER3[\"Layer-wise Structure\"]\n    end\n    \n    EXPAND --\u003e LAYER3\n```\n\nThe expander employs three structural paradigms: random structure (adding constants randomly), sequential structure (chain-based reasoning), and layer-wise structure (traversing leaf constants systematically). The layer-wise approach proves most effective for generating complex reasoning chains without shortcuts.\n\nSources: [WebShaper/readme.md:53-70]()\n\n## Data Synthesis Pipeline Implementation\n\nThe synthesis pipeline transforms formalized structures into natural language questions with corresponding answers and supporting URLs. Each synthesized instance maintains strict correspondence to its underlying formalization.\n\n```mermaid\ngraph TD\n    subgraph \"Input Processing\"\n        FORM_INPUT[\"formalization_input\"] --\u003e PARSE[\"structure_parser\"]\n        PARSE --\u003e VAR_MAP[\"variable_mapping\"]\n        PARSE --\u003e REL_MAP[\"relationship_mapping\"]\n    end\n    \n    subgraph \"Generation Process\"\n        VAR_MAP --\u003e NLG[\"natural_language_generator\"]\n        REL_MAP --\u003e NLG\n        NLG --\u003e QUESTION[\"question_synthesis\"]\n        NLG --\u003e ANSWER[\"answer_extraction\"]\n    end\n    \n    subgraph \"Validation and Output\"\n        QUESTION --\u003e VALIDATE[\"consistency_validator\"]\n        ANSWER --\u003e VALIDATE\n        VALIDATE --\u003e FORMAT[\"output_formatter\"]\n        FORMAT --\u003e JSONL[\"webshaper_500_jsonl\"]\n    end\n    \n    INFO_COLLECT[\"information_collection\"] --\u003e ANSWER\n    INFO_COLLECT --\u003e URLS[\"url_compilation\"]\n    URLS --\u003e FORMAT\n```\n\nThe pipeline ensures that each generated question requires traversal through all variables in the formalization, preventing reasoning shortcuts and maintaining complexity. The R-Union operation effectively creates scenarios where information must be synthesized from distributed web sources.\n\nSources: [WebShaper/readme.md:61-70](), [WebShaper/data/webshaper.500.jsonl:1-500]()\n\n## Output Format and Data Structures\n\nWebShaper produces structured outputs in JSONL format, with each instance containing five key components that maintain traceability from formalization to natural language.\n\n```mermaid\ngraph LR\n    subgraph \"webshaper_500_jsonl Structure\"\n        INSTANCE[\"data_instance\"] --\u003e ID[\"id: uuid\"]\n        INSTANCE --\u003e QUESTION[\"question: natural_language\"]\n        INSTANCE --\u003e ANSWER[\"answer: target_value\"]\n        INSTANCE --\u003e FORMAL[\"formalization: list_structure\"]\n        INSTANCE --\u003e URLS[\"urls: source_list\"]\n    end\n    \n    subgraph \"Formalization Format\"\n        FORMAL --\u003e TRIPLETS[\"relationship_triplets\"]\n        TRIPLETS --\u003e VAR_CONST[\"[variable, predicate, constant]\"]\n        TRIPLETS --\u003e VAR_VAR[\"[variable, predicate, variable]\"]\n        TRIPLETS --\u003e TARGET[\"[variable, predicate, '?']\"]\n    end\n```\n\nEach formalization entry represents relationships as triplets, where the target variable marked with \"?\" indicates the information to be retrieved. This structure enables systematic evaluation of model reasoning capabilities across complex multi-hop information seeking scenarios.\n\n| Field | Format | Purpose |\n|-------|--------|---------|\n| `id` | UUID string | Unique instance identifier |\n| `question` | Natural language | Human-readable query |\n| `answer` | String/Number | Expected response |\n| `formalization` | List of triplets | Logical structure representation |\n| `urls` | List of URLs | Supporting information sources |\n\nThe formalization triplets follow the pattern `[subject, predicate, object]` where subjects and objects can be variables (`V@identifier`) or constants (`C@entity_name`), and predicates define the relationships between entities.\n\nSources: [WebShaper/data/webshaper.500.jsonl:1-10](), [WebShaper/readme.md:20-27]()\n\n## Performance and Validation Metrics\n\nWebShaper's formalization-driven approach demonstrates superior performance on benchmark evaluations, achieving state-of-the-art results on GAIA (60.19%) and WebWalkerQA (52.50%) benchmarks. The method's effectiveness stems from its systematic avoidance of reasoning shortcuts and requirement for comprehensive information synthesis.\n\nThe layer-wise structure expansion particularly excels at creating scenarios where models must strictly follow the reasoning chain through all intermediate variables, ensuring robust evaluation of information-seeking capabilities without redundancy or direct connections between constants and target variables.\n\nSources: [WebShaper/readme.md:13-14](), [WebShaper/readme.md:61-70]()"])</script><script>self.__next_f.push([1,"18:T32a4,"])</script><script>self.__next_f.push([1,"# Architecture and Implementation\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebShaper/assets/case.png](WebShaper/assets/case.png)\n- [WebShaper/assets/case_study.png](WebShaper/assets/case_study.png)\n- [WebShaper/assets/formalization.png](WebShaper/assets/formalization.png)\n- [WebShaper/assets/layer_wise.png](WebShaper/assets/layer_wise.png)\n- [WebShaper/assets/tongyi.png](WebShaper/assets/tongyi.png)\n- [WebShaper/readme.md](WebShaper/readme.md)\n\n\u003c/details\u003e\n\n\n\nThis page documents the core architecture and implementation details of WebSailor, focusing on the `MultiTurnReactAgent` system, tool integration, and execution pipeline. For training methodology details, see [Training Pipeline](#3.1). For tool-specific implementations, see [Tools and Components](#3.3).\n\n## Core Architecture\n\nWebSailor's architecture centers around the `MultiTurnReactAgent` class, which orchestrates web information seeking through a ReAct (Reasoning + Acting) framework. The system operates as a multi-turn conversation agent that can call tools dynamically based on user queries.\n\n### System Components\n\n```mermaid\ngraph TB\n    subgraph \"WebSailor Agent System\"\n        MTRA[\"MultiTurnReactAgent\"]\n        FCALL[\"FnCallAgent (Base)\"]\n        OPENAI[\"OpenAI Client\"]\n    end\n    \n    subgraph \"Dual SGLang Server Architecture\"\n        SGLANG1[\"SGLang Server Port 6001\u003cbr/\u003e(Original Model)\"]\n        SGLANG2[\"SGLang Server Port 6002\u003cbr/\u003e(Summary Model)\"]\n        HEALTHCHECK[\"Health Check Logic\"]\n    end\n    \n    subgraph \"Tool Ecosystem\"\n        SEARCH[\"search tool\"]\n        VISIT[\"visit tool\"]\n        TOOLCALL[\"_call_tool method\"]\n    end\n    \n    subgraph \"Execution Pipeline\"\n        RUNSH[\"run.sh\"]\n        RUNSCRIPT[\"run_multi_react.py\"]\n        THREADPOOL[\"ThreadPoolExecutor\"]\n        EVALUATE[\"evaluate.py\"]\n    end\n    \n    MTRA --\u003e FCALL\n    MTRA --\u003e OPENAI\n    MTRA --\u003e TOOLCALL\n    OPENAI --\u003e SGLANG1\n    TOOLCALL --\u003e SEARCH\n    TOOLCALL --\u003e VISIT\n    \n    RUNSH --\u003e SGLANG1\n    RUNSH --\u003e SGLANG2\n    RUNSH --\u003e HEALTHCHECK\n    RUNSH --\u003e RUNSCRIPT\n    RUNSCRIPT --\u003e MTRA\n    RUNSCRIPT --\u003e THREADPOOL\n    RUNSCRIPT --\u003e EVALUATE\n```\n\n**Sources:** [WebSailor/src/react_agent.py:1-163](), [WebSailor/src/run_multi_react.py:1-189](), [WebSailor/src/run.sh:32-43]()\n\n## Agent Implementation\n\n### MultiTurnReactAgent Class\n\nThe `MultiTurnReactAgent` extends `FnCallAgent` and implements the core conversation logic with tool calling capabilities:\n\n```mermaid\ngraph TD\n    subgraph \"MultiTurnReactAgent Lifecycle\"\n        INIT[\"__init__\"]\n        RUN[\"_run method\"]\n        CALLSERVER[\"call_server\"]\n        COUNTTOKENS[\"count_tokens\"]\n        CALLTOOL[\"_call_tool\"]\n    end\n    \n    subgraph \"Configuration\"\n        LLMCFG[\"llm_generate_cfg\"]\n        LLMPATH[\"llm_local_path\"]\n        SYSMSG[\"system_message\"]\n        FUNCLIST[\"function_list\"]\n        MODEL[\"self.model\"]\n    end\n    \n    subgraph \"Processing Loop\"\n        MSGLOOP[\"Message Loop\"]\n        TOOLPARSE[\"Tool Call Parsing\"]\n        TOKENCHECK[\"Token Limit Check\"]\n        TERMCHECK[\"Termination Check\"]\n        ANSWERCHECK[\"Answer Extraction\"]\n    end\n    \n    subgraph \"Server Communication\"\n        OPENAI_CLIENT[\"OpenAI Client\"]\n        API_BASE[\"http://127.0.0.1:6001/v1\"]\n        RETRY_LOGIC[\"max_tries=10\"]\n    end\n    \n    INIT --\u003e LLMCFG\n    INIT --\u003e LLMPATH\n    INIT --\u003e SYSMSG\n    INIT --\u003e FUNCLIST\n    INIT --\u003e MODEL\n    \n    RUN --\u003e CALLSERVER\n    RUN --\u003e COUNTTOKENS\n    RUN --\u003e CALLTOOL\n    \n    CALLSERVER --\u003e OPENAI_CLIENT\n    CALLSERVER --\u003e API_BASE\n    CALLSERVER --\u003e RETRY_LOGIC\n    \n    RUN --\u003e MSGLOOP\n    MSGLOOP --\u003e TOOLPARSE\n    MSGLOOP --\u003e TOKENCHECK\n    MSGLOOP --\u003e TERMCHECK\n    MSGLOOP --\u003e ANSWERCHECK\n```\n\n**Sources:** [WebSailor/src/react_agent.py:20-38](), [WebSailor/src/react_agent.py:80-162](), [WebSailor/src/react_agent.py:39-68]()\n\n### Key Implementation Details\n\n| Component | Implementation | Purpose |\n|-----------|----------------|---------|\n| `call_server` | OpenAI API via SGLang | Model inference with retry logic |\n| `count_tokens` | AutoTokenizer/tiktoken | Context length management |\n| `_call_tool` | JSON parsing + tool dispatch | Dynamic tool execution |\n| Message loop | While loop with limits | Multi-turn conversation control |\n\nThe agent maintains a maximum of `MAX_LLM_CALL_PER_RUN` calls (default 40) and `MAX_TOKEN_LENGTH` tokens (31KB - 500) per conversation.\n\n**Sources:** [WebSailor/src/react_agent.py:15-17](), [WebSailor/src/react_agent.py:39-68](), [WebSailor/src/react_agent.py:69-78]()\n\n## Tool Integration\n\n### Tool Call Mechanism\n\nWebSailor implements a structured tool calling system using JSON-formatted tool specifications:\n\n```mermaid\ngraph LR\n    subgraph \"Tool Call Flow\"\n        CONTENT[\"Assistant Content\"]\n        PARSE[\"JSON Parsing\"]\n        DISPATCH[\"Tool Dispatch\"]\n        RESPONSE[\"Tool Response\"]\n    end\n    \n    subgraph \"Tool Specifications\"\n        SEARCH_SPEC[\"search tool spec\"]\n        VISIT_SPEC[\"visit tool spec\"]\n    end\n    \n    subgraph \"Tool Execution\"\n        SEARCH_IMPL[\"search implementation\"]\n        VISIT_IMPL[\"visit implementation\"]\n        BATCH_EXEC[\"Batch Execution\"]\n    end\n    \n    CONTENT --\u003e PARSE\n    PARSE --\u003e DISPATCH\n    DISPATCH --\u003e SEARCH_IMPL\n    DISPATCH --\u003e VISIT_IMPL\n    SEARCH_IMPL --\u003e RESPONSE\n    VISIT_IMPL --\u003e RESPONSE\n    \n    SEARCH_SPEC --\u003e SEARCH_IMPL\n    VISIT_SPEC --\u003e VISIT_IMPL\n    SEARCH_IMPL --\u003e BATCH_EXEC\n    VISIT_IMPL --\u003e BATCH_EXEC\n```\n\n**Sources:** [WebSailor/src/react_agent.py:103-113](), [WebSailor/src/prompt.py:29-72]()\n\n### Tool Specifications\n\nThe system defines two primary tools through structured JSON specifications:\n\n1. **Search Tool**: Performs batched web searches with query arrays\n2. **Visit Tool**: Visits webpages with goal-oriented content extraction\n\nTool calls are parsed from assistant responses between `\u003ctool_call\u003e` and `\u003c/tool_call\u003e` tags, with responses wrapped in `\u003ctool_response\u003e` tags.\n\n**Sources:** [WebSailor/src/prompt.py:31-49](), [WebSailor/src/prompt.py:50-71]()\n\n## Execution Pipeline\n\n### Multi-Rollout Processing\n\nThe execution pipeline supports multiple rollouts with concurrent processing and dual server orchestration:\n\n```mermaid\ngraph TB\n    subgraph \"run.sh Orchestration\"\n        START_ORIG[\"Start Original Model\u003cbr/\u003ePort 6001\"]\n        START_SUM[\"Start Summary Model\u003cbr/\u003ePort 6002\"]\n        HEALTH_CHECK[\"Health Check Loop\u003cbr/\u003ecurl endpoints\"]\n        INFERENCE[\"Start Inference\"]\n        EVALUATION[\"Start Evaluation\"]\n    end\n    \n    subgraph \"run_multi_react.py Flow\"\n        ARGPARSE[\"Argument Parsing\"]\n        DATALOAD[\"Data Loading\u003cbr/\u003eeval_data/*.jsonl\"]\n        ROLLOUT[\"Rollout Loop\u003cbr/\u003e(roll_out_count=3)\"]\n        TASKGEN[\"Task Generation\"]\n        THREADPOOL[\"ThreadPoolExecutor\u003cbr/\u003e(max_workers=20)\"]\n        RESULTS[\"Result Collection\"]\n    end\n    \n    subgraph \"Task Processing\"\n        AGENT[\"MultiTurnReactAgent\"]\n        CONCURRENT[\"as_completed()\"]\n        WRITELOCK[\"threading.Lock()\"]\n        FILEWRITE[\"JSONL Output\"]\n        ERRORHANDLE[\"Exception Handling\"]\n    end\n    \n    START_ORIG --\u003e HEALTH_CHECK\n    START_SUM --\u003e HEALTH_CHECK\n    HEALTH_CHECK --\u003e INFERENCE\n    INFERENCE --\u003e ARGPARSE\n    \n    ARGPARSE --\u003e DATALOAD\n    DATALOAD --\u003e ROLLOUT\n    ROLLOUT --\u003e TASKGEN\n    TASKGEN --\u003e THREADPOOL\n    THREADPOOL --\u003e CONCURRENT\n    CONCURRENT --\u003e AGENT\n    AGENT --\u003e RESULTS\n    RESULTS --\u003e WRITELOCK\n    WRITELOCK --\u003e FILEWRITE\n    \n    CONCURRENT --\u003e ERRORHANDLE\n    INFERENCE --\u003e EVALUATION\n```\n\n**Sources:** [WebSailor/src/run_multi_react.py:14-189](), [WebSailor/src/run.sh:32-96](), [WebSailor/src/run.sh:114-126]()\n\n### Configuration Parameters\n\n| Parameter | Default | Purpose | Source |\n|-----------|---------|---------|---------|\n| `max_workers` | 20 | Concurrent processing threads | run_multi_react.py |\n| `roll_out_count` | 3 | Number of inference rollouts | run_multi_react.py |\n| `temperature` | 0.6 | LLM generation temperature | run.sh |\n| `top_p` | 0.95 | LLM generation top-p | run_multi_react.py |\n| `max_input_tokens` | 320000 | Maximum input context | run_multi_react.py |\n| `max_retries` | 10 | API retry attempts | run_multi_react.py |\n| `MAX_LLM_CALL_PER_RUN` | 40 | Maximum LLM calls per conversation | react_agent.py |\n| `MAX_TOKEN_LENGTH` | 31744 | Maximum token context (31KB - 500) | react_agent.py |\n| `timeout` | 3000 | Server startup timeout (seconds) | run.sh |\n\n**Sources:** [WebSailor/src/run_multi_react.py:26-31](), [WebSailor/src/run_multi_react.py:124-133](), [WebSailor/src/react_agent.py:15-17](), [WebSailor/src/run.sh:49-78]()\n\n## Server Architecture\n\n### Dual SGLang Server Setup\n\nWebSailor employs a dual server architecture for different model purposes:\n\n```mermaid\ngraph TB\n    subgraph \"Server Infrastructure\"\n        ORIG_SERVER[\"Original Model Server\u003cbr/\u003ePort 6001\u003cbr/\u003eCUDA_VISIBLE_DEVICES=0,1,2,3\u003cbr/\u003etp=2\"]\n        SUM_SERVER[\"Summary Model Server\u003cbr/\u003ePort 6002\u003cbr/\u003eCUDA_VISIBLE_DEVICES=4,5,6,7\u003cbr/\u003etp=4\"]\n        HEALTH_MONITOR[\"Health Check Monitor\u003cbr/\u003ecurl endpoints\"]\n    end\n    \n    subgraph \"Model Deployment\"\n        MODEL_PATH[\"$MODEL_PATH\u003cbr/\u003e(Evaluation Model)\"]\n        SUMMARY_MODEL[\"$SUMMARY_MODEL_PATH\u003cbr/\u003e(Summary Model)\"]\n        SGLANG_LAUNCH[\"sglang.launch_server\"]\n    end\n    \n    subgraph \"Agent Integration\"\n        OPENAI_CLIENT[\"OpenAI Client\"]\n        API_BASE_6001[\"http://127.0.0.1:6001/v1\"]\n        API_BASE_6002[\"http://127.0.0.1:6002/v1\"]\n        MULTITURNAGENT[\"MultiTurnReactAgent\"]\n    end\n    \n    MODEL_PATH --\u003e ORIG_SERVER\n    SUMMARY_MODEL --\u003e SUM_SERVER\n    SGLANG_LAUNCH --\u003e ORIG_SERVER\n    SGLANG_LAUNCH --\u003e SUM_SERVER\n    \n    HEALTH_MONITOR --\u003e ORIG_SERVER\n    HEALTH_MONITOR --\u003e SUM_SERVER\n    \n    OPENAI_CLIENT --\u003e API_BASE_6001\n    OPENAI_CLIENT --\u003e API_BASE_6002\n    MULTITURNAGENT --\u003e OPENAI_CLIENT\n```\n\n**Sources:** [WebSailor/src/run.sh:32-43](), [WebSailor/src/run.sh:49-96](), [WebSailor/src/react_agent.py:39-47]()\n\n## Prompt Engineering\n\n### System Architecture\n\nWebSailor employs a structured prompt system with dynamic date injection:\n\n```mermaid\ngraph TB\n    subgraph \"Prompt Construction\"\n        SYSTEM_BASE[\"SYSTEM_PROMPT_MULTI\"]\n        DATE_INJECT[\"Current date injection\u003cbr/\u003edatetime.now().strftime()\"]\n        USER_BASE[\"USER_PROMPT\"]\n        QUESTION_APPEND[\"Question appending\"]\n    end\n    \n    subgraph \"Tool Integration\"\n        FUNCTION_LIST[\"function_list=['search', 'visit']\"]\n        TOOL_SPECS[\"Tool JSON specifications\"]\n        TOOL_EXAMPLES[\"Tool usage examples\"]\n    end\n    \n    subgraph \"Response Structure\"\n        THINK[\"\u003cthink\u003e reasoning tags\"]\n        TOOLCALL[\"\u003ctool_call\u003e JSON tags\"]\n        TOOLRESP[\"\u003ctool_response\u003e result tags\"]\n        ANSWER[\"\u003canswer\u003e final tags\"]\n    end\n    \n    SYSTEM_BASE --\u003e DATE_INJECT\n    USER_BASE --\u003e QUESTION_APPEND\n    \n    FUNCTION_LIST --\u003e TOOL_SPECS\n    TOOL_SPECS --\u003e TOOL_EXAMPLES\n    \n    TOOL_EXAMPLES --\u003e THINK\n    TOOL_EXAMPLES --\u003e TOOLCALL\n    TOOL_EXAMPLES --\u003e TOOLRESP\n    TOOL_EXAMPLES --\u003e ANSWER\n```\n\n**Sources:** [WebSailor/src/run_multi_react.py:135-141](), [WebSailor/src/react_agent.py:89-91]()\n\n### Response Format\n\nThe system enforces structured responses with specific XML-like tags:\n\n- `\u003cthink\u003e`: Reasoning process\n- `\u003ctool_call\u003e`: JSON-formatted tool invocation\n- `\u003ctool_response\u003e`: Tool execution results\n- `\u003canswer\u003e`: Final answer delivery\n\nThis structure enables reliable parsing and execution control throughout the multi-turn conversation.\n\n**Sources:** [WebSailor/src/prompt.py:74-95](), [WebSailor/src/react_agent.py:114-116]()\n\n## Context Management\n\n### Token Limit Handling\n\nThe system implements sophisticated context management to handle token limits:\n\n```mermaid\ngraph TD\n    subgraph \"Context Management Flow\"\n        TOKENCOUNT[\"count_tokens\"]\n        LIMITCHECK[\"Token Limit Check\"]\n        TRUNCATE[\"Context Truncation\"]\n        FINALGEN[\"Final Generation\"]\n    end\n    \n    subgraph \"Limits and Thresholds\"\n        MAXTOKEN[\"MAX_TOKEN_LENGTH\"]\n        MAXCALLS[\"MAX_LLM_CALL_PER_RUN\"]\n        AUTOTOKENIZER[\"AutoTokenizer\"]\n    end\n    \n    subgraph \"Recovery Mechanisms\"\n        FORCEANSWER[\"Force Answer Generation\"]\n        ERRORHANDLING[\"Error Handling\"]\n        TERMINATION[\"Termination Logic\"]\n    end\n    \n    TOKENCOUNT --\u003e LIMITCHECK\n    LIMITCHECK --\u003e TRUNCATE\n    TRUNCATE --\u003e FINALGEN\n    \n    MAXTOKEN --\u003e LIMITCHECK\n    MAXCALLS --\u003e LIMITCHECK\n    AUTOTOKENIZER --\u003e TOKENCOUNT\n    \n    LIMITCHECK --\u003e FORCEANSWER\n    FORCEANSWER --\u003e ERRORHANDLING\n    ERRORHANDLING --\u003e TERMINATION\n```\n\n**Sources:** [WebSailor/src/react_agent.py:69-78](), [WebSailor/src/react_agent.py:120-144](), [WebSailor/src/react_agent.py:15-17]()\n\nWhen token limits are exceeded, the system injects a special message prompting the agent to provide a final answer based on accumulated information, ensuring graceful degradation rather than failure.\n\n**Sources:** [WebSailor/src/react_agent.py:127-128]()"])</script><script>self.__next_f.push([1,"19:T1f60,"])</script><script>self.__next_f.push([1,"# Formalization Method\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebShaper/assets/webagent-gaia.png](WebShaper/assets/webagent-gaia.png)\n- [WebShaper/assets/webshaper.png](WebShaper/assets/webshaper.png)\n- [WebShaper/data/webshaper.500.jsonl](WebShaper/data/webshaper.500.jsonl)\n- [assets/roadmap.png](assets/roadmap.png)\n\n\u003c/details\u003e\n\n\n\nThis document covers WebShaper's core formalization approach for converting natural language information-seeking tasks into structured logical representations. The formalization method enables systematic decomposition of complex queries into machine-processable formats that support automated reasoning and answer extraction.\n\nFor information about WebShaper's overall data synthesis pipeline, see [Data Synthesis Pipeline](#2.1). For implementation details of WebShaper's architecture, see [Architecture and Implementation](#2.2).\n\n## Overview\n\nWebShaper's formalization method transforms natural language questions into structured logical representations using a graph-based approach. This process enables the systematic decomposition of complex multi-hop reasoning tasks into atomic facts and relationships that can be efficiently processed by language models.\n\nThe formalization serves as an intermediate representation that bridges the gap between unstructured natural language and the structured knowledge graphs needed for reliable information retrieval and reasoning.\n\n## Formalization Structure\n\n### Core Components\n\nThe formalization method uses a triplet-based representation where each fact is expressed as `[subject, predicate, object]` tuples. The system employs a systematic variable and constant naming convention:\n\n| Element Type | Format | Example | Purpose |\n|--------------|--------|---------|---------|\n| Variables | `V@{identifier}` | `V@X`, `V@Person`, `V@Event` | Represent entities to be resolved |\n| Constants | `C@{value}` | `C@1985`, `C@Berlin`, `C@football_club` | Represent known literal values |\n| Target Query | `?` | `?` | Marks the information being sought |\n\n### Data Format\n\nEach formalized task contains:\n\n```json\n{\n  \"id\": \"unique_identifier\",\n  \"question\": \"natural_language_question\", \n  \"answer\": \"expected_answer\",\n  \"formalization\": [\n    [\"subject\", \"predicate\", \"object\"],\n    // Additional triplets...\n  ],\n  \"urls\": [\"source_urls\"]\n}\n```\n\nSources: [WebShaper/data/webshaper.500.jsonl:1-10]()\n\n## Formalization Process Flow\n\n```mermaid\ngraph TD\n    A[\"Natural Language Question\"] --\u003e B[\"Entity Identification\"]\n    B --\u003e C[\"Relationship Extraction\"] \n    C --\u003e D[\"Variable Assignment\"]\n    D --\u003e E[\"Constraint Formalization\"]\n    E --\u003e F[\"Target Query Specification\"]\n    F --\u003e G[\"Structured Representation\"]\n    \n    H[\"Domain Knowledge\"] --\u003e C\n    I[\"Temporal Constraints\"] --\u003e E\n    J[\"Spatial Constraints\"] --\u003e E\n    \n    G --\u003e K[\"WebShaperQA Dataset\"]\n    K --\u003e L[\"Model Training\"]\n```\n\nThis diagram shows how natural language questions are systematically transformed into structured logical representations through entity identification, relationship extraction, and constraint formalization.\n\nSources: [WebShaper/data/webshaper.500.jsonl:1-500]()\n\n## Variable and Constant Conventions\n\n### Variable Types\n\nThe system uses semantically meaningful variable identifiers:\n\n- **Generic Variables**: `V@X`, `V@Y`, `V@Z` for abstract entities\n- **Typed Variables**: `V@Person`, `V@Event`, `V@Location` for specific entity types  \n- **Suffixed Variables**: `V@Person_body`, `V@X1`, `V@Y2` for related or multiple instances\n\n### Constant Encoding\n\nConstants represent literal values and are encoded with type-specific prefixes:\n\n- **Temporal**: `C@1985`, `C@October 2019`, `C@16 August 1986`\n- **Numerical**: `C@10`, `C@500_dollars`, `C@160km`\n- **Categorical**: `C@football_club`, `C@sports_complex`, `C@Berlin`\n- **Complex Types**: `C@Asian-American_protagonist_without_stereotypes`\n\nSources: [WebShaper/data/webshaper.500.jsonl:1-20]()\n\n## Relationship Modeling\n\n### Predicate Categories\n\nThe formalization employs diverse predicate types to capture different relationship semantics:\n\n```mermaid\ngraph LR\n    A[\"Predicates\"] --\u003e B[\"Temporal\"]\n    A --\u003e C[\"Spatial\"] \n    A --\u003e D[\"Causal\"]\n    A --\u003e E[\"Hierarchical\"]\n    A --\u003e F[\"Attributive\"]\n    \n    B --\u003e B1[\"occurred_in\"]\n    B --\u003e B2[\"took_place_on\"]\n    B --\u003e B3[\"achieved_between_years\"]\n    \n    C --\u003e C1[\"is_located_in\"]\n    C --\u003e C2[\"hosted_opening_match_of\"]\n    C --\u003e C3[\"is_based_in\"]\n    \n    D --\u003e D1[\"was_terminated_by\"]\n    D --\u003e D2[\"resulted_in_death_of\"]\n    D --\u003e D3[\"led_to_conviction_of\"]\n    \n    E --\u003e E1[\"is_part_of\"]\n    E --\u003e E2[\"led_gang_that_committed\"]\n    E --\u003e E3[\"is_first_volume_of\"]\n    \n    F --\u003e F1[\"has_record_for_most\"]\n    F --\u003e F2[\"has_type\"]\n    F --\u003e F3[\"had_number_of_spectators\"]\n```\n\nThis taxonomy enables precise modeling of different relationship types found in complex information-seeking tasks.\n\nSources: [WebShaper/data/webshaper.500.jsonl:1-50]()\n\n## Query Target Specification\n\n### Answer Marking\n\nThe target information is marked using the special `?` symbol in the object position of relevant triplets:\n\n```json\n[\"V@M\", \"had_number_of_spectators\", \"?\"]\n[\"V@X\", \"has_name\", \"?\"] \n[\"V@Book\", \"has_number_of_pages\", \"?\"]\n```\n\n### Multi-Variable Queries\n\nComplex queries may involve multiple interconnected variables with a single target:\n\n```json\n[\n  [\"V@X\", \"is_time_in_seconds_and_is\", \"?\"],\n  [\"V@Y\", \"is_behind_V@Z_by\", \"V@X\"],\n  [\"V@Y\", \"in\", \"V@W\"],\n  [\"V@W\", \"is_general_classification_after\", \"V@S\"]\n]\n```\n\nSources: [WebShaper/data/webshaper.500.jsonl:1-10]()\n\n## Implementation Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Input Processing\"\n        A[\"Natural Language Question\"]\n        B[\"Entity Extractor\"]\n        C[\"Relation Classifier\"]\n    end\n    \n    subgraph \"Formalization Engine\"\n        D[\"Variable_Assigner\"]\n        E[\"Triplet_Generator\"] \n        F[\"Constraint_Resolver\"]\n        G[\"Target_Identifier\"]\n    end\n    \n    subgraph \"Output Generation\"\n        H[\"Structured_Representation\"]\n        I[\"Validation_Engine\"]\n        J[\"webshaper_dataset\"]\n    end\n    \n    A --\u003e B\n    A --\u003e C\n    B --\u003e D\n    C --\u003e E\n    D --\u003e E\n    E --\u003e F\n    F --\u003e G\n    G --\u003e H\n    H --\u003e I\n    I --\u003e J\n    \n    K[\"Knowledge_Base\"] --\u003e F\n    L[\"Ontology_Rules\"] --\u003e I\n```\n\nThe formalization engine processes natural language inputs through multiple stages, applying domain knowledge and validation rules to produce consistent structured representations.\n\nSources: [WebShaper/data/webshaper.500.jsonl:1-500]()\n\n## Complexity Handling\n\n### Multi-Hop Reasoning\n\nThe formalization method handles complex multi-hop reasoning by chaining relationships across multiple variables:\n\n```json\n[\n  [\"V@Y\", \"holds_record_for_most\", \"C@Tour_de_France_participations\"],\n  [\"V@Y\", \"is_nationality\", \"V@Nat\"], \n  [\"V@Y\", \"rode_for\", \"V@P\"],\n  [\"V@M\", \"rebranded_to\", \"V@P\"],\n  [\"V@N\", \"is_winner_of\", \"C@2008_Tour_de_France\"],\n  [\"V@N\", \"rode_for\", \"V@M\"]\n]\n```\n\n### Temporal Constraints\n\nTemporal relationships are explicitly modeled to maintain chronological consistency:\n\n```json\n[\n  [\"V@Event\", \"occurred_in\", \"C@October_2019\"],\n  [\"V@Area\", \"was_designated_as_conservation_area_in\", \"C@1987\"],\n  [\"V@W\", \"occurred_before\", \"V@X1\"]\n]\n```\n\nSources: [WebShaper/data/webshaper.500.jsonl:2-3]()\n\n## Validation and Quality Control\n\n### Consistency Checking\n\nThe formalization process includes validation mechanisms to ensure:\n\n- **Variable Consistency**: Each variable maintains consistent type and role\n- **Relationship Validity**: Predicates align with subject/object types  \n- **Target Specification**: Exactly one query target per formalization\n- **Source Alignment**: Formalization matches provided answer and URLs\n\n### Error Patterns\n\nCommon formalization errors include:\n- Ambiguous variable references\n- Missing temporal or spatial constraints\n- Incomplete relationship chains\n- Inconsistent entity typing\n\nSources: [WebShaper/data/webshaper.500.jsonl:1-500]()"])</script><script>self.__next_f.push([1,"1a:T2a69,"])</script><script>self.__next_f.push([1,"# WebDancer\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [WebDancer/readme.md](WebDancer/readme.md)\n- [WebDancer/requirements.txt](WebDancer/requirements.txt)\n- [WebDancer/scripts/deploy_model.sh](WebDancer/scripts/deploy_model.sh)\n- [assets/aliyun.png](assets/aliyun.png)\n\n\u003c/details\u003e\n\n\n\nThis document covers WebDancer, an autonomous information seeking agent that serves as one of the five core components in the WebAgent ecosystem. WebDancer focuses on native agentic search capabilities using the ReAct framework to enable autonomous web information seeking and complex reasoning tasks.\n\nFor information about data synthesis methodologies, see [WebShaper](#2). For complex reasoning and post-training approaches, see [WebSailor](#4). For benchmarking frameworks, see [WebWalker](#5.2).\n\n## System Overview\n\nWebDancer represents a novel end-to-end agentic training framework designed to enhance multi-step information-seeking capabilities of web-based agents. Unlike traditional web agents that rely on external orchestration, WebDancer integrates search, reasoning, and action capabilities into a single model through a comprehensive four-stage training paradigm.\n\nThe system is built on the ReAct (Reasoning and Acting) framework, enabling the model to iteratively reason about information needs, take actions to gather information, and synthesize results to answer complex queries autonomously.\n\n```mermaid\ngraph TB\n    subgraph \"WebAgent Ecosystem\"\n        WS[\"WebShaper\u003cbr/\u003eData Synthesis\"]\n        WSA[\"WebSailor\u003cbr/\u003eComplex Reasoning\"]\n        WD[\"WebDancer\u003cbr/\u003eAutonomous Search\"]\n        WW[\"WebWalker\u003cbr/\u003eBenchmarking\"]\n        WWA[\"WebWatcher\u003cbr/\u003eVision-Language\"]\n    end\n    \n    subgraph \"WebDancer Core\"\n        WD --\u003e REACT[\"ReAct Framework\"]\n        WD --\u003e TRAINING[\"Four-Stage Training\"]\n        WD --\u003e MODEL[\"WebDancer-32B Model\"]\n    end\n    \n    subgraph \"External Dependencies\"\n        SGLANG[\"sglang[all]\"]\n        QWEN[\"qwen-agent[gui,rag,code_interpreter,mcp]\"]\n    end\n    \n    subgraph \"Deployment Stack\"\n        MODEL --\u003e SGLANG\n        SGLANG --\u003e GRADIO[\"Gradio Demo Interface\"]\n        MODEL --\u003e QWEN\n    end\n    \n    subgraph \"Evaluation Benchmarks\"\n        GAIA[\"GAIA: 64.1% Pass@3\"]\n        WWQA[\"WebWalkerQA: 62.0% Pass@3\"]\n    end\n    \n    WD --\u003e GAIA\n    WD --\u003e WWQA\n```\n\n**WebDancer System Architecture**\n\nSources: [README.md:39-40](), [README.md:82-88](), [WebDancer/readme.md:9-14](), [WebDancer/requirements.txt:1-2]()\n\n## Core Architecture\n\nWebDancer implements a ReAct-based architecture that enables autonomous information seeking through iterative reasoning and action cycles. The system integrates multiple components to support complex, multi-step information gathering and synthesis.\n\n```mermaid\ngraph TD\n    subgraph \"User Interface Layer\"\n        USER[\"User Query Input\"]\n        GRADIO[\"gradio_demo.py\"]\n        CLI[\"Command Line Interface\"]\n    end\n    \n    subgraph \"WebDancer Core Model\"\n        MODEL[\"WebDancer-32B Model\"]\n        REACT[\"ReAct Processing Engine\"]\n        REASONING[\"Reasoning Component\"]\n        ACTION[\"Action Planning\"]\n    end\n    \n    subgraph \"Tool Integration Layer\"\n        SEARCH[\"Search Tool\"]\n        VISIT[\"Visit Tool\"]\n        SERPER[\"Google Serper API\"]\n        JINA[\"Jina Reader API\"]\n    end\n    \n    subgraph \"Model Serving Infrastructure\"\n        SGLANG[\"SGLang Server\"]\n        DEPLOY[\"deploy_model.sh\"]\n        CONFIG[\"Model Configuration\"]\n    end\n    \n    subgraph \"Training Data Sources\"\n        QA_DATA[\"sample_qa.jsonl\"]\n        TRAJ_DATA[\"sample_traj.jsonl\"]\n        BROWSER_DATA[\"Browsing Data Construction\"]\n    end\n    \n    USER --\u003e GRADIO\n    USER --\u003e CLI\n    GRADIO --\u003e MODEL\n    CLI --\u003e MODEL\n    \n    MODEL --\u003e REACT\n    REACT --\u003e REASONING\n    REACT --\u003e ACTION\n    \n    ACTION --\u003e SEARCH\n    ACTION --\u003e VISIT\n    SEARCH --\u003e SERPER\n    VISIT --\u003e JINA\n    \n    MODEL --\u003e SGLANG\n    DEPLOY --\u003e SGLANG\n    SGLANG --\u003e CONFIG\n    \n    QA_DATA --\u003e MODEL\n    TRAJ_DATA --\u003e MODEL\n    BROWSER_DATA --\u003e MODEL\n```\n\n**WebDancer Core Architecture and Component Integration**\n\nThe architecture supports autonomous operation through several key mechanisms:\n\n| Component | Purpose | Implementation |\n|-----------|---------|----------------|\n| `ReAct Framework` | Iterative reasoning and action cycles | Integrated into model training and inference |\n| `Search Tool` | Web search capabilities | Google Serper API integration |\n| `Visit Tool` | Web content retrieval | Jina Reader API for content extraction |\n| `SGLang Server` | Model serving infrastructure | High-performance inference serving |\n| `Gradio Interface` | User interaction layer | Web-based demo interface |\n\nSources: [WebDancer/readme.md:11-14](), [README.md:114-118](), [WebDancer/scripts/deploy_model.sh:1-4](), [WebDancer/requirements.txt:1-2]()\n\n## Training Methodology\n\nWebDancer employs a comprehensive four-stage training paradigm that progresses from data construction through reinforcement learning, enabling the development of robust autonomous search and reasoning capabilities.\n\n```mermaid\ngraph TD\n    subgraph \"Stage 1: Browsing Data Construction\"\n        RAW_QA[\"Raw QA Data\"]\n        QA_SAMPLE[\"sample_qa.jsonl\"]\n        DATA_CONSTRUCT[\"Data Construction Pipeline\"]\n    end\n    \n    subgraph \"Stage 2: Trajectory Sampling\"\n        QA_SAMPLE --\u003e TRAJ_SAMPLE[\"Trajectory Sampling\"]\n        TRAJ_SAMPLE --\u003e TRAJ_DATA[\"sample_traj.jsonl\"]\n        TRAJ_DATA --\u003e EXPERT_TRAJ[\"Expert Trajectories\"]\n    end\n    \n    subgraph \"Stage 3: Supervised Fine-Tuning\"\n        EXPERT_TRAJ --\u003e SFT[\"Supervised Fine-Tuning\"]\n        SFT --\u003e LLAMAFACTORY[\"LLaMA-Factory Integration\"]\n        LLAMAFACTORY --\u003e COLD_START[\"Effective Cold Start\"]\n    end\n    \n    subgraph \"Stage 4: Reinforcement Learning\"\n        COLD_START --\u003e RL[\"Reinforcement Learning\"]\n        RL --\u003e VERL[\"Modified VERL Framework\"]\n        VERL --\u003e DAPO[\"DAPO Algorithm\"]\n        DAPO --\u003e FINAL_MODEL[\"WebDancer-32B Model\"]\n    end\n    \n    subgraph \"Training Infrastructure\"\n        LLAMAFACTORY --\u003e TRAINING_SCRIPTS[\"Training Scripts\"]\n        VERL --\u003e RL_TRAINING[\"RL Training Pipeline\"]\n    end\n    \n    RAW_QA --\u003e DATA_CONSTRUCT\n    DATA_CONSTRUCT --\u003e QA_SAMPLE\n```\n\n**Four-Stage Training Pipeline Architecture**\n\n### Stage Details\n\n**Stage 1: Browsing Data Construction**\n- Constructs web browsing data from question-answer pairs\n- Sample data available in `sample_qa.jsonl`\n- Focuses on multi-step information seeking scenarios\n\n**Stage 2: Trajectory Sampling**\n- Generates expert trajectories for complex reasoning tasks\n- Output stored in `sample_traj.jsonl`\n- Captures ReAct-style reasoning and action sequences\n\n**Stage 3: Supervised Fine-Tuning**\n- Implements effective cold start using trajectory-level supervision\n- Integrates with LLaMA-Factory for training infrastructure\n- Establishes baseline autonomous capabilities\n\n**Stage 4: Reinforcement Learning**\n- Uses modified VERL framework for advanced training\n- Implements DAPO (Duplicating Sampling Policy Optimization) algorithm\n- Enhances generalization and autonomous decision-making\n\nSources: [WebDancer/readme.md:79-106](), [WebDancer/readme.md:87-93](), [README.md:82-87]()\n\n## Deployment and Usage\n\nWebDancer deployment follows a standardized pipeline using SGLang for model serving and Gradio for user interaction. The system requires specific API keys for external service integration.\n\n### Deployment Process\n\nThe deployment process involves model setup, server configuration, and demo initialization:\n\n```mermaid\ngraph LR\n    subgraph \"Model Setup\"\n        DOWNLOAD[\"Download WebDancer-32B\"]\n        MODEL_PATH[\"Set MODEL_PATH\"]\n    end\n    \n    subgraph \"Server Deployment\"\n        DEPLOY_SCRIPT[\"deploy_model.sh\"]\n        SGLANG_LAUNCH[\"sglang.launch_server\"]\n        SERVER_CONFIG[\"--tp 4 --port 8004\"]\n    end\n    \n    subgraph \"Demo Configuration\"\n        API_KEYS[\"Configure API Keys\"]\n        GOOGLE_KEY[\"GOOGLE_SEARCH_KEY\"]\n        JINA_KEY[\"JINA_API_KEY\"]\n        DASH_KEY[\"DASHSCOPE_API_KEY\"]\n    end\n    \n    subgraph \"Demo Launch\"\n        RUN_SCRIPT[\"run_demo.sh\"]\n        GRADIO_DEMO[\"Gradio Interface\"]\n    end\n    \n    DOWNLOAD --\u003e MODEL_PATH\n    MODEL_PATH --\u003e DEPLOY_SCRIPT\n    DEPLOY_SCRIPT --\u003e SGLANG_LAUNCH\n    SGLANG_LAUNCH --\u003e SERVER_CONFIG\n    \n    API_KEYS --\u003e GOOGLE_KEY\n    API_KEYS --\u003e JINA_KEY\n    API_KEYS --\u003e DASH_KEY\n    \n    SERVER_CONFIG --\u003e RUN_SCRIPT\n    GOOGLE_KEY --\u003e RUN_SCRIPT\n    JINA_KEY --\u003e RUN_SCRIPT\n    DASH_KEY --\u003e RUN_SCRIPT\n    \n    RUN_SCRIPT --\u003e GRADIO_DEMO\n```\n\n**WebDancer Deployment Pipeline**\n\n### Configuration Requirements\n\n| Component | Purpose | Source |\n|-----------|---------|--------|\n| `GOOGLE_SEARCH_KEY` | Web search functionality | serper.dev |\n| `JINA_API_KEY` | Content extraction | jina.ai/api-dashboard |\n| `DASHSCOPE_API_KEY` | Additional AI services | dashscope.aliyun.com |\n\n### Usage Commands\n\n**Environment Setup:**\n```bash\nconda create -n webdancer python=3.12\npip install -r requirements.txt\n```\n\n**Model Deployment:**\n```bash\ncd scripts\nbash deploy_model.sh WebDancer_PATH\n```\n\n**Demo Launch:**\n```bash\ncd scripts  \nbash run_demo.sh\n```\n\nSources: [README.md:93-124](), [WebDancer/readme.md:22-55](), [WebDancer/scripts/deploy_model.sh:1-4](), [README.md:114-118]()\n\n## Performance and Capabilities\n\nWebDancer demonstrates strong performance across multiple benchmarks, achieving state-of-the-art results in autonomous information seeking tasks. The model excels at long-horizon tasks requiring multiple steps and complex reasoning.\n\n### Benchmark Results\n\n| Benchmark | Score | Metric | Task Type |\n|-----------|-------|--------|-----------|\n| GAIA | 64.1% | Pass@3 | General question answering |\n| WebWalkerQA | 62.0% | Pass@3 | Web traversal tasks |\n\n### Demonstrated Capabilities\n\nWebDancer exhibits several key capabilities that distinguish it from traditional web agents:\n\n- **Multi-step Reasoning**: Executes complex information seeking tasks requiring multiple search and synthesis steps\n- **Autonomous Operation**: Operates independently without external orchestration or human intervention\n- **Web Traversal**: Navigates complex web environments to gather information from multiple sources\n- **Complex Question Answering**: Handles sophisticated queries requiring information integration and reasoning\n\n### Use Cases\n\nThe system supports three primary use case categories as demonstrated in the provided demos:\n\n1. **WebWalkerQA Tasks**: Structured web traversal and information extraction\n2. **GAIA Benchmarks**: General intelligence assessment through complex reasoning\n3. **Daily Use Scenarios**: Practical information seeking for everyday queries\n\nSources: [README.md:87-88](), [WebDancer/readme.md:16-20](), [WebDancer/readme.md:59-75](), [README.md:154-172]()"])</script><script>self.__next_f.push([1,"1b:T22ef,"])</script><script>self.__next_f.push([1,"# Training Pipeline\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebDancer/readme.md](WebDancer/readme.md)\n- [WebDancer/requirements.txt](WebDancer/requirements.txt)\n- [WebDancer/scripts/deploy_model.sh](WebDancer/scripts/deploy_model.sh)\n\n\u003c/details\u003e\n\n\n\nThis document explains WebDancer's four-stage training paradigm that enables autonomous information seeking capabilities. The training pipeline comprises browsing data construction, trajectory sampling, supervised fine-tuning, and reinforcement learning phases that progressively build agent capabilities from raw web data to deployable models.\n\nFor WebDancer's architecture and implementation details, see [3.2](#3.2). For deployment and demo usage, see [3.3](#3.3). For dataset formats and structures, see [3.4](#3.4).\n\n## Training Pipeline Overview\n\nWebDancer employs a systematic four-stage training approach that transforms base language models into capable web information seeking agents. This data-centric methodology integrates trajectory-level supervision with online learning to create scalable agentic training systems.\n\n### Four-Stage Training Flow\n\n```mermaid\ngraph TD\n    RawWeb[\"Raw Web Content\"] --\u003e Stage1[\"Stage 1: Browsing Data Construction\"]\n    Stage1 --\u003e QAData[\"sample_qa.jsonl\"]\n    \n    QAData --\u003e Stage2[\"Stage 2: Trajectory Sampling\"]\n    Stage2 --\u003e TrajData[\"sample_traj.jsonl\"]\n    \n    TrajData --\u003e Stage3[\"Stage 3: Supervised Fine-Tuning\"]\n    Stage3 --\u003e SFTModel[\"SFT Model\"]\n    \n    SFTModel --\u003e Stage4[\"Stage 4: Reinforcement Learning\"]\n    Stage4 --\u003e FinalModel[\"WebDancer-32B\"]\n    \n    subgraph Infrastructure[\"Training Infrastructure\"]\n        LLaMAFactory[\"LLaMA-Factory\"]\n        VERL[\"Modified VERL\"]\n        SGLang[\"SGLang Server\"]\n    end\n    \n    Stage3 --\u003e LLaMAFactory\n    Stage4 --\u003e VERL\n    FinalModel --\u003e SGLang\n```\n\n**Training Pipeline Architecture**\n\nSources: [WebDancer/readme.md:11-14](), [WebDancer/readme.md:79-106]()\n\n## Stage 1: Browsing Data Construction\n\nThe initial stage focuses on constructing high-quality question-answer pairs from web content that require multi-step information seeking to solve. This stage establishes the foundational dataset for training agents to understand complex information seeking tasks.\n\n### Data Construction Process\n\n```mermaid\ngraph LR\n    WebContent[\"Web Content Sources\"] --\u003e DataProcessor[\"Data Construction Pipeline\"]\n    DataProcessor --\u003e QAGenerator[\"QA Pair Generator\"]\n    QAGenerator --\u003e Validator[\"Quality Validator\"]\n    Validator --\u003e QADataset[\"sample_qa.jsonl\"]\n    \n    subgraph ProcessingSteps[\"Construction Steps\"]\n        Extract[\"Content Extraction\"]\n        Formalize[\"Task Formalization\"] \n        Generate[\"QA Generation\"]\n        Filter[\"Quality Filtering\"]\n    end\n    \n    DataProcessor --\u003e Extract\n    Extract --\u003e Formalize\n    Formalize --\u003e Generate\n    Generate --\u003e Filter\n    Filter --\u003e QADataset\n```\n\n**Browsing Data Construction Pipeline**\n\nThe constructed data emphasizes multi-step reasoning requirements and complex web traversal scenarios that cannot be solved through single-step information retrieval.\n\nSources: [WebDancer/readme.md:81-88]()\n\n## Stage 2: Trajectory Sampling\n\nThis stage generates detailed action trajectories that demonstrate how to solve the QA pairs from Stage 1. The trajectories capture the step-by-step reasoning and tool usage patterns required for effective web information seeking.\n\n### Trajectory Generation Framework\n\n```mermaid\ngraph TD\n    QAInput[\"sample_qa.jsonl\"] --\u003e TrajSampler[\"Trajectory Sampling System\"]\n    TrajSampler --\u003e ReActAgent[\"ReAct-based Agent\"]\n    ReActAgent --\u003e ToolLayer[\"Tool Layer\"]\n    \n    subgraph Tools[\"Available Tools\"]\n        SearchTool[\"Search Tool\"]\n        VisitTool[\"Visit Tool\"]\n    end\n    \n    ToolLayer --\u003e Tools\n    Tools --\u003e WebAPIs[\"External Web APIs\"]\n    \n    ReActAgent --\u003e TrajLogger[\"Trajectory Logger\"]\n    TrajLogger --\u003e TrajData[\"sample_traj.jsonl\"]\n    \n    subgraph TrajFormat[\"Trajectory Format\"]\n        Thought[\"Thought\"]\n        Action[\"Action\"] \n        Observation[\"Observation\"]\n        Answer[\"Final Answer\"]\n    end\n    \n    TrajData --\u003e TrajFormat\n```\n\n**Trajectory Sampling Architecture**\n\nThe sampling process captures complete reasoning chains including tool invocations, intermediate observations, and decision-making processes that lead to successful task completion.\n\nSources: [WebDancer/readme.md:90-97]()\n\n## Stage 3: Supervised Fine-Tuning\n\nSupervised fine-tuning provides effective cold start capabilities by training the base model on high-quality trajectories. This stage ensures the model learns proper formatting, tool usage patterns, and reasoning structures before advanced optimization.\n\n### SFT Training Configuration\n\n```mermaid\ngraph LR\n    BaseModel[\"Base Language Model\"] --\u003e SFTTrainer[\"LLaMA-Factory Trainer\"]\n    TrajData[\"sample_traj.jsonl\"] --\u003e SFTTrainer\n    \n    subgraph TrainingConfig[\"SFT Configuration\"]\n        LossFunc[\"Trajectory Loss Function\"]\n        BatchSize[\"Batch Size Settings\"]\n        LearningRate[\"Learning Rate Schedule\"]\n        Epochs[\"Training Epochs\"]\n    end\n    \n    SFTTrainer --\u003e TrainingConfig\n    TrainingConfig --\u003e SFTModel[\"Cold Start Model\"]\n    \n    subgraph LLaMAFactoryComponents[\"LLaMA-Factory Components\"]\n        DataLoader[\"Trajectory Data Loader\"]\n        Optimizer[\"Training Optimizer\"]\n        Scheduler[\"Learning Rate Scheduler\"]\n        Checkpoints[\"Model Checkpoints\"]\n    end\n    \n    SFTTrainer --\u003e LLaMAFactoryComponents\n```\n\n**Supervised Fine-Tuning Infrastructure**\n\nThe SFT stage utilizes LLaMA-Factory's training scripts to ensure stable convergence and proper trajectory learning before proceeding to reinforcement learning optimization.\n\nSources: [WebDancer/readme.md:99-101](), [WebDancer/readme.md:109]()\n\n## Stage 4: Reinforcement Learning\n\nThe final stage employs reinforcement learning to improve generalization and enable online learning from environment feedback. This stage uses a modified VERL framework with trajectory-level supervision for enhanced performance.\n\n### RL Training Framework\n\n```mermaid\ngraph TD\n    SFTModel[\"Cold Start Model\"] --\u003e RLTrainer[\"Modified VERL Trainer\"]\n    \n    subgraph RLComponents[\"RL Training Components\"]\n        PolicyModel[\"Policy Model\"]\n        ValueModel[\"Value Model\"] \n        RewardModel[\"Reward Model\"]\n        CriticModel[\"Critic Model\"]\n    end\n    \n    RLTrainer --\u003e RLComponents\n    \n    subgraph Environment[\"Training Environment\"]\n        WebEnv[\"Web Environment\"]\n        TaskSampler[\"Task Sampler\"]\n        RewardComputer[\"Reward Computer\"]\n    end\n    \n    PolicyModel --\u003e Environment\n    Environment --\u003e RewardModel\n    RewardModel --\u003e ValueModel\n    ValueModel --\u003e PolicyOptimizer[\"Policy Optimizer\"]\n    PolicyOptimizer --\u003e FinalModel[\"WebDancer-32B\"]\n    \n    subgraph VERLFramework[\"VERL Framework\"]\n        PPOTrainer[\"PPO Trainer\"]\n        TrajectoryBuffer[\"Trajectory Buffer\"]\n        OnlineUpdate[\"Online Updates\"]\n    end\n    \n    RLTrainer --\u003e VERLFramework\n```\n\n**Reinforcement Learning Training Architecture**\n\nThe RL stage enables the model to learn from online interactions and improve performance through trajectory-level supervision and policy optimization techniques.\n\nSources: [WebDancer/readme.md:104-105](), [WebDancer/readme.md:109]()\n\n## Training Infrastructure and Dependencies\n\nWebDancer's training pipeline relies on established frameworks and custom modifications to support agentic training requirements.\n\n### Infrastructure Components\n\n| Component | Purpose | Configuration |\n|-----------|---------|---------------|\n| `LLaMA-Factory` | SFT Training | Standard trajectory fine-tuning scripts |\n| `Modified VERL` | RL Training | Custom trajectory-level supervision |\n| `SGLang` | Model Deployment | Multi-GPU inference serving |\n| `sample_qa.jsonl` | QA Dataset | Structured question-answer pairs |\n| `sample_traj.jsonl` | Trajectory Dataset | Complete action sequences |\n\n### Deployment Configuration\n\n```mermaid\ngraph LR\n    TrainedModel[\"WebDancer-32B\"] --\u003e DeployScript[\"deploy_model.sh\"]\n    DeployScript --\u003e SGLangServer[\"SGLang Server\"]\n    \n    subgraph DeploymentConfig[\"Deployment Settings\"]\n        ModelPath[\"MODEL_PATH\"]\n        Host[\"Host: 0.0.0.0\"]\n        TensorParallel[\"TP: 4\"]\n        Port[\"Port: 8004\"]\n    end\n    \n    DeployScript --\u003e DeploymentConfig\n    SGLangServer --\u003e InferenceAPI[\"Inference API\"]\n    \n    subgraph Requirements[\"Dependencies\"]\n        SGLangAll[\"sglang[all]\"]\n        QwenAgent[\"qwen-agent[gui,rag,code_interpreter,mcp]\"]\n    end\n    \n    SGLangServer --\u003e Requirements\n```\n\n**Model Deployment Infrastructure**\n\nThe training pipeline outputs models compatible with SGLang's distributed inference system, enabling efficient deployment for both evaluation and production usage.\n\nSources: [WebDancer/scripts/deploy_model.sh:1-4](), [WebDancer/requirements.txt:1-2]()"])</script><script>self.__next_f.push([1,"1c:T2f4c,"])</script><script>self.__next_f.push([1,"# Architecture and Implementation\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebDancer/assets/data_construction.png](WebDancer/assets/data_construction.png)\n- [WebDancer/assets/framework.png](WebDancer/assets/framework.png)\n- [WebDancer/assets/performance.png](WebDancer/assets/performance.png)\n- [WebDancer/readme.md](WebDancer/readme.md)\n- [WebDancer/requirements.txt](WebDancer/requirements.txt)\n- [WebDancer/scripts/deploy_model.sh](WebDancer/scripts/deploy_model.sh)\n\n\u003c/details\u003e\n\n\n\nThis document covers the technical architecture and implementation details of WebDancer, focusing on its ReAct-based agent design, tool integration patterns, and deployment infrastructure. WebDancer is an autonomous information seeking agent that forms one component of the larger WebAgent ecosystem.\n\nFor information about WebDancer's training methodology and datasets, see [Training Pipeline](#3.1) and [Datasets and Trajectories](#3.4). For user-facing demo deployment, see [Demo and Usage](#3.3).\n\n## Core Architecture Overview\n\nWebDancer implements a ReAct (Reasoning + Acting) framework for autonomous web information seeking. The system follows a modular architecture that separates reasoning logic from tool execution and external service integration.\n\n```mermaid\ngraph TB\n    subgraph \"WebDancer Core\"\n        Agent[\"WebDancerAgent\u003cbr/\u003eReAct Implementation\"]\n        Reasoning[\"ReasoningEngine\u003cbr/\u003ePlanning \u0026 Decision Making\"]\n        ActionExecutor[\"ActionExecutor\u003cbr/\u003eTool Invocation\"]\n    end\n    \n    subgraph \"Tool Layer\"\n        SearchTool[\"SearchTool\u003cbr/\u003eGoogle Serper Integration\"]\n        VisitTool[\"VisitTool\u003cbr/\u003eJina Reader Integration\"]\n        ToolRegistry[\"ToolRegistry\u003cbr/\u003eTool Management\"]\n    end\n    \n    subgraph \"External Services\"\n        SGLang[\"sglang.launch_server\u003cbr/\u003eModel Inference\"]\n        SerperAPI[\"serper.dev\u003cbr/\u003eSearch Service\"]\n        JinaAPI[\"jina.ai\u003cbr/\u003eContent Extraction\"]\n        DashScope[\"dashscope.aliyun.com\u003cbr/\u003eSummary Models\"]\n    end\n    \n    subgraph \"Interface Layer\"\n        GradioDemo[\"Gradio Interface\u003cbr/\u003eWeb UI\"]\n        CLI[\"Command Line\u003cbr/\u003eDirect Access\"]\n    end\n    \n    Agent --\u003e Reasoning\n    Agent --\u003e ActionExecutor\n    ActionExecutor --\u003e ToolRegistry\n    ToolRegistry --\u003e SearchTool\n    ToolRegistry --\u003e VisitTool\n    \n    SearchTool --\u003e SerperAPI\n    VisitTool --\u003e JinaAPI\n    Agent --\u003e SGLang\n    VisitTool --\u003e DashScope\n    \n    GradioDemo --\u003e Agent\n    CLI --\u003e Agent\n```\n\n**WebDancer Architecture Overview**\n\nThe architecture implements a clean separation of concerns where the core agent focuses on reasoning and action selection, while external integrations are abstracted through standardized tool interfaces.\n\nSources: [WebDancer/readme.md:11-14](), [WebDancer/scripts/deploy_model.sh:1-4]()\n\n## ReAct Agent Implementation\n\nWebDancer's core agent follows the ReAct paradigm, interleaving reasoning steps with action execution. The implementation supports multi-step information seeking with trajectory-level supervision.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant WebDancerAgent as \"WebDancerAgent\"\n    participant ReasoningEngine as \"ReasoningEngine\"\n    participant ActionExecutor as \"ActionExecutor\"\n    participant ToolRegistry as \"ToolRegistry\"\n    participant SGLang as \"sglang.launch_server\"\n    \n    User-\u003e\u003eWebDancerAgent: \"submit_query(query)\"\n    WebDancerAgent-\u003e\u003eSGLang: \"generate_initial_plan(query)\"\n    SGLang-\u003e\u003eWebDancerAgent: \"reasoning_trajectory\"\n    \n    loop \"Multi-step Information Seeking\"\n        WebDancerAgent-\u003e\u003eReasoningEngine: \"analyze_context(state)\"\n        ReasoningEngine-\u003e\u003eSGLang: \"generate_next_action(context)\"\n        SGLang-\u003e\u003eReasoningEngine: \"action_plan\"\n        ReasoningEngine-\u003e\u003eActionExecutor: \"execute_action(action)\"\n        ActionExecutor-\u003e\u003eToolRegistry: \"invoke_tool(tool_name, params)\"\n        ToolRegistry-\u003e\u003eActionExecutor: \"tool_results\"\n        ActionExecutor-\u003e\u003eWebDancerAgent: \"action_results\"\n        WebDancerAgent-\u003e\u003eSGLang: \"update_trajectory(results)\"\n        SGLang-\u003e\u003eWebDancerAgent: \"continue_or_finish\"\n    end\n    \n    WebDancerAgent-\u003e\u003eUser: \"final_answer_with_sources\"\n```\n\n**ReAct Agent Execution Flow**\n\nThe agent maintains a conversation trajectory that combines reasoning steps with action results, enabling it to make informed decisions about subsequent actions based on accumulated context.\n\nSources: [WebDancer/readme.md:14](), [WebDancer/readme.md:79-106]()\n\n## Tool Integration Layer\n\nWebDancer implements a standardized tool interface that abstracts external service integrations. This design allows the agent to use different tools through a common API while maintaining flexibility for future extensions.\n\n```mermaid\nclassDiagram\n    class ToolRegistry {\n        +register_tool(tool_name, tool_instance)\n        +get_tool(tool_name)\n        +list_available_tools()\n        +invoke_tool(tool_name, parameters)\n    }\n    \n    class BaseTool {\n        \u003c\u003cabstract\u003e\u003e\n        +execute(parameters)\n        +validate_parameters(parameters)\n        +get_schema()\n    }\n    \n    class SearchTool {\n        -google_search_key: str\n        -serper_client: SerperClient\n        +execute(query: str)\n        +search_web(query: str)\n        +format_results(raw_results)\n    }\n    \n    class VisitTool {\n        -jina_api_key: str\n        -jina_client: JinaClient\n        -dashscope_client: DashScopeClient\n        +execute(url: str)\n        +extract_content(url: str)\n        +summarize_content(content: str)\n    }\n    \n    class ActionExecutor {\n        -tool_registry: ToolRegistry\n        +execute_action(action_type, parameters)\n        +validate_action(action)\n        +handle_tool_errors(error)\n    }\n    \n    ToolRegistry --\u003e BaseTool\n    BaseTool \u003c|-- SearchTool\n    BaseTool \u003c|-- VisitTool\n    ActionExecutor --\u003e ToolRegistry\n    \n    SearchTool --\u003e \"serper.dev\"\n    VisitTool --\u003e \"jina.ai\"\n    VisitTool --\u003e \"dashscope.aliyun.com\"\n```\n\n**Tool Integration Class Structure**\n\n| Tool | External Service | Configuration Key | Purpose |\n|------|------------------|-------------------|---------|\n| `SearchTool` | serper.dev | `GOOGLE_SEARCH_KEY` | Web search queries |\n| `VisitTool` | jina.ai | `JINA_API_KEY` | Content extraction |\n| `VisitTool` | dashscope.aliyun.com | `DASHSCOPE_API_KEY` | Content summarization |\n\nSources: [WebDancer/readme.md:44-48]()\n\n## Training Pipeline Architecture\n\nWebDancer's training follows a four-stage paradigm that progresses from data construction through reinforcement learning. The architecture supports both supervised fine-tuning and online learning components.\n\n```mermaid\nflowchart TD\n    subgraph \"Stage 1: Data Construction\"\n        QAData[\"QA Data Sampling\u003cbr/\u003edatasets/sample_qa.jsonl\"]\n        BrowsingData[\"Browsing Data\u003cbr/\u003eWeb Interaction Logs\"]\n    end\n    \n    subgraph \"Stage 2: Trajectory Sampling\"\n        TrajSampling[\"Trajectory Generation\u003cbr/\u003edatasets/sample_traj.jsonl\"]\n        ExpertDemo[\"Expert Demonstrations\u003cbr/\u003eMulti-step Reasoning\"]\n    end\n    \n    subgraph \"Stage 3: Supervised Fine-tuning\"\n        LlamaFactory[\"LLaMA-Factory\u003cbr/\u003eSFT Training Scripts\"]\n        ColdStart[\"Cold Start Training\u003cbr/\u003eBasic Agent Capabilities\"]\n    end\n    \n    subgraph \"Stage 4: Reinforcement Learning\"\n        VERL[\"Modified VERL\u003cbr/\u003eRL Training Framework\"]\n        OnlineLearning[\"Online Learning\u003cbr/\u003ePolicy Optimization\"]\n    end\n    \n    subgraph \"Model Deployment\"\n        SGLangServer[\"sglang.launch_server\u003cbr/\u003e--model-path MODEL_PATH\u003cbr/\u003e--tp 4 --port 8004\"]\n        WebDancerModel[\"WebDancer-32B\u003cbr/\u003eTrained Model\"]\n    end\n    \n    QAData --\u003e TrajSampling\n    BrowsingData --\u003e TrajSampling\n    TrajSampling --\u003e LlamaFactory\n    ExpertDemo --\u003e LlamaFactory\n    LlamaFactory --\u003e VERL\n    ColdStart --\u003e OnlineLearning\n    VERL --\u003e WebDancerModel\n    OnlineLearning --\u003e WebDancerModel\n    WebDancerModel --\u003e SGLangServer\n```\n\n**Training Pipeline Architecture**\n\nThe pipeline integrates multiple training frameworks to support the complete agent development lifecycle from data synthesis to deployment-ready models.\n\nSources: [WebDancer/readme.md:79-106](), [WebDancer/readme.md:87-93](), [WebDancer/readme.md:101-105](), [WebDancer/scripts/deploy_model.sh:1-4]()\n\n## Deployment Infrastructure\n\nWebDancer uses SGLang for high-performance model serving with support for tensor parallelism and optimized inference. The deployment architecture supports both development and production scenarios.\n\n```mermaid\ngraph LR\n    subgraph \"Model Serving\"\n        ModelPath[\"MODEL_PATH\u003cbr/\u003eWebDancer-32B\"]\n        SGLangLauncher[\"sglang.launch_server\"]\n        ServerConfig[\"--host 0.0.0.0\u003cbr/\u003e--tp 4\u003cbr/\u003e--port 8004\"]\n    end\n    \n    subgraph \"Runtime Dependencies\"\n        SGLangAll[\"sglang[all]\"]\n        QwenAgent[\"qwen-agent[gui,rag,code_interpreter,mcp]\"]\n    end\n    \n    subgraph \"Interface Layers\"\n        GradioUI[\"Gradio Web Interface\u003cbr/\u003eInteractive Demo\"]\n        DirectAPI[\"Direct API Access\u003cbr/\u003eProgrammatic Usage\"]\n    end\n    \n    subgraph \"External API Keys\"\n        GoogleKey[\"GOOGLE_SEARCH_KEY\u003cbr/\u003eserper.dev\"]\n        JinaKey[\"JINA_API_KEY\u003cbr/\u003ejina.ai\"] \n        DashScopeKey[\"DASHSCOPE_API_KEY\u003cbr/\u003edashscope.aliyun.com\"]\n    end\n    \n    ModelPath --\u003e SGLangLauncher\n    ServerConfig --\u003e SGLangLauncher\n    SGLangAll --\u003e SGLangLauncher\n    QwenAgent --\u003e GradioUI\n    \n    SGLangLauncher --\u003e GradioUI\n    SGLangLauncher --\u003e DirectAPI\n    \n    GradioUI --\u003e GoogleKey\n    GradioUI --\u003e JinaKey\n    GradioUI --\u003e DashScopeKey\n```\n\n**Deployment Infrastructure Components**\n\n| Component | Configuration | Purpose |\n|-----------|---------------|---------|\n| `sglang.launch_server` | `--tp 4` | Tensor parallelism for inference acceleration |\n| `sglang.launch_server` | `--port 8004` | Server endpoint configuration |\n| `qwen-agent[gui]` | Gradio integration | Web interface for demonstrations |\n| External APIs | Environment variables | Service integrations for tools |\n\nSources: [WebDancer/scripts/deploy_model.sh:3-4](), [WebDancer/requirements.txt:1-2](), [WebDancer/readme.md:44-54]()\n\n## Code Entity Relationships\n\nThis diagram maps WebDancer's high-level concepts to specific code entities and configuration elements that developers interact with.\n\n```mermaid\ngraph TB\n    subgraph \"Core Implementation Files\"\n        DeployScript[\"deploy_model.sh\u003cbr/\u003esglang.launch_server\"]\n        RunDemo[\"run_demo.sh\u003cbr/\u003eGradio Interface Setup\"]\n        Requirements[\"requirements.txt\u003cbr/\u003esglang[all], qwen-agent[gui]\"]\n    end\n    \n    subgraph \"Data Assets\"\n        SampleQA[\"datasets/sample_qa.jsonl\u003cbr/\u003eQA Training Data\"]\n        SampleTraj[\"datasets/sample_traj.jsonl\u003cbr/\u003eTrajectory Data\"]\n    end\n    \n    subgraph \"Configuration Variables\"\n        ModelPath[\"MODEL_PATH\u003cbr/\u003eWebDancer-32B\"]\n        GoogleSearchKey[\"GOOGLE_SEARCH_KEY\u003cbr/\u003eserper.dev integration\"]\n        JinaAPIKey[\"JINA_API_KEY\u003cbr/\u003ejina.ai integration\"]\n        DashScopeKey[\"DASHSCOPE_API_KEY\u003cbr/\u003edashscope.aliyun.com\"]\n    end\n    \n    subgraph \"External Frameworks\"\n        LlamaFactory[\"LLaMA-Factory\u003cbr/\u003eSFT Training\"]\n        VERL[\"verl\u003cbr/\u003eRL Training\"]\n        SGLang[\"sglang\u003cbr/\u003eModel Serving\"]\n    end\n    \n    subgraph \"Performance Metrics\"\n        GAIABench[\"GAIA Benchmark\u003cbr/\u003e64.1% Pass@3\"]\n        WebWalkerQA[\"WebWalkerQA\u003cbr/\u003e62.0% Pass@3\"]\n    end\n    \n    DeployScript --\u003e ModelPath\n    DeployScript --\u003e SGLang\n    RunDemo --\u003e GoogleSearchKey\n    RunDemo --\u003e JinaAPIKey\n    RunDemo --\u003e DashScopeKey\n    \n    SampleQA --\u003e LlamaFactory\n    SampleTraj --\u003e LlamaFactory\n    LlamaFactory --\u003e VERL\n    VERL --\u003e ModelPath\n    \n    Requirements --\u003e SGLang\n    SGLang --\u003e GAIABench\n    SGLang --\u003e WebWalkerQA\n```\n\n**Code Entity to System Component Mapping**\n\nThis diagram enables developers to quickly locate implementation details by connecting system architecture concepts to specific files, configuration variables, and external dependencies in the codebase.\n\nSources: [WebDancer/readme.md:87-88](), [WebDancer/readme.md:92-93](), [WebDancer/scripts/deploy_model.sh:1-4](), [WebDancer/requirements.txt:1-2](), [WebDancer/readme.md:44-48]()"])</script><script>self.__next_f.push([1,"1d:T20cc,"])</script><script>self.__next_f.push([1,"# Demo and Usage\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebDancer/demos/assistant_qwq_chat.py](WebDancer/demos/assistant_qwq_chat.py)\n- [WebDancer/readme.md](WebDancer/readme.md)\n- [WebDancer/requirements.txt](WebDancer/requirements.txt)\n- [WebDancer/scripts/deploy_model.sh](WebDancer/scripts/deploy_model.sh)\n\n\u003c/details\u003e\n\n\n\nThis document provides a comprehensive guide for deploying and using WebDancer through the Gradio demo interface. It covers environment setup, model deployment using SGLang, API configuration, and interactive usage through the web interface.\n\nFor information about WebDancer's training pipeline, see [Training Pipeline](#3.1). For architectural details, see [Architecture and Implementation](#3.2).\n\n## Overview\n\nWebDancer provides a user-friendly Gradio web interface that enables interactive information seeking through a ReAct-based agent. The demo system integrates multiple components including the SGLang model server, external API services, and a web-based chat interface for real-time interaction with the WebDancer agent.\n\n## Environment Setup\n\n### Prerequisites\n\nThe WebDancer demo requires Python 3.12 and several dependencies for model serving and web interface functionality.\n\n**Required Dependencies:**\n- `sglang[all]` - Model serving framework with full features\n- `qwen-agent[gui,rag,code_interpreter,mcp]` - Agent framework with GUI, RAG, code interpreter, and MCP support\n\n**Environment Configuration:**\n```bash\nconda create -n webdancer python=3.12\npip install -r requirements.txt\n```\n\nSources: [WebDancer/requirements.txt:1-2](), [WebDancer/readme.md:26-29]()\n\n## Model Deployment Architecture\n\n### SGLang Server Deployment\n\nWebDancer uses SGLang for efficient model serving with tensor parallelism support. The deployment process involves downloading the pre-trained model and launching the SGLang server.\n\n**Deployment Flow:**\n\n```mermaid\ngraph TD\n    ModelDownload[\"Model Download\u003cbr/\u003eHuggingFace: Alibaba-NLP/WebDancer-32B\"]\n    DeployScript[\"deploy_model.sh\u003cbr/\u003esglang.launch_server\"]\n    SGLangServer[\"SGLang Server\u003cbr/\u003ePort: 8004\u003cbr/\u003eTensor Parallel: 4\"]\n    \n    ModelDownload --\u003e DeployScript\n    DeployScript --\u003e SGLangServer\n    \n    SGLangServer --\u003e OAIEndpoint[\"OpenAI-Compatible API\u003cbr/\u003e/v1 endpoint\"]\n    OAIEndpoint --\u003e DemoInterface[\"Gradio Demo Interface\u003cbr/\u003eassistant_qwq_chat.py\"]\n```\n\n**Server Configuration:**\n- Host: `0.0.0.0` (all interfaces)\n- Port: `8004`\n- Tensor Parallelism: `4` (for multi-GPU deployment)\n- API Compatibility: OpenAI v1 format\n\nThe deployment script [WebDancer/scripts/deploy_model.sh:1-4]() uses `sglang.launch_server` with the following parameters: `--model-path`, `--host`, `--tp`, and `--port`.\n\nSources: [WebDancer/scripts/deploy_model.sh:1-4](), [WebDancer/readme.md:33-40]()\n\n## Demo Interface Components\n\n### Core Agent Architecture\n\nThe demo interface is built around the `SearchAgent` class which integrates with external APIs and the SGLang model server.\n\n```mermaid\ngraph TD\n    WebUI[\"WebUI\u003cbr/\u003eGradio Interface\u003cbr/\u003ePort: 7860\"]\n    SearchAgent[\"SearchAgent\u003cbr/\u003eWebDancer Agent\"]\n    TextChatAtOAI[\"TextChatAtOAI\u003cbr/\u003eLLM Configuration\"]\n    \n    subgraph \"External APIs\"\n        GoogleAPI[\"Google Search API\u003cbr/\u003eserper.dev\u003cbr/\u003eGOOGLE_SEARCH_KEY\"]\n        JinaAPI[\"Jina Reader API\u003cbr/\u003ejina.ai\u003cbr/\u003eJINA_API_KEY\"]\n        DashScopeAPI[\"DashScope API\u003cbr/\u003edashscope.aliyun.com\u003cbr/\u003eDASHSCOPE_API_KEY\"]\n    end\n    \n    subgraph \"Tools\"\n        SearchTool[\"Search Tool\u003cbr/\u003eGoogle Serper Integration\"]\n        VisitTool[\"Visit Tool\u003cbr/\u003eJina Reader Integration\"]\n    end\n    \n    WebUI --\u003e SearchAgent\n    SearchAgent --\u003e TextChatAtOAI\n    TextChatAtOAI --\u003e SGLangServer[\"SGLang Server\u003cbr/\u003e127.0.0.1:8004\"]\n    \n    SearchAgent --\u003e SearchTool\n    SearchAgent --\u003e VisitTool\n    \n    SearchTool --\u003e GoogleAPI\n    VisitTool --\u003e JinaAPI\n    SearchAgent --\u003e DashScopeAPI\n```\n\n### Configuration Parameters\n\nThe `init_dev_search_agent_service` function [WebDancer/demos/assistant_qwq_chat.py:20-85]() configures the SearchAgent with:\n\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| `name` | `WebDancer-QwQ-32B` | Agent identifier |\n| `port` | `8004` | SGLang server port |\n| `reasoning` | `True` | Enable reasoning capabilities |\n| `max_llm_calls` | `50` | Maximum LLM interaction limit |\n| `tools` | `['search', 'visit']` | Available tool set |\n\n**LLM Configuration:**\n- Model Server: `http://127.0.0.1:8004/v1`\n- Temperature: `0.6`\n- Top-p: `0.95`\n- Max Tokens: `32768`\n- Timeout: `3000` seconds\n\nSources: [WebDancer/demos/assistant_qwq_chat.py:20-38](), [WebDancer/demos/assistant_qwq_chat.py:91-102]()\n\n## API Configuration\n\n### Required API Keys\n\nThe demo requires three external API services for full functionality:\n\n1. **Google Search API (Serper)**\n   - Variable: `GOOGLE_SEARCH_KEY`\n   - Provider: [serper.dev](https://serper.dev/)\n   - Purpose: Web search capabilities\n\n2. **Jina Reader API**\n   - Variable: `JINA_API_KEY`\n   - Provider: [jina.ai](https://jina.ai/api-dashboard/)\n   - Purpose: Web page content extraction\n\n3. **DashScope API**\n   - Variable: `DASHSCOPE_API_KEY`\n   - Provider: [dashscope.aliyun.com](https://dashscope.aliyun.com/)\n   - Purpose: Additional AI services\n\nThese keys must be configured in the [WebDancer/scripts/run_demo.sh]() script before launching the demo.\n\nSources: [WebDancer/readme.md:44-48]()\n\n## Usage Interface\n\n### Gradio Web Interface\n\nThe demo launches a Gradio web interface on `127.0.0.1:7860` with the following configuration:\n\n**Interface Features:**\n- **Concurrency Limit:** 20 simultaneous sessions\n- **Mention Support:** Disabled\n- **Sharing:** Local only (not publicly shared)\n\n**Pre-configured Queries:**\n\nThe interface includes suggested queries covering various domains:\n\n| Category | Example Query |\n|----------|---------------|\n| Sports | Chinese national football team match analysis |\n| Academic | ACL 2025 conference details and deadlines |\n| Research | NASA award number verification tasks |\n| Entertainment | Chinese entertainment industry historical events |\n| Travel | Beijing Dragon Boat Festival travel itinerary |\n| Technology | Automotive performance comparison analysis |\n\n### Interaction Pattern\n\nThe agent follows a structured thinking and action pattern defined in the `custom_user_prompt` [WebDancer/demos/assistant_qwq_chat.py:61-82]():\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant WebUI as \"Gradio WebUI\"\n    participant SearchAgent as \"SearchAgent\"\n    participant SGLang as \"SGLang Server\"\n    participant Tools as \"Search/Visit Tools\"\n    participant APIs as \"External APIs\"\n    \n    User-\u003e\u003eWebUI: \"Submit query\"\n    WebUI-\u003e\u003eSearchAgent: \"Process request\"\n    \n    loop \"Multi-step reasoning\"\n        SearchAgent-\u003e\u003eSGLang: \"\"\n        SGLang-\u003e\u003eSearchAgent: \"Generated thoughts\"\n        SearchAgent-\u003e\u003eSGLang: \"\u003ctool_call\u003e with parameters \u003c/tool_call\u003e\"\n        SGLang-\u003e\u003eSearchAgent: \"Tool execution plan\"\n        SearchAgent-\u003e\u003eTools: \"Execute search/visit\"\n        Tools-\u003e\u003eAPIs: \"API calls\"\n        APIs-\u003e\u003eTools: \"Results\"\n        Tools-\u003e\u003eSearchAgent: \"\u003ctool_response\u003e data \u003c/tool_response\u003e\"\n    end\n    \n    SearchAgent-\u003e\u003eSGLang: \"\u003canswer\u003e final response \u003c/answer\u003e\"\n    SGLang-\u003e\u003eSearchAgent: \"Formatted answer\"\n    SearchAgent-\u003e\u003eWebUI: \"Complete response\"\n    WebUI-\u003e\u003eUser: \"Display results\"\n```\n\n### System Prompt Configuration\n\nThe agent uses a comprehensive system prompt [WebDancer/demos/assistant_qwq_chat.py:39-47]() that defines its behavior:\n\n**Core Principles:**\n1. **Persistent Actions:** Continue until satisfactory answer found\n2. **Repeated Verification:** Cross-check and validate information\n3. **Attention to Detail:** Analyze sources for accuracy and relevance\n\nThe system prompt includes current datetime context using `date2str(get_date_now(), with_week=True)` for temporal awareness.\n\nSources: [WebDancer/demos/assistant_qwq_chat.py:39-83](), [WebDancer/demos/assistant_qwq_chat.py:105-136]()\n\n## Deployment Commands\n\n### Complete Deployment Process\n\n1. **Model Deployment:**\n```bash\ncd scripts\nbash deploy_model.sh /path/to/WebDancer-32B\n```\n\n2. **Demo Launch:**\n```bash\ncd scripts\nbash run_demo.sh\n```\n\nThe deployment creates a complete information seeking system accessible through a web browser at the configured address.\n\nSources: [WebDancer/readme.md:35-55]()"])</script><script>self.__next_f.push([1,"1e:T252f,"])</script><script>self.__next_f.push([1,"# Datasets and Trajectories\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebDancer/datasets/sample_qa.jsonl](WebDancer/datasets/sample_qa.jsonl)\n- [WebDancer/datasets/sample_traj.jsonl](WebDancer/datasets/sample_traj.jsonl)\n\n\u003c/details\u003e\n\n\n\nThis document covers WebDancer's training datasets and trajectory formats, including the structure of conversational trajectories, question-answer pairs, and their role in the training pipeline. For information about WebDancer's training methodology, see [Training Pipeline](#3.1). For details about WebDancer's architecture, see [Architecture and Implementation](#3.2).\n\n## Dataset Overview\n\nWebDancer utilizes two primary dataset formats for training: trajectory data in ChatML format and question-answer pairs. These datasets support the four-stage training paradigm including data construction, trajectory sampling, supervised fine-tuning, and reinforcement learning.\n\n```mermaid\ngraph TB\n    subgraph \"Dataset Types\"\n        TD[\"sample_traj.jsonl\u003cbr/\u003eTrajectory Data\"]\n        QD[\"sample_qa.jsonl\u003cbr/\u003eQA Data\"]\n    end\n    \n    subgraph \"Training Stages\"\n        SFT[\"Supervised Fine-tuning\"]\n        RL[\"Reinforcement Learning\"]\n        TS[\"Trajectory Sampling\"]\n    end\n    \n    subgraph \"Data Formats\"\n        CM[\"ChatML Format\"]\n        QA[\"Question-Answer Pairs\"]\n    end\n    \n    TD --\u003e CM\n    QD --\u003e QA\n    CM --\u003e SFT\n    QA --\u003e SFT\n    CM --\u003e TS\n    TS --\u003e RL\n    \n    TD -.-\u003e |\"type: chatml\"| CM\n    QD -.-\u003e |\"question/answer fields\"| QA\n```\n\n**Dataset Structure and Training Integration**\n\nSources: [WebDancer/datasets/sample_traj.jsonl:1-6](), [WebDancer/datasets/sample_qa.jsonl:1-10]()\n\n## Trajectory Format and Structure\n\nWebDancer trajectories use the ChatML (Chat Markup Language) format to represent multi-turn conversations between the agent and tools. Each trajectory contains structured interaction data for training the ReAct-based reasoning capabilities.\n\n### ChatML Trajectory Schema\n\n```mermaid\ngraph LR\n    subgraph \"Trajectory Entry\"\n        TYPE[\"type: 'chatml'\"]\n        TASK[\"task: 'agent/multiturn_search'\"]\n        MESSAGES[\"messages: []\"]\n    end\n    \n    subgraph \"Message Types\"\n        SYSTEM[\"system\u003cbr/\u003eRole Definition\"]\n        USER[\"user\u003cbr/\u003eTool Definitions + Query\"]\n        ASSISTANT[\"assistant\u003cbr/\u003eReasoning + Actions\"]\n    end\n    \n    subgraph \"Assistant Components\"\n        THINK[\"\u003cthink\u003e tags\u003cbr/\u003eReasoning Process\"]\n        TOOLCALL[\"\u003ctool_call\u003e tags\u003cbr/\u003eFunction Calls\"]\n        TOOLRESP[\"\u003ctool_response\u003e tags\u003cbr/\u003eTool Outputs\"]\n        ANSWER[\"\u003canswer\u003e tags\u003cbr/\u003eFinal Response\"]\n    end\n    \n    MESSAGES --\u003e SYSTEM\n    MESSAGES --\u003e USER\n    MESSAGES --\u003e ASSISTANT\n    \n    ASSISTANT --\u003e THINK\n    ASSISTANT --\u003e TOOLCALL\n    ASSISTANT --\u003e TOOLRESP\n    ASSISTANT --\u003e ANSWER\n```\n\n**Trajectory Message Structure**\n\n### System Message Components\n\nThe system message establishes the agent's role and operational principles:\n\n```json\n{\n  \"role\": \"system\",\n  \"content\": \"You are a Web Information Seeking Master. Your task is to thoroughly seek the internet for information and provide accurate answers to questions. No matter how complex the query, you will not give up until you find the corresponding information.\\n\\nAs you proceed, adhere to the following principles:\\n\\n1. **Persistent Actions for Answers**: You will engage in many interactions, delving deeply into the topic to explore all possible aspects until a satisfactory answer is found.\\n\\n2. **Repeated Verification**: Before presenting a Final Answer, you will **cross-check** and **validate the information** you've gathered to confirm its accuracy and reliability.\\n\\n3. **Attention to Detail**: You will carefully analyze each information source to ensure that all data is current, relevant, and from credible origins.\\nCurrent date: 2025-07-18\"\n}\n```\n\n### Tool Definitions\n\nThe user message includes tool specifications for `search` and `visit` functions:\n\n| Tool | Parameters | Description |\n|------|------------|-------------|\n| `search` | `query: array\u003cstring\u003e` | Performs batched web searches with multiple queries |\n| `visit` | `url: array\u003cstring\u003e`, `goal: string` | Visits webpages and returns content summaries |\n\n### Assistant Response Format\n\nAssistant responses follow a structured ReAct pattern with XML-style tags:\n\n```mermaid\nsequenceDiagram\n    participant A as Assistant\n    participant T as Tools\n    \n    A-\u003e\u003eA: \u003cthink\u003e Reasoning Process\n    A-\u003e\u003eT: \u003ctool_call\u003e Function Execution\n    T-\u003e\u003eA: \u003ctool_response\u003e Results\n    A-\u003e\u003eA: \u003cthink\u003e Continue Reasoning\n    A-\u003e\u003eT: \u003ctool_call\u003e Additional Calls\n    T-\u003e\u003eA: \u003ctool_response\u003e More Results\n    A-\u003e\u003eA: \u003cthink\u003e Final Analysis\n    A-\u003e\u003eA: \u003canswer\u003e Complete Response\n```\n\n**ReAct Reasoning and Action Cycle**\n\nSources: [WebDancer/datasets/sample_traj.jsonl:1-2]()\n\n## QA Dataset Format\n\nThe question-answer dataset provides structured training pairs for information seeking tasks, organized with categorical tags for different data sources and complexity levels.\n\n### QA Entry Structure\n\n```mermaid\ngraph TB\n    subgraph \"QA Entry Schema\"\n        Q[\"question: string\u003cbr/\u003eInformation seeking query\"]\n        A[\"answer: string\u003cbr/\u003eTarget response\"]\n        T[\"tag: string\u003cbr/\u003eData source category\"]\n    end\n    \n    subgraph \"Tag Categories\"\n        E2H[\"e2hqa\u003cbr/\u003eEnd-to-End QA\"]\n        CRW[\"crawlqa\u003cbr/\u003eWeb Crawling QA\"]\n    end\n    \n    T --\u003e E2H\n    T --\u003e CRW\n```\n\n**QA Dataset Schema and Categories**\n\n### Question Categories and Complexity\n\n| Tag | Description | Example Count | Complexity Level |\n|-----|-------------|---------------|------------------|\n| `e2hqa` | End-to-end question answering | 100 entries | Multi-hop reasoning |\n| `crawlqa` | Web crawling based QA | 100 entries | Single-source retrieval |\n\n### Sample QA Patterns\n\n**Multi-hop Reasoning (e2hqa)**:\n- Questions requiring multiple information sources\n- Complex entity relationships\n- Historical and cultural knowledge integration\n\n**Web Crawling (crawlqa)**:\n- Direct information retrieval\n- Specific factual queries\n- Website-specific data extraction\n\nSources: [WebDancer/datasets/sample_qa.jsonl:1-201]()\n\n## Data Flow in Training Pipeline\n\nThe datasets integrate into WebDancer's training pipeline through multiple stages, supporting both supervised learning and reinforcement learning phases.\n\n```mermaid\ngraph TB\n    subgraph \"Dataset Sources\"\n        TRAJ_FILE[\"sample_traj.jsonl\"]\n        QA_FILE[\"sample_qa.jsonl\"]\n    end\n    \n    subgraph \"Data Processing\"\n        PARSER[\"ChatML Parser\"]\n        QA_PROC[\"QA Processor\"]\n        TOKENIZER[\"Tokenization\"]\n    end\n    \n    subgraph \"Training Components\"\n        SFT_DATA[\"SFT Training Data\"]\n        RL_TRAJ[\"RL Trajectories\"]\n        EVAL_SET[\"Evaluation Set\"]\n    end\n    \n    subgraph \"Model Training\"\n        SFT_STAGE[\"Supervised Fine-tuning\"]\n        RL_STAGE[\"DAPO Reinforcement Learning\"]\n    end\n    \n    TRAJ_FILE --\u003e PARSER\n    QA_FILE --\u003e QA_PROC\n    \n    PARSER --\u003e TOKENIZER\n    QA_PROC --\u003e TOKENIZER\n    \n    TOKENIZER --\u003e SFT_DATA\n    TOKENIZER --\u003e RL_TRAJ\n    TOKENIZER --\u003e EVAL_SET\n    \n    SFT_DATA --\u003e SFT_STAGE\n    RL_TRAJ --\u003e RL_STAGE\n    \n    SFT_STAGE --\u003e RL_STAGE\n```\n\n**Training Data Pipeline Integration**\n\n### Trajectory Sampling Process\n\nThe training pipeline uses trajectory data for:\n\n1. **Initial Supervised Fine-tuning**: Learning basic ReAct patterns\n2. **Trajectory Generation**: Creating diverse reasoning paths\n3. **Reward Model Training**: Evaluating response quality\n4. **Policy Optimization**: DAPO reinforcement learning\n\n### Evaluation Integration\n\nBoth datasets contribute to evaluation through:\n- **Question Answering Accuracy**: Direct QA performance\n- **Trajectory Quality**: Reasoning coherence and tool usage\n- **Information Seeking Success**: End-to-end task completion\n\nSources: [WebDancer/datasets/sample_traj.jsonl:1-6](), [WebDancer/datasets/sample_qa.jsonl:1-10]()\n\n## Usage in WebDancer Training\n\nThe datasets support WebDancer's autonomous information seeking capabilities through structured training on multi-step reasoning and tool usage patterns.\n\n### Training Data Characteristics\n\n| Aspect | Trajectory Data | QA Data |\n|--------|----------------|---------|\n| Format | ChatML conversations | Question-answer pairs |\n| Purpose | ReAct pattern learning | Task completion training |\n| Complexity | Multi-turn interactions | Single-turn responses |\n| Tool Usage | Explicit tool calls | Implicit tool requirements |\n\n### Integration with Training Stages\n\n```mermaid\ngraph LR\n    subgraph \"Stage 1: Data Construction\"\n        DC[\"Browsing Data\u003cbr/\u003eConstruction\"]\n    end\n    \n    subgraph \"Stage 2: Trajectory Sampling\"\n        TS[\"Expert Trajectory\u003cbr/\u003eGeneration\"]\n    end\n    \n    subgraph \"Stage 3: Supervised Fine-tuning\"\n        SFT[\"ReAct Pattern\u003cbr/\u003eLearning\"]\n    end\n    \n    subgraph \"Stage 4: Reinforcement Learning\"\n        RL[\"DAPO Policy\u003cbr/\u003eOptimization\"]\n    end\n    \n    DC --\u003e TS\n    TS --\u003e SFT\n    SFT --\u003e RL\n    \n    DC -.-\u003e |\"sample_qa.jsonl\"| SFT\n    TS -.-\u003e |\"sample_traj.jsonl\"| SFT\n    SFT -.-\u003e |\"Generated trajectories\"| RL\n```\n\n**Four-Stage Training Integration**\n\nThe datasets enable WebDancer to develop sophisticated information seeking behaviors through exposure to diverse reasoning patterns, tool usage strategies, and question-answering scenarios that span multiple domains and complexity levels.\n\nSources: [WebDancer/datasets/sample_traj.jsonl:1-6](), [WebDancer/datasets/sample_qa.jsonl:1-201]()"])</script><script>self.__next_f.push([1,"1f:T3271,"])</script><script>self.__next_f.push([1,"# WebSailor\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [WebSailor/README.md](WebSailor/README.md)\n- [assets/aliyun.png](assets/aliyun.png)\n\n\u003c/details\u003e\n\n\n\nWebSailor implements a post-training methodology for super-human reasoning in web navigation tasks. The system provides specialized training techniques for handling high-uncertainty information seeking through the `SailorFog-QA` data synthesis pipeline and `DUPO` reinforcement learning algorithm.\n\nKey implementation components include the `MultiTurnReactAgent` class for web interaction, `SGLang` server orchestration for model deployment, and integration with `Search` and `Visit` tools for web information retrieval.\n\nFor training pipeline implementation, see [Training Pipeline](#4.1). For `MultiTurnReactAgent` architecture and SGLang deployment, see [Architecture and Implementation](#4.2). For `Search` and `Visit` tool components, see [Tools and Components](#4.3).\n\n## Overview\n\nWebSailor provides post-training methodology for web agents handling Level 3 difficulty tasks characterized by high uncertainty and non-linear solution paths. The implementation consists of three core modules:\n\n### Core Components\n\n| Component | Implementation | Purpose |\n|-----------|---------------|---------|\n| `SailorFog-QA` | Knowledge graph construction and information obfuscation | High-uncertainty task generation |\n| `RFT` | Rejection sampling fine-tuning | Cold-start training phase |\n| `DUPO` | Duplicating Sampling Policy Optimization | Agentic reinforcement learning |\n\n### System Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Data Pipeline\"\n        SFQA[\"sailorfog-QA.jsonl\u003cbr/\u003eDataset\"]\n        KG[\"Knowledge Graph\u003cbr/\u003eConstruction\"]\n        OBF[\"Information\u003cbr/\u003eObfuscation\"]\n    end\n    \n    subgraph \"Training Modules\"\n        RFT[\"RFT Cold Start\u003cbr/\u003eModule\"]\n        DUPO[\"DUPO RL\u003cbr/\u003eAlgorithm\"]\n        VERL[\"verl Framework\u003cbr/\u003eIntegration\"]\n    end\n    \n    subgraph \"Deployment Infrastructure\"\n        SGLANG1[\"sglang_server_6001\u003cbr/\u003eOriginal Model\"]\n        SGLANG2[\"sglang_server_6002\u003cbr/\u003eSummary Model\"]\n        AGENT[\"MultiTurnReactAgent\u003cbr/\u003eImplementation\"]\n    end\n    \n    subgraph \"Tool Layer\"\n        SEARCH[\"Search Tool\u003cbr/\u003eGoogle Serper API\"]\n        VISIT[\"Visit Tool\u003cbr/\u003eJina Reader API\"]\n    end\n    \n    KG --\u003e OBF\n    OBF --\u003e SFQA\n    SFQA --\u003e RFT\n    RFT --\u003e DUPO\n    DUPO --\u003e VERL\n    \n    VERL --\u003e SGLANG1\n    VERL --\u003e SGLANG2\n    SGLANG1 --\u003e AGENT\n    SGLANG2 --\u003e AGENT\n    \n    AGENT --\u003e SEARCH\n    AGENT --\u003e VISIT\n```\n\nWebSailor-7B achieves state-of-the-art performance among open-source agents, outperforming larger baseline models through specialized post-training techniques.\n\nSources: [WebSailor/README.md:1-122](), [WebSailor/README.md:96]()\n\n## Implementation Architecture\n\nWebSailor implements a three-phase training methodology through specific code modules and data pipelines:\n\n### Training Phase Implementation\n\n```mermaid\ngraph TD\n    subgraph \"Phase 1: Data Synthesis\"\n        DATASET[\"dataset/sailorfog-QA.jsonl\u003cbr/\u003eTraining Data\"]\n        CONSTRUCT[\"Knowledge Graph\u003cbr/\u003eConstruction Module\"]\n        OBFUSCATE[\"Information\u003cbr/\u003eObfuscation Pipeline\"]\n    end\n    \n    subgraph \"Phase 2: Cold Start Training\"\n        TRAJECTORY[\"Expert Trajectory\u003cbr/\u003eGeneration\"]\n        RECONSTRUCT[\"Reasoning\u003cbr/\u003eReconstruction\"]\n        RFT_MODULE[\"RFT Training\u003cbr/\u003eModule\"]\n    end\n    \n    subgraph \"Phase 3: Reinforcement Learning\"\n        DUPO_ALGO[\"DUPO Algorithm\u003cbr/\u003eImplementation\"]\n        VERL_FRAMEWORK[\"verl Framework\u003cbr/\u003eIntegration\"]\n        POLICY_REFINE[\"Policy\u003cbr/\u003eRefinement\"]\n    end\n    \n    CONSTRUCT --\u003e OBFUSCATE\n    OBFUSCATE --\u003e DATASET\n    DATASET --\u003e TRAJECTORY\n    TRAJECTORY --\u003e RECONSTRUCT\n    RECONSTRUCT --\u003e RFT_MODULE\n    RFT_MODULE --\u003e DUPO_ALGO\n    DUPO_ALGO --\u003e VERL_FRAMEWORK\n    VERL_FRAMEWORK --\u003e POLICY_REFINE\n```\n\n### Task Classification Framework\n\nWebSailor implements a three-level difficulty classification system for information-seeking tasks:\n\n| Level | Implementation | Characteristics | Data Examples |\n|-------|---------------|----------------|---------------|\n| Level 1 | Direct query processing | Low uncertainty, linear paths | Simple factual lookups |\n| Level 2 | Multi-step reasoning | Moderate uncertainty, structured logic | Comparative analysis tasks |\n| Level 3 | `SailorFog-QA` pipeline | High uncertainty, non-linear exploration | Complex research scenarios |\n\nThe `SailorFog-QA` dataset specifically targets Level 3 tasks, providing training data for scenarios requiring creative exploration and synthesis across uncertain information landscapes.\n\nSources: [WebSailor/README.md:11-17](), [WebSailor/README.md:96]()\n\n## SailorFog-QA Pipeline Implementation\n\nThe `SailorFog-QA` data synthesis pipeline generates high-uncertainty training data through systematic knowledge graph manipulation and information obfuscation techniques:\n\n### Pipeline Architecture\n\n```mermaid\nflowchart TD\n    subgraph \"Graph Construction Module\"\n        EXTRACT[\"Entity Extraction\u003cbr/\u003ePipeline\"]\n        MAP[\"Relation Mapping\u003cbr/\u003eAlgorithm\"] \n        ASSEMBLE[\"Graph Assembly\u003cbr/\u003eComponent\"]\n    end\n    \n    subgraph \"Obfuscation Engine\"\n        HIDE[\"Information Hiding\u003cbr/\u003eStrategy\"]\n        SCATTER[\"Path Scattering\u003cbr/\u003eAlgorithm\"]\n        INJECT[\"Uncertainty Injection\u003cbr/\u003eModule\"]\n    end\n    \n    subgraph \"QA Generation System\"\n        FORMULATE[\"Question Formulation\u003cbr/\u003eGenerator\"]\n        VALIDATE[\"Ground Truth\u003cbr/\u003eValidator\"]\n        OUTPUT[\"sailorfog-QA.jsonl\u003cbr/\u003eOutput\"]\n    end\n    \n    EXTRACT --\u003e MAP\n    MAP --\u003e ASSEMBLE\n    ASSEMBLE --\u003e HIDE\n    HIDE --\u003e SCATTER\n    SCATTER --\u003e INJECT\n    INJECT --\u003e FORMULATE\n    FORMULATE --\u003e VALIDATE\n    VALIDATE --\u003e OUTPUT\n```\n\n### Data Format Structure\n\nThe `sailorfog-QA.jsonl` dataset contains training examples with the following structure:\n\n```json\n{\n  \"question\": \"High-uncertainty query requiring exploration\",\n  \"answer\": \"Ground truth response\",\n  \"obfuscation_level\": \"Level 3\",\n  \"knowledge_graph\": \"Fragmented graph structure\",\n  \"exploration_path\": \"Non-linear solution trajectory\"\n}\n```\n\nThe obfuscation algorithms ensure generated tasks require multi-step exploration and cannot be solved through direct pattern matching or memorized responses.\n\nSources: [WebSailor/README.md:13-14](), [WebSailor/README.md:96]()\n\n## Training and Deployment Architecture\n\nWebSailor implements a distributed training and evaluation architecture using `SGLang` servers and the `verl` framework:\n\n### Infrastructure Components\n\n```mermaid\ngraph TB\n    subgraph \"SGLang Deployment\"\n        SGLANG_6001[\"sglang_server_6001\u003cbr/\u003eWebSailor Model\u003cbr/\u003ePort 6001\"]\n        SGLANG_6002[\"sglang_server_6002\u003cbr/\u003eSummary Model\u003cbr/\u003ePort 6002\"]\n        DEPLOY_SCRIPT[\"scripts/deploy_model.sh\u003cbr/\u003eDeployment Script\"]\n    end\n    \n    subgraph \"Training Framework\"\n        RFT_TRAINER[\"RFT Training\u003cbr/\u003eComponent\"]\n        DUPO_TRAINER[\"DUPO Algorithm\u003cbr/\u003eImplementation\"]\n        VERL_ENGINE[\"verl Framework\u003cbr/\u003eEngine\"]\n    end\n    \n    subgraph \"Agent Implementation\"\n        MULTIAGENT[\"MultiTurnReactAgent\u003cbr/\u003eClass\"]\n        REACT_LOOP[\"ReAct Framework\u003cbr/\u003eLoop\"]\n        ACTION_PARSER[\"Action Parser\u003cbr/\u003eModule\"]\n    end\n    \n    subgraph \"Evaluation System\"\n        TEST_SCRIPT[\"src/scripts/test.sh\u003cbr/\u003eEvaluation Script\"]\n        THREAD_EXECUTOR[\"ThreadPoolExecutor\u003cbr/\u003eConcurrent Processing\"]\n        EVAL_METRICS[\"Evaluation Metrics\u003cbr/\u003eCalculator\"]\n    end\n    \n    subgraph \"External Integrations\"\n        SERPER_API[\"Google Serper API\u003cbr/\u003eGOOGLE_SEARCH_KEY\"]\n        JINA_API[\"Jina Reader API\u003cbr/\u003eJINA_API_KEY\"]\n        DASHSCOPE_API[\"DashScope API\u003cbr/\u003eDASHSCOPE_API_KEY\"]\n    end\n    \n    DEPLOY_SCRIPT --\u003e SGLANG_6001\n    DEPLOY_SCRIPT --\u003e SGLANG_6002\n    \n    SGLANG_6001 --\u003e RFT_TRAINER\n    SGLANG_6002 --\u003e DUPO_TRAINER\n    RFT_TRAINER --\u003e VERL_ENGINE\n    DUPO_TRAINER --\u003e VERL_ENGINE\n    \n    VERL_ENGINE --\u003e MULTIAGENT\n    MULTIAGENT --\u003e REACT_LOOP\n    REACT_LOOP --\u003e ACTION_PARSER\n    \n    MULTIAGENT --\u003e TEST_SCRIPT\n    TEST_SCRIPT --\u003e THREAD_EXECUTOR\n    THREAD_EXECUTOR --\u003e EVAL_METRICS\n    \n    MULTIAGENT --\u003e SERPER_API\n    MULTIAGENT --\u003e JINA_API\n    MULTIAGENT --\u003e DASHSCOPE_API\n```\n\n### Implementation Details\n\nThe training architecture uses dual `SGLang` servers for specialized model serving:\n- **Port 6001**: Primary WebSailor model for inference\n- **Port 6002**: Summary model for response processing\n\nThe `MultiTurnReactAgent` class implements the core reasoning loop, integrating with external APIs through standardized tool interfaces.\n\nSources: [WebSailor/README.md:62-65](), [WebSailor/README.md:105-108]()\n\n## Model Variants and Benchmark Results\n\nWebSailor provides multiple model configurations optimized for different deployment scenarios:\n\n### Model Specifications\n\n| Model | Parameters | HuggingFace Repository | Primary Benchmarks |\n|-------|------------|----------------------|-------------------|\n| `WebSailor-3B` | 3 billion | `Alibaba-NLP/WebSailor-3B` | BrowseComp, GAIA |\n| `WebSailor-7B` | 7 billion | `Alibaba-NLP/WebSailor-7B` | BrowseComp, SimpleQA |\n| `WebSailor-32B` | 32 billion | Available soon | All evaluation sets |\n| `WebSailor-72B` | 72 billion | Proprietary | BrowseComp-en/zh |\n\n### Evaluation Dataset Requirements\n\nThe evaluation pipeline requires specific dataset files in `src/eval_data/`:\n\n```bash\nsrc/eval_data/\nâ”œâ”€â”€ browsecomp_en.jsonl\nâ”œâ”€â”€ browsecomp_zh.jsonl  \nâ”œâ”€â”€ gaia.jsonl\nâ”œâ”€â”€ xbench-deepsearch.jsonl\nâ””â”€â”€ example.jsonl\n```\n\n### Performance Results\n\n| Benchmark | WebSailor-72B | Key Metrics |\n|-----------|---------------|-------------|\n| **BrowseComp-en** | 12.0% | Open-source SOTA |\n| **BrowseComp-zh** | 30.1% | Multilingual capability |\n| **GAIA** | 55.4% | Competitive with proprietary |\n| **SimpleQA** | Strong performance | Cross-difficulty generalization |\n\nThe `test.sh` script performs three inference runs and aggregates results for final evaluation scoring.\n\nSources: [WebSailor/README.md:19-34](), [WebSailor/README.md:49-58]()\n\n## WebAgent Ecosystem Integration\n\nWebSailor implements post-training methodology within the WebAgent framework, integrating with other components through shared datasets and evaluation pipelines:\n\n### Component Interactions\n\n```mermaid\ngraph TB\n    subgraph \"WebAgent Components\"\n        WEBSHAPER[\"WebShaper/\u003cbr/\u003eFormalization-driven\u003cbr/\u003eData Synthesis\"]\n        WEBSAILOR[\"WebSailor/\u003cbr/\u003ePost-training\u003cbr/\u003eMethodology\"]\n        WEBDANCER[\"WebDancer/\u003cbr/\u003eAutonomous\u003cbr/\u003eInformation Seeking\"]\n        WEBWALKER[\"WebWalker/\u003cbr/\u003eBenchmarking\u003cbr/\u003eFramework\"]\n    end\n    \n    subgraph \"Dataset Artifacts\"\n        WSQA_DATA[\"WebShaperQA\u003cbr/\u003edataset/\"]\n        SFQA_DATA[\"dataset/sailorfog-QA.jsonl\u003cbr/\u003eSailorFog-QA\"]\n        WWQA_DATA[\"WebWalkerQA\u003cbr/\u003ebenchmark/\"]\n    end\n    \n    subgraph \"Shared Tools\"\n        SEARCH_TOOL[\"Search Tool\u003cbr/\u003eGoogle Serper API\"]\n        VISIT_TOOL[\"Visit Tool\u003cbr/\u003eJina Reader API\"]\n        MULTIAGENT_IMPL[\"MultiTurnReactAgent\u003cbr/\u003eImplementation\"]\n    end\n    \n    subgraph \"Evaluation Infrastructure\"\n        GAIA_EVAL[\"GAIA Benchmark\u003cbr/\u003eEvaluation\"]\n        BC_EVAL[\"BrowseComp\u003cbr/\u003een/zh Evaluation\"]\n        SIMPLE_EVAL[\"SimpleQA\u003cbr/\u003eEvaluation\"]\n    end\n    \n    WEBSHAPER --\u003e WSQA_DATA\n    WEBSAILOR --\u003e SFQA_DATA\n    WEBWALKER --\u003e WWQA_DATA\n    \n    WEBSAILOR --\u003e SEARCH_TOOL\n    WEBSAILOR --\u003e VISIT_TOOL\n    WEBSAILOR --\u003e MULTIAGENT_IMPL\n    \n    WEBDANCER --\u003e SEARCH_TOOL\n    WEBDANCER --\u003e VISIT_TOOL\n    WEBDANCER --\u003e MULTIAGENT_IMPL\n    \n    WEBSAILOR --\u003e GAIA_EVAL\n    WEBSAILOR --\u003e BC_EVAL\n    WEBSAILOR --\u003e SIMPLE_EVAL\n```\n\n### Shared Infrastructure\n\nWebSailor leverages common WebAgent components:\n- **Tool Integration**: Standardized `Search` and `Visit` tool interfaces\n- **Agent Framework**: `MultiTurnReactAgent` class for web interaction\n- **Evaluation Pipeline**: Shared benchmark datasets and scoring mechanisms\n- **Deployment**: Common `SGLang` server deployment patterns\n\nThe `DUPO` reinforcement learning algorithm and `SailorFog-QA` pipeline represent WebSailor's unique contributions to the ecosystem.\n\nSources: [README.md:74-81](), [WebSailor/README.md:1-122]()\n\n## Quick Start and Deployment\n\nWebSailor provides streamlined deployment through standardized scripts and model distribution:\n\n### Environment Setup\n```bash\nconda create -n websailor python=3.11\npip install -r requirements.txt\n```\n\n### Model Access\nModels are distributed via Hugging Face Hub at `Alibaba-NLP/WebSailor-3B` and related repositories for larger variants.\n\n### Evaluation Infrastructure\nThe evaluation system uses `test.sh` script that:\n1. Launches dual SGLang servers (ports 6001, 6002)\n2. Performs three inference runs\n3. Conducts evaluation based on aggregated results\n4. Requires Google Search and Jina API keys\n\nSources: [WebSailor/README.md:36-66]()"])</script><script>self.__next_f.push([1,"20:T2b7a,"])</script><script>self.__next_f.push([1,"# Training Pipeline\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebSailor/README.md](WebSailor/README.md)\n\n\u003c/details\u003e\n\n\n\nThis page documents WebSailor's comprehensive three-stage training methodology that enables super-human reasoning for web navigation tasks. The training pipeline transforms base language models into specialized web agents capable of handling high-uncertainty information-seeking scenarios.\n\nFor information about WebSailor's architecture and implementation details, see [Architecture and Implementation](#4.2). For details about the tools and components used during training, see [Tools and Components](#4.3).\n\n## Overview\n\nWebSailor employs a sophisticated three-stage training paradigm designed to progressively build web navigation capabilities:\n\n1. **Expert Trajectories Generation** - Creates high-quality demonstration data through SailorFog-QA pipeline\n2. **RFT Cold Start** - Establishes baseline capabilities using Rejection Sampling Fine-tuning  \n3. **DUPO Reinforcement Learning** - Refines exploratory strategies using Duplicating Sampling Policy Optimization\n\nThis approach addresses the extreme uncertainty inherent in web information-seeking tasks by first providing structured supervision, then bootstrapping performance through rejection sampling, and finally optimizing exploration strategies through reinforcement learning.\n\n## Training Pipeline Architecture\n\n```mermaid\ngraph TD\n    subgraph \"Stage 1: Expert Trajectories\"\n        A[\"SailorFog-QA Pipeline\"] --\u003e B[\"Knowledge Graph Construction\"]\n        B --\u003e C[\"Information Obfuscation\"]\n        C --\u003e D[\"High-Uncertainty Questions\"]\n        D --\u003e E[\"Expert Trajectory Generation\"]\n        E --\u003e F[\"sailorfog-QA.jsonl\"]\n    end\n    \n    subgraph \"Stage 2: RFT Cold Start\"\n        F --\u003e G[\"Rejection Sampling Fine-tuning\"]\n        G --\u003e H[\"High-Quality Example Selection\"]\n        H --\u003e I[\"Baseline Capability Establishment\"]\n        I --\u003e J[\"RFT Checkpoint\"]\n    end\n    \n    subgraph \"Stage 3: DUPO RL\"\n        J --\u003e K[\"Duplicating Sampling Policy Optimization\"]\n        K --\u003e L[\"verl Framework\"]\n        L --\u003e M[\"Exploratory Strategy Refinement\"]\n        M --\u003e N[\"Final WebSailor Model\"]\n    end\n    \n    subgraph \"Data Sources\"\n        O[\"Complex Knowledge Graphs\"] --\u003e B\n        P[\"Information Landscapes\"] --\u003e C\n    end\n    \n    subgraph \"Training Infrastructure\"\n        Q[\"SGLang Server\"] --\u003e G\n        Q --\u003e K\n        R[\"Multi-GPU Setup\"] --\u003e L\n    end\n    \n    O --\u003e A\n    P --\u003e A\n    Q --\u003e E\n    R --\u003e G\n```\n\n**Training Pipeline Flow Diagram**\n\nThis diagram illustrates the complete training pipeline from data construction through final model optimization. Each stage builds upon the previous one to create increasingly sophisticated web navigation capabilities.\n\nSources: [WebSailor/README.md:88-109]()\n\n## Stage 1: Expert Trajectories Generation\n\nThe first stage focuses on creating high-quality training data through the SailorFog-QA pipeline. This process constructs challenging information-seeking scenarios that require complex reasoning and exploration.\n\n### SailorFog-QA Data Construction\n\n```mermaid\ngraph LR\n    subgraph \"Knowledge Graph Processing\"\n        A[\"Complex KG Sources\"] --\u003e B[\"Graph Sampling\"]\n        B --\u003e C[\"Entity Relationship Mapping\"]\n        C --\u003e D[\"Multi-hop Path Construction\"]\n    end\n    \n    subgraph \"Information Obfuscation\"\n        D --\u003e E[\"Uncertainty Injection\"]\n        E --\u003e F[\"Non-linear Solution Paths\"]\n        F --\u003e G[\"Level 3 Difficulty Tasks\"]\n    end\n    \n    subgraph \"Quality Control\"\n        G --\u003e H[\"Expert Validation\"]\n        H --\u003e I[\"Trajectory Verification\"]\n        I --\u003e J[\"sailorfog-QA.jsonl\"]\n    end\n    \n    subgraph \"Output Format\"\n        J --\u003e K[\"Question-Answer Pairs\"]\n        K --\u003e L[\"Multi-step Reasoning Chains\"]\n        L --\u003e M[\"Tool Usage Patterns\"]\n    end\n```\n\n**SailorFog-QA Construction Pipeline**\n\nThe SailorFog-QA pipeline creates Level 3 difficulty tasks characterized by high uncertainty and complex, non-linear solution paths. The process begins with knowledge graph sampling and applies information obfuscation techniques to generate questions requiring creative exploration.\n\n### Data Format and Structure\n\nThe generated training data follows a structured format that captures both the question complexity and the required reasoning trajectory:\n\n| Component | Description | Purpose |\n|-----------|-------------|---------|\n| Question | High-uncertainty information-seeking query | Drives exploration behavior |\n| Context | Obfuscated knowledge graph fragments | Provides partial information |\n| Trajectory | Multi-step reasoning and action sequence | Supervision for training |\n| Tools | Search and Visit tool usage patterns | Teaches tool interaction |\n\nSources: [WebSailor/README.md:13-16](), [WebSailor/README.md:90-96]()\n\n## Stage 2: RFT Cold Start\n\nThe Rejection Sampling Fine-tuning (RFT) stage establishes baseline capabilities by training on a curated set of high-quality examples. This \"cold start\" approach addresses the challenge of bootstrapping performance before reinforcement learning.\n\n### RFT Implementation Process\n\n```mermaid\nsequenceDiagram\n    participant Base as \"Base Language Model\"\n    participant RFT as \"RFT Process\"\n    participant Expert as \"Expert Trajectories\"\n    participant Sampler as \"Rejection Sampler\"\n    participant Model as \"RFT Checkpoint\"\n    \n    Base-\u003e\u003eRFT: \"Initialize with base weights\"\n    Expert-\u003e\u003eRFT: \"Load sailorfog-QA.jsonl\"\n    RFT-\u003e\u003eSampler: \"Generate candidate responses\"\n    Sampler-\u003e\u003eExpert: \"Compare with expert trajectories\"\n    Expert-\u003e\u003eSampler: \"Quality scores\"\n    Sampler-\u003e\u003eRFT: \"Accept/reject samples\"\n    RFT-\u003e\u003eModel: \"Update weights on accepted samples\"\n    Model-\u003e\u003eRFT: \"Improved baseline capability\"\n```\n\n**RFT Cold Start Sequence**\n\nThe RFT process uses rejection sampling to select high-quality training examples, avoiding the stylistic and verbosity issues common in teacher model supervision. This creates a clean baseline for subsequent reinforcement learning.\n\n### Training Configuration\n\nThe RFT stage uses specific hyperparameters and training strategies optimized for web navigation tasks:\n\n- **Sampling Strategy**: Rejection sampling with expert trajectory comparison\n- **Quality Threshold**: Configurable acceptance criteria for generated responses\n- **Batch Processing**: Efficient handling of trajectory sequences\n- **Checkpoint Management**: Intermediate model saving for stage transitions\n\nSources: [WebSailor/README.md:15-16]()\n\n## Stage 3: DUPO Reinforcement Learning\n\nThe final stage employs Duplicating Sampling Policy Optimization (DUPO) to refine the agent's exploratory strategies. This reinforcement learning approach optimizes the model's ability to navigate uncertain information landscapes.\n\n### DUPO Algorithm Architecture\n\n```mermaid\ngraph TD\n    subgraph \"DUPO Framework\"\n        A[\"RFT Checkpoint\"] --\u003e B[\"Policy Network\"]\n        B --\u003e C[\"Action Sampling\"]\n        C --\u003e D[\"Environment Interaction\"]\n        D --\u003e E[\"Reward Computation\"]\n        E --\u003e F[\"Policy Gradient Update\"]\n        F --\u003e B\n    end\n    \n    subgraph \"Duplicating Sampling\"\n        C --\u003e G[\"Multiple Trajectory Sampling\"]\n        G --\u003e H[\"Trajectory Duplication\"]\n        H --\u003e I[\"Variance Reduction\"]\n        I --\u003e E\n    end\n    \n    subgraph \"Environment Setup\"\n        D --\u003e J[\"Search Tool\"]\n        D --\u003e K[\"Visit Tool\"]\n        J --\u003e L[\"Google Serper API\"]\n        K --\u003e M[\"Jina Reader API\"]\n    end\n    \n    subgraph \"Training Infrastructure\"\n        N[\"verl Framework\"] --\u003e F\n        O[\"Multi-GPU Coordination\"] --\u003e N\n        P[\"SGLang Server\"] --\u003e C\n    end\n```\n\n**DUPO Reinforcement Learning Architecture**\n\nDUPO introduces a novel duplicating sampling mechanism that generates multiple trajectories for each training example, reducing variance and improving policy optimization stability.\n\n### VERL Integration\n\nWebSailor integrates with the `verl` framework for distributed reinforcement learning training:\n\n| Component | Role | Implementation |\n|-----------|------|----------------|\n| `verl` Framework | RL training orchestration | Distributed policy optimization |\n| Policy Network | Action selection | Fine-tuned language model |\n| Reward Model | Trajectory evaluation | Success-based reward computation |\n| Environment | Web interaction simulation | Tool-mediated web access |\n\nThe DUPO algorithm specifically addresses the challenge of exploration in high-dimensional web navigation spaces by duplicating successful sampling strategies and optimizing for diverse solution paths.\n\nSources: [WebSailor/README.md:15-16](), [WebSailor/README.md:108-109]()\n\n## Training Infrastructure Requirements\n\n### Computational Resources\n\nThe three-stage training pipeline requires substantial computational resources:\n\n```mermaid\ngraph LR\n    subgraph \"Hardware Requirements\"\n        A[\"Multi-GPU Setup\"] --\u003e B[\"SGLang Server Deployment\"]\n        B --\u003e C[\"Model Inference\"]\n        C --\u003e D[\"Trajectory Generation\"]\n    end\n    \n    subgraph \"Software Stack\"\n        E[\"verl Framework\"] --\u003e F[\"RL Training\"]\n        G[\"SGLang\"] --\u003e H[\"Model Serving\"]\n        I[\"Python 3.11\"] --\u003e J[\"Environment Setup\"]\n    end\n    \n    subgraph \"External Dependencies\"\n        K[\"Google Serper API\"] --\u003e L[\"Search Functionality\"]\n        M[\"Jina Reader API\"] --\u003e N[\"Content Processing\"]\n        O[\"HuggingFace Models\"] --\u003e P[\"Base Model Weights\"]\n    end\n    \n    A --\u003e E\n    B --\u003e G\n    J --\u003e I\n    L --\u003e D\n    N --\u003e D\n    P --\u003e C\n```\n\n**Training Infrastructure Components**\n\n### Model Deployment Pipeline\n\nThe training process requires careful orchestration of model serving and training components:\n\n1. **SGLang Server Setup**: Hosts both evaluation models and summary models (Qwen2.5-72B-Instruct)\n2. **API Configuration**: Requires Google search keys and Jina keys for tool functionality  \n3. **Multi-instance Training**: Supports parallel trajectory generation and model updates\n4. **Checkpoint Management**: Automated saving and loading between training stages\n\nSources: [WebSailor/README.md:38-65]()\n\n## Performance Validation\n\n### Training Metrics\n\nThe training pipeline incorporates multiple validation mechanisms:\n\n| Metric | Stage | Purpose |\n|--------|-------|---------|\n| Trajectory Quality | Expert Generation | Ensures high-quality supervision |\n| Acceptance Rate | RFT Cold Start | Monitors rejection sampling effectiveness |\n| Exploration Diversity | DUPO RL | Validates policy exploration |\n| Task Success Rate | All Stages | Measures overall capability development |\n\n### Benchmark Integration\n\nThe training pipeline is validated against multiple benchmarks:\n\n- **BrowseComp-en/zh**: Complex web navigation tasks\n- **SimpleQA**: Generalization validation  \n- **GAIA**: Multi-modal reasoning evaluation\n- **XBench-DeepSearch**: Advanced search capabilities\n\nEach training stage can be evaluated independently to ensure progressive capability development and identify potential training issues early in the pipeline.\n\nSources: [WebSailor/README.md:19-34](), [WebSailor/README.md:49-57]()"])</script><script>self.__next_f.push([1,"21:T314e,"])</script><script>self.__next_f.push([1,"# Architecture and Implementation\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebSailor/README.md](WebSailor/README.md)\n- [WebSailor/src/react_agent.py](WebSailor/src/react_agent.py)\n- [WebSailor/src/run.sh](WebSailor/src/run.sh)\n- [WebSailor/src/run_multi_react.py](WebSailor/src/run_multi_react.py)\n\n\u003c/details\u003e\n\n\n\nThis document covers the technical architecture and implementation details of WebSailor's core components, including the `MultiTurnReactAgent` implementation, SailorFog-QA data synthesis pipeline, and deployment infrastructure. For information about WebSailor's training methodology, see [Training Pipeline](#4.1). For details about specific tools and components, see [Tools and Components](#4.3).\n\n## Core Agent Architecture\n\nWebSailor's primary implementation is the `MultiTurnReactAgent` class, which extends the `FnCallAgent` framework to provide sophisticated reasoning capabilities for web navigation tasks. The agent operates using a ReAct (Reasoning and Acting) paradigm with structured tool calling and context management.\n\n### MultiTurnReactAgent Implementation\n\n```mermaid\nclassDiagram\n    class FnCallAgent {\n        +function_list\n        +llm\n        +system_message\n    }\n    \n    class MultiTurnReactAgent {\n        +llm_generate_cfg\n        +llm_local_path\n        +model\n        +user_prompt\n        +call_server(msgs, max_tries)\n        +count_tokens(messages, model)\n        +_run(data, model, user_prompt)\n        +_call_tool(tool_name, tool_args)\n    }\n    \n    class OpenAI {\n        +api_key\n        +base_url\n        +chat.completions.create()\n    }\n    \n    class BaseTool {\n        +search\n        +visit\n    }\n    \n    FnCallAgent \u003c|-- MultiTurnReactAgent\n    MultiTurnReactAgent --\u003e OpenAI : \"uses SGLang API\"\n    MultiTurnReactAgent --\u003e BaseTool : \"calls tools\"\n    \n    note for MultiTurnReactAgent \"Core agent with ReAct loop\\nand token management\"\n    note for OpenAI \"SGLang server at\\nhttp://127.0.0.1:6001/v1\"\n```\n\nThe `MultiTurnReactAgent` implements several key architectural patterns:\n\n- **Token Management**: Dynamic token counting using `count_tokens()` with automatic context truncation when approaching the `MAX_TOKEN_LENGTH` limit [WebSailor/src/react_agent.py:69-78]()\n- **Structured Tool Calling**: JSON-based tool invocation with error handling and response formatting [WebSailor/src/react_agent.py:103-113]()\n- **Iterative Reasoning**: Multi-round conversation with termination conditions based on answer generation or call limits [WebSailor/src/react_agent.py:94-162]()\n\nSources: WebSailor/src/react_agent.py\n\n### ReAct Loop Implementation\n\n```mermaid\nflowchart TD\n    Start[\"User Query Input\"] --\u003e Init[\"Initialize messages with\\nsystem_message + user_prompt\"]\n    Init --\u003e LLMCall[\"call_server(messages)\"]\n    LLMCall --\u003e ParseResp[\"Parse response for\\n\u003ctool_call\u003e and \u003canswer\u003e\"]\n    \n    ParseResp --\u003e HasTool{\"Contains\\n\u003ctool_call\u003e?\"}\n    HasTool --\u003e|Yes| ExtractTool[\"Extract JSON from\\n\u003ctool_call\u003e...\u003c/tool_call\u003e\"]\n    ExtractTool --\u003e CallTool[\"_call_tool(tool_name, tool_args)\"]\n    CallTool --\u003e ToolResp[\"Append \u003ctool_response\u003e\\nto messages\"]\n    \n    HasTool --\u003e|No| CheckAnswer{\"Contains\\n\u003canswer\u003e?\"}\n    CheckAnswer --\u003e|Yes| ExtractAnswer[\"Extract answer and\\nterminate with 'answer'\"]\n    CheckAnswer --\u003e|No| CheckLimits{\"num_llm_calls_available\\n\u003e 0 AND\\ntoken_count \u003c MAX_TOKEN_LENGTH?\"}\n    \n    ToolResp --\u003e TokenCheck[\"count_tokens(messages)\"]\n    TokenCheck --\u003e CheckLimits\n    \n    CheckLimits --\u003e|Yes| LLMCall\n    CheckLimits --\u003e|No| ForceAnswer[\"Force answer generation\\nwith context limit message\"]\n    ForceAnswer --\u003e ExtractAnswer\n    \n    ExtractAnswer --\u003e End[\"Return result with\\nprediction and termination\"]\n```\n\nThe ReAct loop enforces strict limits: `MAX_LLM_CALL_PER_RUN` (default 40) for reasoning steps and `MAX_TOKEN_LENGTH` (31K - 500 tokens) for context management [WebSailor/src/react_agent.py:15-16]().\n\nSources: WebSailor/src/react_agent.py:80-162\n\n## SailorFog-QA Pipeline Architecture\n\nWebSailor employs the SailorFog-QA data synthesis pipeline to generate high-uncertainty tasks that require complex reasoning patterns. The pipeline creates questions with initial uncertainty that demand creative exploration beyond simple structured reasoning.\n\n### Data Synthesis Components\n\n```mermaid\ngraph TB\n    subgraph \"Knowledge Graph Construction\"\n        KG[\"Knowledge Graphs\"] --\u003e Sample[\"Graph Sampling\"]\n        Sample --\u003e Entities[\"Entity Extraction\"]\n    end\n    \n    subgraph \"Information Obfuscation\"\n        Entities --\u003e Obfus[\"Information Obfuscation\\nProcess\"]\n        Obfus --\u003e Uncertainty[\"High Uncertainty\\nQuestion Generation\"]\n    end\n    \n    subgraph \"Task Classification\"\n        Uncertainty --\u003e Level1[\"Level 1: Simple Facts\"]\n        Uncertainty --\u003e Level2[\"Level 2: Multi-step Reasoning\"]\n        Uncertainty --\u003e Level3[\"Level 3: High Uncertainty\\nNon-linear Paths\"]\n    end\n    \n    subgraph \"Quality Control\"\n        Level3 --\u003e Validation[\"Task Validation\"]\n        Validation --\u003e SailorFogQA[\"SailorFog-QA Dataset\"]\n    end\n    \n    note for Level3 \"Target difficulty level\\nfor WebSailor training\"\n    note for SailorFogQA \"Available at:\\ndataset/sailorfog-QA.jsonl\"\n```\n\nThe SailorFog-QA pipeline focuses on generating Level 3 tasks characterized by both high uncertainty and complex, non-linear solution paths. These tasks transcend simple structured reasoning patterns and require sophisticated exploration strategies.\n\nSources: WebSailor/README.md:11-15, WebSailor/README.md:96\n\n## Deployment Architecture\n\nWebSailor uses a dual-server architecture with SGLang for efficient model inference and separate services for different model roles.\n\n### SGLang Server Configuration\n\n```mermaid\ngraph LR\n    subgraph \"Hardware Allocation\"\n        GPU01[\"CUDA_VISIBLE_DEVICES=0,1,2,3\"] --\u003e OrigServer[\"Original Model\\nSGLang Server\\nPort 6001\\nTP=2\"]\n        GPU47[\"CUDA_VISIBLE_DEVICES=4,5,6,7\"] --\u003e SummaryServer[\"Summary Model\\nSGLang Server\\nPort 6002\\nTP=4\"]\n    end\n    \n    subgraph \"Client Integration\"\n        ReactAgent[\"MultiTurnReactAgent\"] --\u003e OpenAIClient[\"OpenAI Client\\nbase_url=http://127.0.0.1:6001/v1\"]\n        SummaryTasks[\"Summary Tasks\"] --\u003e SummaryClient[\"OpenAI Client\\nbase_url=http://127.0.0.1:6002/v1\"]\n    end\n    \n    OpenAIClient --\u003e OrigServer\n    SummaryClient --\u003e SummaryServer\n    \n    subgraph \"Health Monitoring\"\n        HealthCheck[\"curl health checks\\nevery 10 seconds\"] --\u003e OrigServer\n        HealthCheck --\u003e SummaryServer\n    end\n```\n\nThe deployment script [WebSailor/src/run.sh:32-42]() automatically manages server lifecycle, including startup sequencing, health monitoring, and graceful shutdown. The original model server uses tensor parallelism (TP=2) while the summary model uses TP=4 for different performance characteristics.\n\nSources: WebSailor/src/run.sh:32-95\n\n### Server Startup Sequence\n\n```mermaid\nsequenceDiagram\n    participant Script as \"run.sh\"\n    participant OrigServer as \"Original Model Server\\n(Port 6001)\"\n    participant SummServer as \"Summary Model Server\\n(Port 6002)\"\n    participant HealthCheck as \"Health Monitor\"\n    \n    Script-\u003e\u003eOrigServer: Launch with CUDA_VISIBLE_DEVICES=0,1,2,3\n    Script-\u003e\u003eSummServer: Launch with CUDA_VISIBLE_DEVICES=4,5,6,7\n    \n    loop Every 10 seconds (max 3000s timeout)\n        HealthCheck-\u003e\u003eOrigServer: curl http://localhost:6001/v1/chat/completions\n        HealthCheck-\u003e\u003eSummServer: curl http://localhost:6002/v1/chat/completions\n        \n        alt Both servers ready\n            HealthCheck-\u003e\u003eScript: server1_ready=true \u0026\u0026 server2_ready=true\n            Script-\u003e\u003eScript: Break loop and proceed\n        else Timeout exceeded\n            HealthCheck-\u003e\u003eScript: Warning and proceed with available servers\n        end\n    end\n    \n    Script-\u003e\u003eScript: Start inference with run_multi_react.py\n```\n\nSources: WebSailor/src/run.sh:48-95\n\n## Multi-Worker Inference System\n\nWebSailor implements a concurrent inference system using `ThreadPoolExecutor` to handle multiple queries simultaneously while maintaining result consistency and error handling.\n\n### Concurrent Processing Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Input Processing\"\n        DataFile[\"eval_data/{dataset}.jsonl\"] --\u003e TaskGen[\"Task Generation\\nfor each rollout_idx\"]\n        TaskGen --\u003e FilterProc[\"Filter Processed\\nQuestions\"]\n    end\n    \n    subgraph \"Thread Pool Execution\"\n        FilterProc --\u003e ThreadPool[\"ThreadPoolExecutor\\nmax_workers=20\"]\n        ThreadPool --\u003e Worker1[\"Worker Thread 1\\nMultiTurnReactAgent._run()\"]\n        ThreadPool --\u003e Worker2[\"Worker Thread 2\\nMultiTurnReactAgent._run()\"]\n        ThreadPool --\u003e WorkerN[\"Worker Thread N\\nMultiTurnReactAgent._run()\"]\n    end\n    \n    subgraph \"Result Management\"\n        Worker1 --\u003e WriteLock[\"threading.Lock()\\nfor file writes\"]\n        Worker2 --\u003e WriteLock\n        WorkerN --\u003e WriteLock\n        WriteLock --\u003e OutputFile[\"Output File\\n{dataset}/iter{rollout_idx}.jsonl\"]\n    end\n    \n    subgraph \"Error Handling\"\n        Worker1 --\u003e ErrorLog[\"Exception Logging\\nwith error_result\"]\n        Worker2 --\u003e ErrorLog\n        WorkerN --\u003e ErrorLog\n        ErrorLog --\u003e OutputFile\n    end\n```\n\nThe system processes multiple rollouts (default 3) for each dataset, with each rollout generating independent predictions. The `write_lock` ensures thread-safe file operations [WebSailor/src/run_multi_react.py:144]().\n\nSources: WebSailor/src/run_multi_react.py:75-188\n\n### Task Execution Flow\n\n```mermaid\nflowchart TD\n    Start[\"Load dataset items\"] --\u003e CreateTasks[\"Create tasks for rollout_idx=1 to roll_out_count\"]\n    CreateTasks --\u003e CheckProcessed[\"Check processed_queries\\nfrom existing output files\"]\n    CheckProcessed --\u003e FilterTasks[\"Filter unprocessed questions\"]\n    \n    FilterTasks --\u003e InitAgent[\"Initialize MultiTurnReactAgent\\nwith llm_cfg and tools\"]\n    InitAgent --\u003e SubmitFutures[\"Submit futures to ThreadPoolExecutor\"]\n    \n    SubmitFutures --\u003e ExecuteParallel[\"Execute test_agent._run()\\nin parallel workers\"]\n    ExecuteParallel --\u003e CollectResults[\"Collect results with\\nas_completed() and tqdm\"]\n    \n    CollectResults --\u003e WriteResult[\"Thread-safe write to\\niter{rollout_idx}.jsonl\"]\n    WriteResult --\u003e NextRollout{\"More rollouts?\"}\n    \n    NextRollout --\u003e|Yes| CreateTasks\n    NextRollout --\u003e|No| Complete[\"All rollouts completed\"]\n    \n    ExecuteParallel --\u003e HandleError[\"Exception handling\\nwith error_result logging\"]\n    HandleError --\u003e WriteResult\n```\n\nEach worker processes a single question through the complete ReAct loop, with results written atomically to prevent data corruption during concurrent execution.\n\nSources: WebSailor/src/run_multi_react.py:74-188\n\n## Integration Patterns\n\nWebSailor follows several key integration patterns that enable modularity and extensibility across the system.\n\n### Tool Integration Interface\n\n```mermaid\nclassDiagram\n    class MultiTurnReactAgent {\n        +function_list: [\"search\", \"visit\"]\n        +_call_tool(tool_name, tool_args)\n    }\n    \n    class ToolRegistry {\n        +search: SearchTool\n        +visit: VisitTool\n    }\n    \n    class SearchTool {\n        +Google Serper API\n        +execute(query)\n    }\n    \n    class VisitTool {\n        +Jina Reader API\n        +execute(url)\n    }\n    \n    MultiTurnReactAgent --\u003e ToolRegistry : \"function_list\"\n    ToolRegistry --\u003e SearchTool\n    ToolRegistry --\u003e VisitTool\n    \n    note for MultiTurnReactAgent \"Tool calls via JSON:\\n{'name': 'search', 'arguments': {...}}\"\n    note for SearchTool \"External API integration\\nwith error handling\"\n```\n\nThe agent dynamically invokes tools based on JSON-structured calls within the ReAct loop, with standardized error handling for malformed tool calls [WebSailor/src/react_agent.py:106-112]().\n\nSources: WebSailor/src/react_agent.py:103-113, WebSailor/src/run_multi_react.py:139\n\n### Configuration Management\n\n| Component | Configuration Source | Key Parameters |\n|-----------|---------------------|----------------|\n| `MultiTurnReactAgent` | `llm_cfg` dict | `model`, `generate_cfg`, `model_type` |\n| SGLang Server | Environment variables | `MODEL_PATH`, `TEMPERATURE`, `MAX_WORKERS` |\n| Inference Pipeline | Command line args | `--dataset`, `--output`, `--roll_out_count` |\n| Tool APIs | Environment variables | Google Search key, Jina key |\n\nThe configuration system uses a layered approach with environment variables for deployment settings and structured dictionaries for model parameters [WebSailor/src/run_multi_react.py:124-133]().\n\nSources: WebSailor/src/run_multi_react.py:124-141, WebSailor/src/run.sh:1-13"])</script><script>self.__next_f.push([1,"22:T459b,"])</script><script>self.__next_f.push([1,"# Tools and Components\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebDancer/demos/tools/private/search.py](WebDancer/demos/tools/private/search.py)\n- [WebSailor/src/scripts/test.sh](WebSailor/src/scripts/test.sh)\n- [WebSailor/src/tool_search.py](WebSailor/src/tool_search.py)\n- [WebSailor/src/tool_visit.py](WebSailor/src/tool_visit.py)\n\n\u003c/details\u003e\n\n\n\nThis document covers WebSailor's core web interaction tools: the Search and Visit components that enable autonomous web information seeking. These tools provide the fundamental capabilities for searching the web and extracting content from webpages, serving as the interface layer between WebSailor agents and external web services.\n\nFor information about WebSailor's training methodology, see [Training Pipeline](#4.1). For the overall system architecture, see [Architecture and Implementation](#4.2).\n\n## Tool Architecture Overview\n\nWebSailor implements two primary web interaction tools that form the foundation of its information seeking capabilities. Both tools are registered with the qwen_agent framework and provide standardized interfaces for web operations.\n\n### Tool Integration Architecture\n\n```mermaid\nflowchart TD\n    subgraph \"WebSailor Agent Layer\"\n        MultiTurnAgent[\"MultiTurnReactAgent\"]\n        ToolCalling[\"Tool Calling Logic\"]\n    end\n    \n    subgraph \"Tool Registration\"\n        QwenAgent[\"qwen_agent.tools.base\"]\n        SearchReg[\"@register_tool('search')\"]\n        VisitReg[\"@register_tool('visit')\"]\n    end\n    \n    subgraph \"Core Tools\"\n        SearchTool[\"Search Tool Class\"]\n        VisitTool[\"Visit Tool Class\"]\n    end\n    \n    subgraph \"External APIs\"\n        GoogleSerper[\"Google Serper API\u003cbr/\u003egoogle.serper.dev\"]\n        JinaReader[\"Jina Reader API\u003cbr/\u003er.jina.ai\"]\n        SummaryModel[\"Summary Model\u003cbr/\u003eOpenAI-compatible API\"]\n    end\n    \n    MultiTurnAgent --\u003e ToolCalling\n    ToolCalling --\u003e QwenAgent\n    \n    QwenAgent --\u003e SearchReg\n    QwenAgent --\u003e VisitReg\n    \n    SearchReg --\u003e SearchTool\n    VisitReg --\u003e VisitTool\n    \n    SearchTool --\u003e GoogleSerper\n    VisitTool --\u003e JinaReader\n    VisitTool --\u003e SummaryModel\n```\n\n**Sources:** [WebSailor/src/tool_search.py:13-14](), [WebSailor/src/tool_visit.py:20-21]()\n\n## Search Tool Implementation\n\nThe `Search` class implements batched web search functionality through the Google Serper API, supporting concurrent query execution for efficient information retrieval.\n\n### Search Tool Core Architecture\n\n```mermaid\nflowchart TD\n    SearchClass[\"Search Class\u003cbr/\u003eBaseTool\"]\n    CallMethod[\"call(params)\"]\n    GoogleSearch[\"google_search(query)\"]\n    ThreadPool[\"ThreadPoolExecutor\u003cbr/\u003emax_workers=3\"]\n    \n    subgraph \"API Integration\"\n        SerperAPI[\"google.serper.dev/search\"]\n        APIHeaders[\"X-API-KEY header\"]\n        RequestData[\"JSON payload with query\"]\n    end\n    \n    subgraph \"Response Processing\"\n        OrganicResults[\"Extract organic results\"]\n        FormatSnippets[\"Format as markdown\"]\n        JoinResults[\"Join multiple queries\"]\n    end\n    \n    SearchClass --\u003e CallMethod\n    CallMethod --\u003e GoogleSearch\n    CallMethod --\u003e ThreadPool\n    \n    ThreadPool --\u003e GoogleSearch\n    GoogleSearch --\u003e SerperAPI\n    \n    SerperAPI --\u003e OrganicResults\n    OrganicResults --\u003e FormatSnippets\n    FormatSnippets --\u003e JoinResults\n    \n    subgraph \"Error Handling\"\n        RetryLogic[\"5 retry attempts\"]\n        TimeoutHandle[\"Graceful timeout\"]\n        NoResults[\"No results fallback\"]\n    end\n    \n    SerperAPI -.-\u003e RetryLogic\n    RetryLogic -.-\u003e TimeoutHandle\n    OrganicResults -.-\u003e NoResults\n```\n\n**Sources:** [WebSailor/src/tool_search.py:1-104](), [WebDancer/demos/tools/private/search.py:1-100]()\n\n### Search Tool Parameters and Interface\n\nThe `Search` tool accepts both single queries and batch queries through a unified interface:\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `query` | `array` of strings | Array of query strings for batch processing |\n\n**Parameter Schema:**\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"query\": {\n      \"type\": \"array\",\n      \"items\": {\"type\": \"string\"},\n      \"description\": \"Array of query strings. Include multiple complementary search queries in a single call.\"\n    }\n  },\n  \"required\": [\"query\"]\n}\n```\n\n### Search Implementation Details\n\n| Component | Implementation | Purpose |\n|-----------|---------------|---------|\n| Tool Registration | `@register_tool(\"search\", allow_overwrite=True)` | Registers with qwen_agent framework |\n| Concurrent Processing | `ThreadPoolExecutor(max_workers=3)` | Parallel execution of multiple queries |\n| API Client | `google_search(query: str)` | Individual Google Serper API calls |\n| Retry Logic | 5-attempt loop with exception handling | Robust error recovery |\n| Response Formatting | Markdown snippet extraction | Structured result presentation |\n\n**Sources:** [WebSailor/src/tool_search.py:13-29](), [WebSailor/src/tool_search.py:89-103]()\n\n### Search Response Processing\n\nThe Google Serper API responses undergo structured processing to extract and format search results:\n\n```mermaid\nflowchart LR\n    subgraph \"API Response\"\n        JSONResponse[\"Serper JSON Response\"]\n        OrganicField[\"results.organic[]\"]\n    end\n    \n    subgraph \"Result Processing\"\n        ExtractFields[\"Extract Fields:\u003cbr/\u003etitle, link, date, source, snippet\"]\n        FormatEntry[\"Format as Markdown Entry\"]\n        NumberResults[\"Add Sequential Numbers\"]\n    end\n    \n    subgraph \"Output Format\"\n        MarkdownResult[\"## Web Results\u003cbr/\u003e1. [title](link)\u003cbr/\u003eDate: date\u003cbr/\u003eSource: source\u003cbr/\u003esnippet\"]\n    end\n    \n    JSONResponse --\u003e OrganicField\n    OrganicField --\u003e ExtractFields\n    ExtractFields --\u003e FormatEntry\n    FormatEntry --\u003e NumberResults\n    NumberResults --\u003e MarkdownResult\n```\n\n**Result Structure per Query:**\n1. **Organic Results Extraction** - Filters `organic` array from API response\n2. **Metadata Integration** - Includes publication dates and source attribution\n3. **Content Formatting** - Creates numbered markdown entries with links\n4. **Error Handling** - Provides fallback messages for failed or empty queries\n\nEach formatted result includes: title, clickable link, publication date, source, and content snippet.\n\n**Sources:** [WebSailor/src/tool_search.py:57-86](), [WebDancer/demos/tools/private/search.py:68-96]()\n\n## Visit Tool Implementation\n\nThe `Visit` class implements webpage content extraction and summarization through the Jina Reader API, providing goal-oriented content analysis for visited URLs.\n\n### Visit Tool Architecture\n\n```mermaid\nflowchart TD\n    subgraph \"Visit Tool Components\"\n        VisitClass[\"Visit Class\u003cbr/\u003eBaseTool\"]\n        CallMethod[\"call(params)\"]\n        ReadPage[\"readpage(url, goal)\"]\n        JinaRead[\"jina_readpage(url)\"]\n    end\n    \n    subgraph \"Content Processing Pipeline\"\n        JinaAPI[\"Jina Reader API\u003cbr/\u003er.jina.ai/{url}\"]\n        ContentExtract[\"Raw Content Extraction\"]\n        ContentTruncate[\"Content Truncation\u003cbr/\u003eWEBCONTENT_MAXLENGTH\"]\n        SummaryCall[\"Summary Model Call\"]\n    end\n    \n    subgraph \"Summary Generation\"\n        ExtractorPrompt[\"EXTRACTOR_PROMPT\u003cbr/\u003ewith goal and content\"]\n        OpenAIClient[\"OpenAI-compatible API\u003cbr/\u003e127.0.0.1:6002\"]\n        JSONParsing[\"JSON Response Parsing\u003cbr/\u003eevidence + summary\"]\n    end\n    \n    subgraph \"Output Structure\"\n        UsefulInfo[\"Useful Information Format\u003cbr/\u003eEvidence + Summary\"]\n    end\n    \n    VisitClass --\u003e CallMethod\n    CallMethod --\u003e ReadPage\n    ReadPage --\u003e JinaRead\n    \n    JinaRead --\u003e JinaAPI\n    JinaAPI --\u003e ContentExtract\n    ContentExtract --\u003e ContentTruncate\n    ContentTruncate --\u003e SummaryCall\n    \n    SummaryCall --\u003e ExtractorPrompt\n    ExtractorPrompt --\u003e OpenAIClient\n    OpenAIClient --\u003e JSONParsing\n    JSONParsing --\u003e UsefulInfo\n```\n\n**Sources:** [WebSailor/src/tool_visit.py:20-221]()\n\n### Visit Tool Parameters\n\nThe `Visit` tool requires both URL and goal parameters for context-aware content extraction:\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `url` | `string` or `array` | URL(s) to visit - supports single URL or batch processing |\n| `goal` | `string` | The goal/purpose of visiting the webpage(s) |\n\n**Parameter Schema:**\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"url\": {\n      \"type\": [\"string\", \"array\"],\n      \"items\": {\"type\": \"string\"},\n      \"description\": \"The URL(s) of the webpage(s) to visit\"\n    },\n    \"goal\": {\n      \"type\": \"string\", \n      \"description\": \"The goal of the visit for webpage(s)\"\n    }\n  },\n  \"required\": [\"url\", \"goal\"]\n}\n```\n\n**Sources:** [WebSailor/src/tool_visit.py:26-43]()\n\n### Visit Content Processing Pipeline\n\nThe Visit tool implements a sophisticated content processing pipeline with multiple fallback mechanisms:\n\n```mermaid\nflowchart TD\n    subgraph \"Content Retrieval\"\n        JinaAPI[\"Jina Reader API\u003cbr/\u003er.jina.ai/{url}\"]\n        APIRetry[\"3 retry attempts\u003cbr/\u003e10 second timeout\"]\n        ContentValidation[\"Content Validation\u003cbr/\u003enon-empty, no error markers\"]\n    end\n    \n    subgraph \"Content Processing\"\n        TruncateContent[\"Truncate to WEBCONTENT_MAXLENGTH\u003cbr/\u003edefault: 150,000 chars\"]\n        ExtractorCall[\"call_server() with\u003cbr/\u003eEXTRACTOR_PROMPT\"]\n        SummaryRetry[\"Summary Retry Logic\u003cbr/\u003eprogressive truncation\"]\n    end\n    \n    subgraph \"JSON Processing\"\n        JSONParse[\"JSON Response Parsing\u003cbr/\u003eextract evidence + summary\"]\n        ParseRetry[\"3 parse retry attempts\"]\n        FallbackResponse[\"Fallback error response\"]\n    end\n    \n    subgraph \"Output Generation\"\n        FormatResponse[\"Format as:\u003cbr/\u003eEvidence in page + Summary\"]\n    end\n    \n    JinaAPI --\u003e APIRetry\n    APIRetry --\u003e ContentValidation\n    ContentValidation --\u003e TruncateContent\n    TruncateContent --\u003e ExtractorCall\n    ExtractorCall --\u003e SummaryRetry\n    SummaryRetry --\u003e JSONParse\n    JSONParse --\u003e ParseRetry\n    ParseRetry --\u003e FormatResponse\n    \n    ContentValidation -.-\u003e|\"validation fails\"| FallbackResponse\n    ParseRetry -.-\u003e|\"parse fails\"| FallbackResponse\n```\n\n**Error Handling Mechanisms:**\n1. **API Retry Logic** - 3 attempts with 10-second timeout for Jina API calls\n2. **Content Validation** - Checks for empty content and error markers\n3. **Progressive Truncation** - Reduces content length on summary failures\n4. **JSON Parse Recovery** - 3 retry attempts for malformed JSON responses\n5. **Graceful Fallback** - Structured error responses when all methods fail\n\n**Sources:** [WebSailor/src/tool_visit.py:102-221]()\n\n### Visit Tool Configuration\n\n| Configuration | Environment Variable | Default Value | Purpose |\n|---------------|---------------------|---------------|---------|\n| Content Length Limit | `WEBCONTENT_MAXLENGTH` | 150,000 | Maximum characters for content processing |\n| Jina API Key | `JINA_API_KEY` | Required | Authentication for Jina Reader service |\n| Summary Model Endpoint | Hardcoded | `127.0.0.1:6002/v1` | Local OpenAI-compatible API server |\n| Jina Bypass | `IGNORE_JINA` | false | Development flag to skip Jina service |\n\n**Sources:** [WebSailor/src/tool_visit.py:12-18](), [WebSailor/src/tool_visit.py:72-77]()\n\n## Agent Components\n\nWebSailor's agent system is built around the `MultiTurnReactAgent` class, which extends the qwen_agent `FnCallAgent` to support multi-turn conversations with tool integration.\n\n### MultiTurnReactAgent Architecture\n\n```mermaid\nflowchart TD\n    subgraph \"MultiTurnReactAgent Class\"\n        Agent[\"MultiTurnReactAgent\u003cbr/\u003eextends FnCallAgent\"]\n        CallServer[\"call_server()\u003cbr/\u003eOpenAI API client\"]\n        CountTokens[\"count_tokens()\u003cbr/\u003eAutoTokenizer/tiktoken\"]\n        RunMethod[\"_run()\u003cbr/\u003emain execution loop\"]\n    end\n    \n    subgraph \"External Dependencies\"\n        SGLangServer[\"SGLang Server\u003cbr/\u003e127.0.0.1:6001\"]\n        ToolRegistry[\"Tool Registry\u003cbr/\u003esearch, visit tools\"]\n        TokenLimit[\"Token Management\u003cbr/\u003eMAX_TOKEN_LENGTH=31KB\"]\n    end\n    \n    subgraph \"Execution Flow\"\n        MessageLoop[\"Message Loop\u003cbr/\u003eMAX_LLM_CALL_PER_RUN=40\"]\n        ToolCalling[\"Tool Calling\u003cbr/\u003e\u003ctool_call\u003e parsing\"]\n        AnswerExtraction[\"Answer Extraction\u003cbr/\u003e\u003canswer\u003e parsing\"]\n    end\n    \n    Agent --\u003e|\"initializes with\"| ToolRegistry\n    CallServer --\u003e|\"HTTP requests\"| SGLangServer\n    CountTokens --\u003e|\"monitors\"| TokenLimit\n    \n    RunMethod --\u003e MessageLoop\n    MessageLoop --\u003e ToolCalling\n    ToolCalling --\u003e AnswerExtraction\n    \n    MessageLoop -.-\u003e|\"exceeds limit\"| TokenLimit\n    TokenLimit -.-\u003e|\"truncation\"| AnswerExtraction\n```\n\nThe agent maintains conversation state across multiple turns while enforcing token limits and call constraints to prevent infinite loops.\n\n**Sources:** [WebSailor/src/react_agent.py:20-38](), [WebSailor/src/react_agent.py:80-162]()\n\n### Agent Configuration and Limits\n\nThe agent operates under several configurable constraints:\n\n| Parameter | Default Value | Purpose |\n|-----------|---------------|---------|\n| `MAX_LLM_CALL_PER_RUN` | 40 | Prevents infinite reasoning loops |\n| `MAX_TOKEN_LENGTH` | 31,744 | Context window management |\n| `temperature` | 0.6 | LLM generation randomness |\n| `top_p` | 0.95 | Nucleus sampling parameter |\n\n**Sources:** [WebSailor/src/react_agent.py:15-16](), [WebSailor/src/run_multi_react.py:124-133]()\n\n### Tool Call Processing\n\nThe agent processes tool calls using a structured format:\n\n1. **Tool Call Detection** - Searches for `\u003ctool_call\u003e` and `\u003c/tool_call\u003e` markers\n2. **JSON Parsing** - Extracts tool name and arguments from JSON content\n3. **Tool Execution** - Calls registered tools via `_call_tool()` method\n4. **Response Integration** - Wraps results in `\u003ctool_response\u003e` markers\n\nError handling includes graceful degradation for malformed tool calls and timeout scenarios.\n\n**Sources:** [WebSailor/src/react_agent.py:103-113]()\n\n## Evaluation Pipeline\n\nWebSailor implements a comprehensive evaluation pipeline that supports parallel processing across multiple datasets and model configurations.\n\n### Evaluation Infrastructure\n\n```mermaid\nflowchart TD\n    subgraph \"Server Infrastructure\"\n        OriginalServer[\"Original Model Server\u003cbr/\u003eSGLang Port 6001\u003cbr/\u003eCUDA 0,1,2,3\"]\n        SummaryServer[\"Summary Model Server\u003cbr/\u003eSGLang Port 6002\u003cbr/\u003eCUDA 4,5,6,7\"]\n    end\n    \n    subgraph \"Evaluation Orchestration\"\n        RunScript[\"run.sh\u003cbr/\u003eMain orchestration script\"]\n        MultiReact[\"run_multi_react.py\u003cbr/\u003eParallel execution engine\"]\n        ThreadPool[\"ThreadPoolExecutor\u003cbr/\u003emax_workers=20\"]\n    end\n    \n    subgraph \"Data Processing\"\n        DataLoader[\"Dataset Loader\u003cbr/\u003eeval_data/*.jsonl\"]\n        RolloutManager[\"Rollout Manager\u003cbr/\u003e3 iterations per task\"]\n        ResultAggregator[\"Result Aggregator\u003cbr/\u003eJSON output\"]\n    end\n    \n    RunScript --\u003e|\"starts\"| OriginalServer\n    RunScript --\u003e|\"starts\"| SummaryServer\n    RunScript --\u003e|\"waits for\"| ServerHealthCheck[\"Server Health Check\u003cbr/\u003ecurl endpoints\"]\n    \n    ServerHealthCheck --\u003e|\"ready\"| MultiReact\n    MultiReact --\u003e|\"loads\"| DataLoader\n    MultiReact --\u003e|\"creates\"| ThreadPool\n    \n    ThreadPool --\u003e|\"parallel requests\"| OriginalServer\n    DataLoader --\u003e|\"tasks\"| RolloutManager\n    RolloutManager --\u003e|\"results\"| ResultAggregator\n```\n\nThe evaluation system supports multiple datasets including GAIA, BrowseComp (English/Chinese), XBench-DeepSearch, and SimpleQA.\n\n**Sources:** [WebSailor/src/run.sh:32-96](), [WebSailor/src/run_multi_react.py:14-188]()\n\n### Supported Datasets\n\nThe evaluation pipeline supports the following datasets:\n\n| Dataset | Description | File Location |\n|---------|-------------|---------------|\n| `gaia` | General AI Assistant benchmark | `eval_data/gaia.jsonl` |\n| `browsecomp_zh` | Chinese browsing comprehension (289 cases) | `eval_data/browsecomp_zh.jsonl` |\n| `browsecomp_en` | English browsing comprehension (1266 cases) | `eval_data/browsecomp_en.jsonl` |\n| `xbench-deepsearch` | Deep search evaluation tasks | `eval_data/xbench-deepsearch.jsonl` |\n| `simple_qa` | Basic question answering | `eval_data/simple_qa.jsonl` |\n\n**Sources:** [WebSailor/src/run_multi_react.py:18-25]()\n\n### Parallel Execution Framework\n\nThe evaluation system implements sophisticated parallel processing:\n\n1. **Server Management** - Dual SGLang servers with health monitoring\n2. **Task Distribution** - ThreadPoolExecutor with configurable worker counts\n3. **Progress Tracking** - tqdm progress bars for monitoring\n4. **Error Recovery** - Graceful handling of individual task failures\n5. **Result Persistence** - Atomic writes with file locking\n\nEach task is executed independently with proper error isolation to prevent cascading failures.\n\n**Sources:** [WebSailor/src/run_multi_react.py:146-186]()\n\n### Output Structure and Rollouts\n\nThe evaluation generates structured outputs with multiple rollouts per dataset:\n\n```mermaid\nflowchart LR\n    subgraph \"Output Directory Structure\"\n        OutputBase[\"output_base/\"]\n        ModelDir[\"model_name_sglang/\"]\n        DatasetDir[\"dataset_name/\"]\n        \n        OutputBase --\u003e ModelDir\n        ModelDir --\u003e DatasetDir\n    end\n    \n    subgraph \"Rollout Files\"\n        Iter1[\"iter1.jsonl\"]\n        Iter2[\"iter2.jsonl\"] \n        Iter3[\"iter3.jsonl\"]\n        \n        DatasetDir --\u003e Iter1\n        DatasetDir --\u003e Iter2\n        DatasetDir --\u003e Iter3\n    end\n    \n    subgraph \"Result Format\"\n        Question[\"question\"]\n        Answer[\"answer\"]\n        Messages[\"messages[]\"]\n        Prediction[\"prediction\"]\n        Termination[\"termination\"]\n    end\n    \n    Iter1 --\u003e Question\n    Iter1 --\u003e Answer\n    Iter1 --\u003e Messages\n    Iter1 --\u003e Prediction\n    Iter1 --\u003e Termination\n```\n\nEach rollout contains complete conversation traces, enabling detailed analysis of agent behavior and performance patterns.\n\n**Sources:** [WebSailor/src/run_multi_react.py:75-122](), [WebSailor/src/react_agent.py:154-162]()"])</script><script>self.__next_f.push([1,"23:T2e87,"])</script><script>self.__next_f.push([1,"# WebWatcher and WebWalker\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [WebWatcher/README.md](WebWatcher/README.md)\n- [assets/aliyun.png](assets/aliyun.png)\n- [assets/road_map_webwatcher.png](assets/road_map_webwatcher.png)\n\n\u003c/details\u003e\n\n\n\nWebWatcher and WebWalker represent two specialized components of the WebAgent ecosystem that address distinct but complementary aspects of web information seeking. WebWatcher functions as a vision-language deep research agent capable of processing multimodal content and tackling challenging visual question answering (VQA) problems. WebWalker serves as a comprehensive benchmarking framework for evaluating Large Language Models (LLMs) in web traversal and multi-agent information seeking scenarios.\n\nTogether, these components provide both advanced research capabilities and standardized evaluation methodologies for the WebAgent ecosystem. For detailed information about WebWatcher's vision-language capabilities, see [WebWatcher Research Agent](#5.1). For comprehensive coverage of the benchmarking framework, see [WebWalker Benchmark Framework](#5.2).\n\n## Component Overview\n\nWebWatcher and WebWalker fulfill distinct roles within the WebAgent ecosystem, addressing different aspects of web information seeking and agent evaluation.\n\n### WebWatcher: Vision-Language Deep Research Agent\n\nWebWatcher breaks new frontiers in vision-language research by combining multimodal understanding with tool-based problem solving. The system leverages multiple tools effectively to tackle highly challenging VQA problems that require both visual comprehension and web-based information retrieval.\n\n### WebWalker: Benchmarking Framework\n\nWebWalker serves as the evaluation backbone for the WebAgent ecosystem, providing a comprehensive benchmark framework that measures LLM performance across web traversal tasks. Unlike traditional static benchmarks, WebWalker evaluates dynamic capabilities including multi-step reasoning, web navigation, and information synthesis across multiple domains.\n\nThe WebWalker framework was accepted at ACL 2025 and represents a significant advancement in benchmarking agentic systems, addressing the gap between traditional language model evaluation and real-world web interaction capabilities.\n\n### System Architecture\n\n```mermaid\ngraph TB\n    subgraph \"WebWatcher Component\"\n        WW_AGENT[\"WebWatcher Agent\"]\n        VL_MODEL[\"Vision-Language Model\"]\n        TOOL_MGR[\"Tool Manager\"]\n        VQA_ENGINE[\"VQA Engine\"]\n    end\n    \n    subgraph \"WebWalker Framework\"\n        WW_BENCH[\"WebWalker Benchmark\"]\n        WWQA[\"WebWalkerQA Dataset\"]\n        EVAL_METRICS[\"Evaluation Metrics\"]\n        TASK_REPO[\"Task Repository\"]\n    end\n    \n    subgraph \"Evaluated Systems\"\n        WS[\"WebShaper\u003cbr/\u003eScore: 52.50\"]\n        WD[\"WebDancer\u003cbr/\u003eScore: 62.0%\"]\n        WW_EVAL[\"WebWatcher\u003cbr/\u003e(VQA Performance)\"]\n    end\n    \n    subgraph \"Task Categories\"\n        VQA_TASKS[\"VQA Tasks\"]\n        NAV_TASKS[\"Navigation Tasks\"]\n        INFO_TASKS[\"Information Extraction\"]\n        MULTI_TASKS[\"Multi-step Reasoning\"]\n    end\n    \n    WW_AGENT --\u003e VL_MODEL\n    WW_AGENT --\u003e TOOL_MGR\n    VL_MODEL --\u003e VQA_ENGINE\n    \n    WW_BENCH --\u003e WWQA\n    WW_BENCH --\u003e EVAL_METRICS\n    WW_BENCH --\u003e TASK_REPO\n    \n    WS --\u003e WW_BENCH\n    WD --\u003e WW_BENCH\n    WW_EVAL --\u003e WW_BENCH\n    \n    TASK_REPO --\u003e VQA_TASKS\n    TASK_REPO --\u003e NAV_TASKS\n    TASK_REPO --\u003e INFO_TASKS\n    TASK_REPO --\u003e MULTI_TASKS\n    \n    WW_AGENT --\u003e VQA_TASKS\n```\n\nSources: [README.md:35](), [README.md:40](), [README.md:44](), [README.md:51]()\n\n## WebWatcher Research Agent\n\nWebWatcher represents a breakthrough in vision-language deep research agents, designed to tackle complex multimodal problems that require both visual understanding and web-based tool utilization. The system demonstrates advanced capabilities in processing visual content and leveraging external tools to solve challenging VQA problems.\n\n### Vision-Language Integration\n\nWebWatcher integrates vision and language processing capabilities to handle multimodal research tasks. The system can process complex visual inputs while simultaneously utilizing web-based tools to gather additional context and information needed for comprehensive problem solving.\n\n### Tool-Based Problem Solving\n\n```mermaid\ngraph LR\n    subgraph \"Input Processing\"\n        IMG_INPUT[\"Image Input\"]\n        TEXT_INPUT[\"Text Query\"]\n        CONTEXT[\"Task Context\"]\n    end\n    \n    subgraph \"WebWatcher Core\"\n        VL_PROC[\"Vision-Language Processor\"]\n        TOOL_SEL[\"Tool Selector\"]\n        REASONING[\"Reasoning Engine\"]\n        RESPONSE_GEN[\"Response Generator\"]\n    end\n    \n    subgraph \"Tool Interface\"\n        WEB_SEARCH[\"Web Search Tools\"]\n        VQA_TOOLS[\"VQA Tools\"]\n        INFO_EXTRACT[\"Information Extractors\"]\n    end\n    \n    subgraph \"Output Generation\"\n        ANALYSIS[\"Visual Analysis\"]\n        SYNTHESIS[\"Information Synthesis\"]\n        FINAL_ANSWER[\"Final Response\"]\n    end\n    \n    IMG_INPUT --\u003e VL_PROC\n    TEXT_INPUT --\u003e VL_PROC\n    CONTEXT --\u003e TOOL_SEL\n    \n    VL_PROC --\u003e REASONING\n    TOOL_SEL --\u003e REASONING\n    REASONING --\u003e RESPONSE_GEN\n    \n    TOOL_SEL --\u003e WEB_SEARCH\n    TOOL_SEL --\u003e VQA_TOOLS\n    TOOL_SEL --\u003e INFO_EXTRACT\n    \n    RESPONSE_GEN --\u003e ANALYSIS\n    RESPONSE_GEN --\u003e SYNTHESIS\n    SYNTHESIS --\u003e FINAL_ANSWER\n```\n\nSources: [README.md:35](), [README.md:126-133](), [README.md:183-188]()\n\n## WebWalker Benchmark Framework\n\nWebWalker operates as a comprehensive evaluation framework that interfaces with multiple WebAgent components. The framework provides standardized evaluation protocols that ensure consistent assessment across different model architectures and training methodologies.\n\n### Benchmark Architecture\n\n```mermaid\ngraph LR\n    subgraph \"Input Layer\"\n        QUERY[\"Web Traversal Query\"]\n        CONTEXT[\"Task Context\"]\n        CONSTRAINTS[\"Evaluation Constraints\"]\n    end\n    \n    subgraph \"WebWalker Core\"\n        PARSER[\"Query Parser\"]\n        SCHEDULER[\"Task Scheduler\"]\n        MONITOR[\"Progress Monitor\"]\n        SCORER[\"Scoring Engine\"]\n    end\n    \n    subgraph \"Agent Interface\"\n        API[\"Agent API\"]\n        TOOLS[\"Tool Registry\"]\n        ENV[\"Web Environment\"]\n    end\n    \n    subgraph \"Output Layer\"\n        METRICS[\"Performance Metrics\"]\n        TRACES[\"Execution Traces\"]\n        REPORTS[\"Evaluation Reports\"]\n    end\n    \n    QUERY --\u003e PARSER\n    CONTEXT --\u003e PARSER\n    CONSTRAINTS --\u003e SCHEDULER\n    \n    PARSER --\u003e SCHEDULER\n    SCHEDULER --\u003e MONITOR\n    MONITOR --\u003e SCORER\n    \n    SCHEDULER --\u003e API\n    API --\u003e TOOLS\n    TOOLS --\u003e ENV\n    \n    SCORER --\u003e METRICS\n    MONITOR --\u003e TRACES\n    METRICS --\u003e REPORTS\n```\n\nSources: [README.md:40](), [README.md:51](), [README.md:211-216]()\n\n## Component Integration and Performance\n\nBoth WebWatcher and WebWalker integrate with the broader WebAgent ecosystem, serving complementary roles in research and evaluation. WebWalker serves as the primary evaluation platform for all major WebAgent components, while WebWatcher provides advanced vision-language research capabilities.\n\n### WebAgent Component Performance\n\n| Component | WebWalkerQA Score | Key Strengths | Evaluation Focus |\n|-----------|-------------------|---------------|------------------|\n| WebShaper | 52.50 | Data synthesis, formalization | Systematic question generation |\n| WebDancer | 62.0% | Native agentic training | End-to-end web traversal |\n| WebSailor | Not specified | Complex reasoning | Multi-turn navigation |\n| WebWatcher | VQA Performance | Vision-language integration | Multimodal problem solving |\n\n### Evaluation and Research Integration\n\n```mermaid\nsequenceDiagram\n    participant WW_BENCH as \"WebWalker Framework\"\n    participant WW_AGENT as \"WebWatcher Agent\"\n    participant WS as \"WebShaper Agent\"\n    participant WD as \"WebDancer Agent\" \n    participant WWQA as \"WebWalkerQA Dataset\"\n    participant EVAL as \"Evaluation Engine\"\n    \n    WW_BENCH-\u003e\u003eWWQA: \"Load benchmark tasks\"\n    WWQA--\u003e\u003eWW_BENCH: \"Task specifications\"\n    \n    WW_BENCH-\u003e\u003eWW_AGENT: \"Execute VQA task\"\n    WW_AGENT-\u003e\u003eWW_AGENT: \"Apply vision-language tools\"\n    WW_AGENT--\u003e\u003eWW_BENCH: \"VQA results\"\n    \n    WW_BENCH-\u003e\u003eWS: \"Execute traversal task\"\n    WS-\u003e\u003eWS: \"Apply formalization-driven approach\"\n    WS--\u003e\u003eWW_BENCH: \"Results: 52.50 score\"\n    \n    WW_BENCH-\u003e\u003eWD: \"Execute traversal task\"\n    WD-\u003e\u003eWD: \"Native agentic reasoning\"\n    WD--\u003e\u003eWW_BENCH: \"Results: 62.0% score\"\n    \n    WW_BENCH-\u003e\u003eEVAL: \"Aggregate performance data\"\n    EVAL--\u003e\u003eWW_BENCH: \"Comprehensive metrics\"\n```\n\nSources: [README.md:69](), [README.md:84](), [README.md:126-133]()\n\n## Evaluation and Research Methodologies\n\nBoth WebWatcher and WebWalker employ sophisticated evaluation methodologies to assess performance and advance research in their respective domains.\n\n### WebWatcher Research Evaluation\n\nWebWatcher evaluation focuses on multimodal problem-solving capabilities, assessing the system's ability to integrate visual understanding with tool-based information gathering. The evaluation covers challenging VQA scenarios that require both visual comprehension and external knowledge acquisition.\n\n### WebWalker Benchmark Evaluation\n\nThe WebWalker evaluation framework employs a multi-dimensional assessment approach that captures both task completion accuracy and process quality. The framework evaluates agents across various complexity levels and domain-specific challenges.\n\n#### Evaluation Dimensions\n\nThe benchmark assesses performance across several key dimensions:\n\n- **Navigation Accuracy**: Ability to traverse web structures effectively\n- **Information Extraction**: Quality and completeness of retrieved information  \n- **Multi-step Reasoning**: Coherence across extended task sequences\n- **Robustness**: Performance consistency across diverse web environments\n- **VQA Performance**: Visual question answering capabilities (for WebWatcher)\n\n### Comprehensive Evaluation Pipeline\n\n```mermaid\ngraph TB\n    subgraph \"Evaluation Pipeline\"\n        INIT[\"Task Initialization\"]\n        EXEC[\"Agent Execution\"]\n        MON[\"Progress Monitoring\"]\n        SCORE[\"Scoring Phase\"]\n        REP[\"Report Generation\"]\n    end\n    \n    subgraph \"Scoring Components\"\n        ACC[\"Accuracy Metrics\"]\n        EFF[\"Efficiency Metrics\"]\n        ROB[\"Robustness Metrics\"]\n        VQA_SCORE[\"VQA Quality Metrics\"]\n    end\n    \n    subgraph \"Output Analysis\"\n        TRACE[\"Execution Traces\"]\n        STATS[\"Statistical Analysis\"]\n        COMP[\"Comparative Results\"]\n        VL_ANALYSIS[\"Vision-Language Analysis\"]\n    end\n    \n    INIT --\u003e EXEC\n    EXEC --\u003e MON\n    MON --\u003e SCORE\n    SCORE --\u003e REP\n    \n    SCORE --\u003e ACC\n    SCORE --\u003e EFF\n    SCORE --\u003e ROB\n    SCORE --\u003e VQA_SCORE\n    \n    REP --\u003e TRACE\n    REP --\u003e STATS\n    REP --\u003e COMP\n    REP --\u003e VL_ANALYSIS\n```\n\nSources: [README.md:126-133](), [README.md:40](), [README.md:211-216]()\n\n## Performance Results and Impact\n\nWebWalker has established itself as a critical benchmark in the web agent evaluation landscape, with results demonstrating clear performance differentials between different architectural approaches. The benchmark has been instrumental in advancing the state-of-the-art in web traversal capabilities.\n\n### Notable Achievements\n\nThe WebWalker benchmark has facilitated several breakthrough results:\n\n- **WebShaper**: Achieved 52.50 on WebWalkerQA through formalization-driven synthesis\n- **WebDancer**: Demonstrated 62.0% performance with native agentic training\n- **State-of-the-art advancement**: Multiple systems have achieved new performance records\n\nThese results highlight the effectiveness of different training paradigms and architectural choices in web traversal scenarios, providing valuable insights for future research directions in agentic systems.\n\nSources: [README.md:69](), [README.md:84](), [README.md:48]()"])</script><script>self.__next_f.push([1,"24:T1bfa,"])</script><script>self.__next_f.push([1,"# WebWatcher Research Agent\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebWatcher/README.md](WebWatcher/README.md)\n- [assets/road_map_webwatcher.png](assets/road_map_webwatcher.png)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document covers WebWatcher, the vision-language deep research agent component of the WebAgent system. WebWatcher is designed to provide advanced research capabilities that leverage both visual and textual information processing for comprehensive information seeking tasks.\n\nFor information about other WebAgent components, see [WebShaper](#2) for data synthesis, [WebSailor](#4) for web reasoning, [WebDancer](#3) for autonomous search, and [WebWalker](#5.2) for benchmarking frameworks.\n\n## System Overview\n\nWebWatcher represents the vision-language research component within the broader WebAgent ecosystem. As one of the five core components, it is specifically designed to handle research tasks that require processing and understanding of both visual and textual content from web sources.\n\n### WebWatcher in the WebAgent Architecture\n\n```mermaid\ngraph TB\n    subgraph \"WebAgent Ecosystem\"\n        WA[\"WebAgent System\"]\n        \n        WA --\u003e WW[\"WebWatcher\u003cbr/\u003eVision-Language Research\"]\n        WA --\u003e WS[\"WebShaper\u003cbr/\u003eData Synthesis\"]\n        WA --\u003e WSA[\"WebSailor\u003cbr/\u003eWeb Reasoning\"]\n        WA --\u003e WD[\"WebDancer\u003cbr/\u003eAutonomous Search\"]\n        WA --\u003e WWA[\"WebWalker\u003cbr/\u003eBenchmarking\"]\n    end\n    \n    subgraph \"WebWatcher Focus Area\"\n        WW --\u003e VL[\"Vision-Language\u003cbr/\u003eProcessing\"]\n        WW --\u003e DR[\"Deep Research\u003cbr/\u003eCapabilities\"]\n        WW --\u003e MM[\"Multimodal\u003cbr/\u003eInformation Synthesis\"]\n    end\n    \n    subgraph \"Integration Points\"\n        VL --\u003e TOOLS[\"Shared Tool Layer\"]\n        DR --\u003e APIS[\"External APIs\"]\n        MM --\u003e EVAL[\"Evaluation Framework\"]\n    end\n```\n\n**WebWatcher Architecture Overview**\nThis diagram shows WebWatcher's position within the WebAgent system and its specialized focus on vision-language research capabilities.\n\nSources: WebWatcher/README.md:1-2\n\n## Component Structure\n\nBased on the WebAgent system architecture, WebWatcher is organized to handle the intersection of visual and textual information processing for research-oriented tasks.\n\n### Core Capabilities Framework\n\n```mermaid\ngraph LR\n    subgraph \"Input Processing\"\n        VI[\"Visual Input\u003cbr/\u003eImages, Screenshots\"]\n        TI[\"Textual Input\u003cbr/\u003eWeb Content, Documents\"]\n        MI[\"Multimodal Input\u003cbr/\u003eCombined Sources\"]\n    end\n    \n    subgraph \"WebWatcher Core\"\n        VLP[\"Vision-Language\u003cbr/\u003eProcessor\"]\n        RA[\"Research Agent\u003cbr/\u003eLogic\"]\n        IS[\"Information\u003cbr/\u003eSynthesizer\"]\n    end\n    \n    subgraph \"Output Generation\"\n        RR[\"Research Results\"]\n        VS[\"Visual Summaries\"]\n        CR[\"Comprehensive Reports\"]\n    end\n    \n    VI --\u003e VLP\n    TI --\u003e VLP\n    MI --\u003e VLP\n    \n    VLP --\u003e RA\n    RA --\u003e IS\n    \n    RA --\u003e RR\n    IS --\u003e VS\n    IS --\u003e CR\n```\n\n**WebWatcher Core Processing Pipeline**\nThis diagram illustrates the expected flow of multimodal information through WebWatcher's processing components.\n\nSources: WebWatcher/README.md:1-2\n\n## Implementation Status\n\nThe current WebWatcher implementation appears to be in early development stages. The primary source file [`WebWatcher/README.md`](WebWatcher/README.md:1-2) contains minimal documentation, indicating that the component is likely planned or under active development.\n\n### Directory Structure\n\n```mermaid\ngraph TD\n    subgraph \"WebWatcher Module\"\n        WW_ROOT[\"WebWatcher/\"]\n        WW_ROOT --\u003e README[\"README.md\u003cbr/\u003e(Current: minimal content)\"]\n        WW_ROOT --\u003e IMPL[\"Implementation Files\u003cbr/\u003e(To be developed)\"]\n        WW_ROOT --\u003e CONFIG[\"Configuration\u003cbr/\u003e(To be developed)\"]\n    end\n    \n    subgraph \"Expected Integration\"\n        TOOLS_DIR[\"tools/\u003cbr/\u003e(Shared with other agents)\"]\n        EVAL_DIR[\"evaluation/\u003cbr/\u003e(Benchmark integration)\"]\n        MODELS_DIR[\"models/\u003cbr/\u003e(Vision-language models)\"]\n    end\n    \n    WW_ROOT -.-\u003e TOOLS_DIR\n    WW_ROOT -.-\u003e EVAL_DIR\n    WW_ROOT -.-\u003e MODELS_DIR\n```\n\n**WebWatcher Directory Structure and Integration Points**\nThis diagram shows the current minimal structure and expected integration with shared WebAgent components.\n\nSources: WebWatcher/README.md:1-2\n\n## Research Capabilities\n\nWebWatcher is designed to provide specialized research functionality that distinguishes it from the other WebAgent components:\n\n| Capability | Description | Integration |\n|------------|-------------|-------------|\n| **Vision-Language Processing** | Process and understand visual content alongside textual information | Shared tool layer |\n| **Deep Research** | Conduct comprehensive research across multimodal sources | External API integration |\n| **Multimodal Synthesis** | Combine insights from visual and textual sources | Evaluation framework |\n\n## Development Roadmap\n\nBased on the WebAgent system design, WebWatcher's development likely follows these stages:\n\n1. **Foundation Setup**: Basic vision-language model integration\n2. **Tool Integration**: Connection with shared WebAgent tools and APIs\n3. **Research Logic**: Implementation of specialized research algorithms\n4. **Evaluation Integration**: Benchmarking against multimodal research tasks\n5. **System Integration**: Full integration with other WebAgent components\n\n## Technical Integration\n\nWebWatcher is expected to integrate with the broader WebAgent infrastructure through standardized interfaces:\n\n```mermaid\ngraph TB\n    subgraph \"WebWatcher Integration\"\n        WW_AGENT[\"WebWatcher Agent\"]\n        WW_TOOLS[\"Vision-Language Tools\"]\n        WW_MODELS[\"Multimodal Models\"]\n    end\n    \n    subgraph \"Shared Infrastructure\"\n        TOOL_LAYER[\"Tool Layer\u003cbr/\u003e(search, visit)\"]\n        API_LAYER[\"API Layer\u003cbr/\u003e(external services)\"]\n        EVAL_LAYER[\"Evaluation Layer\u003cbr/\u003e(benchmarks)\"]\n    end\n    \n    subgraph \"External Services\"\n        VLM_APIS[\"Vision-Language APIs\"]\n        RESEARCH_APIS[\"Research APIs\"]\n        STORAGE_APIS[\"Storage APIs\"]\n    end\n    \n    WW_AGENT --\u003e WW_TOOLS\n    WW_TOOLS --\u003e TOOL_LAYER\n    WW_MODELS --\u003e API_LAYER\n    \n    TOOL_LAYER --\u003e VLM_APIS\n    API_LAYER --\u003e RESEARCH_APIS\n    EVAL_LAYER --\u003e STORAGE_APIS\n```\n\n**WebWatcher System Integration Architecture**\nThis diagram shows how WebWatcher is expected to integrate with shared WebAgent infrastructure and external services.\n\nSources: WebWatcher/README.md:1-2\n\n## Future Development\n\nThe WebWatcher component represents an important specialization within the WebAgent ecosystem, focusing on the intersection of vision and language for research tasks. As development progresses, it will likely include:\n\n- Specialized vision-language models for research tasks\n- Integration with academic and research databases\n- Advanced multimodal reasoning capabilities\n- Benchmarking frameworks for vision-language research tasks\n\nThe component's development status can be tracked through updates to the [`WebWatcher/README.md`](WebWatcher/README.md:1-2) file and the addition of implementation files in the WebWatcher directory.\n\nSources: WebWatcher/README.md:1-2"])</script><script>self.__next_f.push([1,"25:T2e57,"])</script><script>self.__next_f.push([1,"# WebWalker Benchmark Framework\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [assets/aliyun.png](assets/aliyun.png)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document covers the WebWalker benchmark framework, which includes both a multi-agent framework for information seeking and the WebWalkerQA benchmark dataset for measuring LLM capabilities in web traversal tasks. WebWalker provides comprehensive tooling for developing, training, and evaluating web-based AI agents through standardized benchmarking and multi-agent coordination.\n\nFor information about WebWatcher's vision-language capabilities, see [WebWatcher Research Agent](#5.1). For comprehensive benchmark results across all WebAgent components, see [Benchmark Results](#7.1).\n\n## Overview\n\nWebWalker is a comprehensive benchmark framework for evaluating Large Language Models (LLMs) in web traversal scenarios. Published as part of the ACL 2025 paper \"WebWalker: Benchmarking LLMs in Web Traversal\", this framework addresses the critical need for standardized evaluation and multi-agent coordination in web navigation and information-seeking tasks.\n\nThe framework consists of two primary components: a multi-agent framework for collaborative information seeking and the WebWalkerQA benchmark dataset for standardized evaluation. Both components are publicly available and serve as key evaluation infrastructure for the WebAgent ecosystem.\n\n**Key Characteristics:**\n- **Multi-Agent Framework**: Enables collaborative information seeking through agent coordination\n- **Benchmark Dataset**: WebWalkerQA provides standardized web traversal evaluation metrics\n- **Comprehensive Evaluation**: Supports both single-agent and multi-agent assessment approaches\n- **Open Source**: Publicly available framework enabling reproducible research and development\n- **Ecosystem Integration**: Serves as evaluation infrastructure for WebShaper, WebDancer, and WebSailor\n\nSources: [README.md:40](), [README.md:52](), [README.md:211-216]()\n\n## Framework Architecture\n\n### WebWalker System Components\n\n```mermaid\ngraph TB\n    subgraph \"WebWalker Framework\"\n        MAF[\"Multi-Agent Framework\"]\n        WQA[\"WebWalkerQA Dataset\"]\n        BM[\"Benchmark Metrics\"]\n        TC[\"Task Coordination\"]\n    end\n    \n    subgraph \"Multi-Agent Components\"\n        AC[\"Agent Coordination\"]\n        IS[\"Information Seeking\"]\n        WT[\"Web Traversal\"]\n        KS[\"Knowledge Synthesis\"]\n    end\n    \n    subgraph \"Evaluation Pipeline\"\n        TG[\"Task Generation\"]\n        EM[\"Execution Monitoring\"]\n        PS[\"Performance Scoring\"]\n        RA[\"Results Analysis\"]\n    end\n    \n    subgraph \"Integration Layer\"\n        HF[\"HuggingFace Dataset\u003cbr/\u003ecallanwu/WebWalkerQA\"]\n        ACL[\"ACL 2025 Paper\"]\n        Demos[\"Demo Videos\"]\n    end\n    \n    MAF --\u003e AC\n    MAF --\u003e IS\n    MAF --\u003e WT\n    MAF --\u003e KS\n    \n    AC --\u003e TG\n    IS --\u003e EM\n    WT --\u003e PS\n    KS --\u003e RA\n    \n    TG --\u003e WQA\n    EM --\u003e BM\n    PS --\u003e BM\n    RA --\u003e TC\n    \n    WQA --\u003e HF\n    BM --\u003e ACL\n    TC --\u003e Demos\n```\n\n### Framework Components\n\nWebWalker consists of several integrated components that work together to provide comprehensive benchmarking and multi-agent coordination capabilities:\n\n| Component | Purpose | Key Features |\n|-----------|---------|-------------|\n| **Multi-Agent Framework** | Agent Coordination | Collaborative information seeking, task distribution, knowledge synthesis |\n| **WebWalkerQA Dataset** | Benchmark Evaluation | Standardized web traversal tasks, performance metrics, evaluation protocols |\n| **Task Coordination** | Workflow Management | Multi-step navigation planning, agent synchronization, result aggregation |\n| **Benchmark Metrics** | Performance Measurement | Standardized scoring, comparative analysis, reproducible evaluation |\n\nSources: [README.md:52](), [README.md:211-216]()\n\n## Multi-Agent Framework\n\n### Agent Coordination Architecture\n\nThe WebWalker multi-agent framework enables collaborative information seeking through coordinated web traversal and knowledge synthesis. The framework supports multiple agents working together to complete complex information-seeking tasks that require diverse capabilities and perspectives.\n\n```mermaid\ngraph LR\n    subgraph \"Multi-Agent Workflow\"\n        TI[\"Task Input\"]\n        TD[\"Task Decomposition\"]\n        AA[\"Agent Assignment\"]\n        PE[\"Parallel Execution\"]\n        KS[\"Knowledge Synthesis\"]\n        FR[\"Final Result\"]\n    end\n    \n    subgraph \"Agent Types\"\n        NA[\"Navigation Agent\"]\n        EA[\"Extraction Agent\"]\n        RA[\"Reasoning Agent\"]\n        VA[\"Verification Agent\"]\n    end\n    \n    subgraph \"Coordination Mechanisms\"\n        SP[\"Shared Plan\"]\n        MS[\"Message System\"]\n        SC[\"State Coordination\"]\n        QC[\"Quality Control\"]\n    end\n    \n    TI --\u003e TD\n    TD --\u003e AA\n    AA --\u003e PE\n    PE --\u003e KS\n    KS --\u003e FR\n    \n    AA --\u003e NA\n    AA --\u003e EA\n    AA --\u003e RA\n    AA --\u003e VA\n    \n    PE --\u003e SP\n    PE --\u003e MS\n    PE --\u003e SC\n    PE --\u003e QC\n```\n\n### Multi-Agent Capabilities\n\nThe framework provides several key capabilities for collaborative information seeking:\n\n| Capability | Description | Implementation |\n|------------|-------------|---------------|\n| **Task Decomposition** | Breaking complex queries into manageable sub-tasks | Hierarchical planning with agent specialization |\n| **Agent Specialization** | Different agents with specialized capabilities | Navigation, extraction, reasoning, and verification agents |\n| **Knowledge Synthesis** | Combining results from multiple agents | Consensus mechanisms and result validation |\n| **Quality Assurance** | Ensuring accuracy and completeness | Cross-validation and verification protocols |\n\nThe multi-agent approach enables handling of complex information-seeking scenarios that require diverse skills and perspectives, improving both accuracy and robustness compared to single-agent approaches.\n\nSources: [README.md:52]()\n\n## WebWalkerQA Benchmark Dataset\n\n### Dataset Generation and Structure\n\nThe WebWalkerQA benchmark dataset is generated through the WebWalker multi-agent framework, providing standardized evaluation tasks for web traversal and information seeking capabilities. The dataset captures the complexity of real-world web navigation scenarios while maintaining reproducible evaluation protocols.\n\n```mermaid\ngraph TB\n    subgraph \"WebWalkerQA Generation\"\n        MAF[\"Multi-Agent Framework\"]\n        TG[\"Task Generation\"]\n        WS[\"Web Scenarios\"]\n        QV[\"Quality Validation\"]\n    end\n    \n    subgraph \"Dataset Structure\"\n        NT[\"Navigation Tasks\"]\n        ET[\"Extraction Tasks\"]\n        RT[\"Reasoning Tasks\"]\n        CT[\"Complex Tasks\"]\n    end\n    \n    subgraph \"Evaluation Protocol\"\n        SM[\"Scoring Metrics\"]\n        EP[\"Evaluation Pipeline\"]\n        RP[\"Results Processing\"]\n        CA[\"Comparative Analysis\"]\n    end\n    \n    MAF --\u003e TG\n    TG --\u003e WS\n    WS --\u003e QV\n    QV --\u003e NT\n    QV --\u003e ET\n    QV --\u003e RT\n    QV --\u003e CT\n    \n    NT --\u003e SM\n    ET --\u003e SM\n    RT --\u003e SM\n    CT --\u003e SM\n    \n    SM --\u003e EP\n    EP --\u003e RP\n    RP --\u003e CA\n```\n\n### Performance Results\n\nThe WebWalkerQA benchmark serves as a key evaluation metric for WebAgent ecosystem components, demonstrating the effectiveness of different training approaches:\n\n| System | WebWalkerQA Score | Training Methodology | Key Capabilities |\n|--------|------------------|---------------------|------------------|\n| **WebDancer** | 62.0% Pass@3 | Native agentic training with ReAct framework | Multi-step reasoning, autonomous navigation, trajectory learning |\n| **WebShaper** | 52.50 | Formalization-driven data synthesis | Systematic question generation, structured information seeking |\n| **Baseline Systems** | Variable | Standard fine-tuning approaches | Basic web interaction capabilities |\n\n**WebDancer Performance**: Achieves the highest score of 62.0% Pass@3 on WebWalkerQA, demonstrating superior multi-step reasoning and autonomous navigation capabilities through its native agentic training approach.\n\n**WebShaper Performance**: Scores 52.50 on WebWalkerQA, showing strong performance through its formalization-driven data synthesis methodology and systematic approach to information-seeking task decomposition.\n\nSources: [README.md:87](), [README.md:72]()\n\n## WebAgent Ecosystem Integration\n\n### Framework Integration Points\n\nWebWalker serves as a central benchmarking infrastructure for the WebAgent ecosystem, providing evaluation capabilities and multi-agent coordination for all major components:\n\n```mermaid\nflowchart TD\n    subgraph \"WebAgent Components\"\n        WS[\"WebShaper\u003cbr/\u003eData Synthesis\"]\n        WSA[\"WebSailor\u003cbr/\u003ePost-Training\"]\n        WD[\"WebDancer\u003cbr/\u003eAgentic Training\"]\n        WW[\"WebWatcher\u003cbr/\u003eVision-Language\"]\n    end\n    \n    subgraph \"WebWalker Framework\"\n        MAF[\"Multi-Agent Framework\"]\n        WQA[\"WebWalkerQA Benchmark\"]\n        EM[\"Evaluation Metrics\"]\n        TC[\"Task Coordination\"]\n    end\n    \n    subgraph \"Evaluation Outputs\"\n        PR[\"Performance Results\"]\n        CA[\"Comparative Analysis\"]\n        RI[\"Research Insights\"]\n        TD[\"Training Direction\"]\n    end\n    \n    subgraph \"External Integration\"\n        HF[\"HuggingFace\u003cbr/\u003ecallanwu/WebWalkerQA\"]\n        ACL[\"ACL 2025 Publication\"]\n        Demos[\"Interactive Demos\"]\n    end\n    \n    WS --\u003e MAF\n    WSA --\u003e MAF\n    WD --\u003e MAF\n    WW --\u003e MAF\n    \n    MAF --\u003e WQA\n    WQA --\u003e EM\n    EM --\u003e TC\n    TC --\u003e PR\n    \n    PR --\u003e CA\n    CA --\u003e RI\n    RI --\u003e TD\n    \n    WQA --\u003e HF\n    EM --\u003e ACL\n    TC --\u003e Demos\n```\n\n### Cross-Component Benchmarking\n\nWebWalker enables standardized evaluation across different training paradigms and architectural approaches within the WebAgent ecosystem:\n\n| Component | Evaluation Focus | WebWalker Integration |\n|-----------|------------------|----------------------|\n| **WebShaper** | Data synthesis effectiveness | Formalization-driven task generation validation |\n| **WebSailor** | Post-training methodology | Complex reasoning and navigation assessment |\n| **WebDancer** | Native agentic capabilities | Autonomous web traversal and decision making |\n| **WebWatcher** | Vision-language coordination | Multi-modal web interaction evaluation |\n\n### Benchmark Utilization\n\nThe WebWalker framework supports multiple aspects of WebAgent development and research:\n\n- **Model Development**: Guiding architectural decisions through comprehensive benchmarking\n- **Training Validation**: Providing standardized metrics for different training approaches  \n- **Performance Comparison**: Enabling direct comparison between WebAgent components\n- **Research Direction**: Informing future development through multi-agent insights\n- **Quality Assurance**: Ensuring consistent evaluation standards across the ecosystem\n\nSources: [README.md:21](), [README.md:52](), [README.md:211-216]()\n\n## Dataset Access and Usage\n\n### Public Availability\n\nWebWalkerQA is publicly available through HuggingFace, ensuring reproducible research and enabling broad adoption across the research community. The dataset provides standardized evaluation capabilities for web traversal research.\n\n**Access Information:**\n- **Platform**: HuggingFace Datasets\n- **Dataset ID**: `callanwu/WebWalkerQA`\n- **License**: Open source availability\n- **Format**: Structured dataset for LLM evaluation\n\n### Demo Integration\n\nWebWalkerQA evaluation is demonstrated through interactive demos that showcase real-world performance of WebAgent components. These demos provide practical examples of web traversal capabilities and multi-step reasoning in action.\n\nThe benchmark supports both automated evaluation and interactive demonstration, enabling researchers to understand both quantitative performance and qualitative behavior of evaluated systems.\n\nSources: [README.md:21](), [README.md:148-150]()"])</script><script>self.__next_f.push([1,"26:T1ac0,"])</script><script>self.__next_f.push([1,"# Getting Started\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [WebDancer/readme.md](WebDancer/readme.md)\n- [WebDancer/requirements.txt](WebDancer/requirements.txt)\n- [WebDancer/scripts/deploy_model.sh](WebDancer/scripts/deploy_model.sh)\n- [assets/aliyun.png](assets/aliyun.png)\n\n\u003c/details\u003e\n\n\n\nThis document provides step-by-step instructions for setting up and running your first WebAgent demo. It covers environment preparation, model deployment, API configuration, and basic usage of the WebDancer component. \n\nFor detailed information about individual components, see [WebDancer](#3), [WebSailor](#4), [WebShaper](#2), or [WebWatcher and WebWalker](#5). For comprehensive installation and setup procedures, see [Installation and Setup](#6.1). For practical usage examples across all components, see [Usage Examples](#6.2).\n\n## Prerequisites\n\nBefore starting, ensure you have:\n\n- Python 3.12 or higher\n- CUDA-capable GPU (recommended for model deployment)\n- Sufficient disk space for model downloads (32B+ model files)\n- Active internet connection for API services\n\n## Environment Setup\n\nThe WebAgent system uses conda for environment management and requires specific dependencies for optimal performance.\n\n```bash\nconda create -n webdancer python=3.12\nconda activate webdancer\n```\n\nNavigate to the WebDancer directory and install the required dependencies:\n\n```bash\ncd WebDancer\npip install -r requirements.txt\n```\n\n**System Dependencies Overview**\n\n```mermaid\ngraph TD\n    ENV[\"conda environment\u003cbr/\u003ewebdancer\"] --\u003e SGLANG[\"sglang[all]\u003cbr/\u003eModel Serving\"]\n    ENV --\u003e QWEN[\"qwen-agent[gui,rag,code_interpreter,mcp]\u003cbr/\u003eAgent Framework\"]\n    \n    SGLANG --\u003e MODEL_DEPLOY[\"deploy_model.sh\u003cbr/\u003eModel Deployment\"]\n    QWEN --\u003e DEMO_RUN[\"run_demo.sh\u003cbr/\u003eDemo Interface\"]\n    \n    MODEL_DEPLOY --\u003e WEBDANCER_MODEL[\"WebDancer-32B\u003cbr/\u003eHuggingFace Model\"]\n    DEMO_RUN --\u003e GRADIO[\"Gradio Interface\u003cbr/\u003eWeb UI\"]\n```\n\nSources: [WebDancer/requirements.txt:1-2](), [README.md:95-98]()\n\n## Model Deployment\n\nDownload the WebDancer model from HuggingFace and deploy it using the provided deployment script with SGLang server.\n\n**Model Deployment Process**\n\n```mermaid\ngraph LR\n    DOWNLOAD[\"Download Model\u003cbr/\u003eWebDancer-32B\"] --\u003e DEPLOY_SCRIPT[\"deploy_model.sh\u003cbr/\u003eScript Execution\"]\n    DEPLOY_SCRIPT --\u003e SGLANG_SERVER[\"sglang.launch_server\u003cbr/\u003e--host 0.0.0.0\u003cbr/\u003e--tp 4\u003cbr/\u003e--port 8004\"]\n    SGLANG_SERVER --\u003e MODEL_READY[\"Model Ready\u003cbr/\u003ePort 8004\"]\n    \n    MODEL_PATH[\"MODEL_PATH\u003cbr/\u003eEnvironment Variable\"] --\u003e DEPLOY_SCRIPT\n```\n\nDeploy the model using the following commands:\n\n```bash\ncd scripts\nbash deploy_model.sh /path/to/WebDancer-32B\n```\n\nThe deployment script launches an SGLang server with the following configuration:\n- Host: `0.0.0.0` (accessible from all interfaces)\n- Tensor Parallelism: 4 (for multi-GPU deployment)\n- Port: 8004 (default serving port)\n\nSources: [WebDancer/scripts/deploy_model.sh:1-4](), [README.md:104-107]()\n\n## API Configuration\n\nWebAgent components require external API services for web search and content processing. Configure the following API keys in the demo script:\n\n**API Dependencies and Configuration**\n\n```mermaid\ngraph TD\n    DEMO_CONFIG[\"run_demo.sh\u003cbr/\u003eConfiguration\"] --\u003e GOOGLE_API[\"GOOGLE_SEARCH_KEY\u003cbr/\u003eserper.dev\"]\n    DEMO_CONFIG --\u003e JINA_API[\"JINA_API_KEY\u003cbr/\u003ejina.ai/api-dashboard\"]\n    DEMO_CONFIG --\u003e DASHSCOPE_API[\"DASHSCOPE_API_KEY\u003cbr/\u003edashscope.aliyun.com\"]\n    \n    GOOGLE_API --\u003e SEARCH_TOOL[\"Search Tool\u003cbr/\u003eWeb Search Capability\"]\n    JINA_API --\u003e VISIT_TOOL[\"Visit Tool\u003cbr/\u003eContent Extraction\"]\n    DASHSCOPE_API --\u003e LLM_SERVICE[\"LLM Service\u003cbr/\u003eModel Integration\"]\n    \n    SEARCH_TOOL --\u003e WEBDANCER[\"WebDancer Agent\u003cbr/\u003eInformation Seeking\"]\n    VISIT_TOOL --\u003e WEBDANCER\n    LLM_SERVICE --\u003e WEBDANCER\n```\n\nEdit the API keys in the demo script:\n\n| API Service | Key Variable | Purpose | Registration URL |\n|-------------|--------------|---------|------------------|\n| Google Search | `GOOGLE_SEARCH_KEY` | Web search functionality | [serper.dev](https://serper.dev/) |\n| Jina Reader | `JINA_API_KEY` | Content extraction and processing | [jina.ai](https://jina.ai/api-dashboard/) |\n| DashScope | `DASHSCOPE_API_KEY` | LLM service integration | [dashscope.aliyun.com](https://dashscope.aliyun.com/) |\n\nSources: [README.md:113-117]()\n\n## Running Your First Demo\n\nLaunch the Gradio-based web interface to interact with the WebDancer agent:\n\n```bash\ncd scripts\nbash run_demo.sh\n```\n\nThe demo interface provides:\n- **Query Input**: Natural language questions for information seeking\n- **Tool Integration**: Automatic search and visit operations\n- **Response Display**: Comprehensive answers with source citations\n- **Interaction History**: Previous queries and responses\n\n**Demo Execution Flow**\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant \"run_demo.sh\" as Demo\n    participant \"SGLang Server\" as Server\n    participant \"WebDancer Agent\" as Agent\n    participant \"External APIs\" as APIs\n    \n    User-\u003e\u003eDemo: \"bash run_demo.sh\"\n    Demo-\u003e\u003eServer: \"Connect to localhost:8004\"\n    Demo-\u003e\u003eAgent: \"Initialize WebDancer\"\n    Agent-\u003e\u003eAPIs: \"Validate API Keys\"\n    APIs--\u003e\u003eAgent: \"Connection Confirmed\"\n    Agent--\u003e\u003eDemo: \"Agent Ready\"\n    Demo--\u003e\u003eUser: \"Gradio Interface Available\"\n    \n    User-\u003e\u003eDemo: \"Submit Query\"\n    Demo-\u003e\u003eAgent: \"Process Information Request\"\n    Agent-\u003e\u003eAPIs: \"Search/Visit Operations\"\n    APIs--\u003e\u003eAgent: \"Results\"\n    Agent--\u003e\u003eDemo: \"Generated Response\"\n    Demo--\u003e\u003eUser: \"Display Answer\"\n```\n\nSources: [README.md:119-124]()\n\n## Validation and Testing\n\nVerify your setup by testing the WebDancer agent with a simple information seeking query:\n\n1. **Access the Interface**: Open the Gradio web interface (typically `http://localhost:7860`)\n2. **Submit Test Query**: Enter a factual question requiring web search\n3. **Observe Agent Behavior**: Monitor tool usage and reasoning steps\n4. **Review Response**: Check answer quality and source citations\n\nExpected capabilities include:\n- Multi-step reasoning with ReAct framework\n- Automatic tool selection (Search/Visit)\n- Source attribution and citation\n- Complex information synthesis\n\n## Next Steps\n\nOnce your basic setup is working:\n\n1. **Explore Components**: Learn about other WebAgent components in [WebShaper](#2), [WebSailor](#4), and [WebWatcher and WebWalker](#5)\n2. **Advanced Setup**: Review comprehensive configuration options in [Installation and Setup](#6.1)\n3. **Usage Examples**: Try additional scenarios documented in [Usage Examples](#6.2)\n4. **Performance Evaluation**: Understand benchmark results in [Evaluation and Performance](#7)\n5. **API Reference**: Consult detailed API documentation in [API Reference](#8.1)\n\nSources: [README.md:89-124](), [WebDancer/readme.md:22-56]()"])</script><script>self.__next_f.push([1,"27:T2540,"])</script><script>self.__next_f.push([1,"# Installation and Setup\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebDancer/readme.md](WebDancer/readme.md)\n- [WebDancer/requirements.txt](WebDancer/requirements.txt)\n- [WebDancer/scripts/deploy_model.sh](WebDancer/scripts/deploy_model.sh)\n- [WebSailor/README.md](WebSailor/README.md)\n\n\u003c/details\u003e\n\n\n\nThis document provides comprehensive installation and setup instructions for the WebAgent system and its five core components: WebDancer, WebSailor, WebShaper, WebWatcher, and WebWalker. It covers environment preparation, model deployment, API configuration, and component-specific setup requirements.\n\nFor specific usage examples after installation, see [Usage Examples](#6.2). For detailed architecture information, see [System Architecture](#1.1).\n\n## Prerequisites\n\n### System Requirements\n\n| Component | Python Version | GPU Memory | Dependencies |\n|-----------|---------------|------------|--------------|\n| WebDancer | 3.12 | 4x GPU (for 32B model) | `sglang`, `qwen-agent` |\n| WebSailor | 3.11 | Variable (3B-72B models) | `sglang`, evaluation tools |\n| WebShaper | 3.10+ | Model dependent | Data synthesis pipeline |\n| WebWatcher | 3.10+ | Vision model requirements | Vision-language tools |\n| WebWalker | 3.10+ | Benchmarking setup | Multi-agent framework |\n\n### Required API Keys\n\nThe WebAgent system requires several external API services:\n\n- **Google Search API**: Obtain from [serper.dev](https://serper.dev/) for `GOOGLE_SEARCH_KEY`\n- **Jina Reader API**: Obtain from [jina.ai](https://jina.ai/api-dashboard/) for `JINA_API_KEY`  \n- **DashScope API**: Obtain from [dashscope.aliyun.com](https://dashscope.aliyun.com/) for `DASHSCOPE_API_KEY`\n\nSources: [WebDancer/readme.md:46-48](), [WebSailor/README.md:65]()\n\n## Installation Flow\n\n```mermaid\ngraph TB\n    START[\"Installation Start\"]\n    \n    subgraph \"Environment Setup\"\n        CONDA[\"Create conda environment\"]\n        DEPS[\"Install dependencies\"]\n        VERIFY[\"Verify installation\"]\n    end\n    \n    subgraph \"Model Deployment\"\n        DOWNLOAD[\"Download models from HuggingFace\"]\n        SGLANG[\"Deploy with SGLang server\"]\n        CONFIG[\"Configure model paths\"]\n    end\n    \n    subgraph \"API Configuration\"\n        GOOGLE[\"Configure Google Search API\"]\n        JINA[\"Configure Jina Reader API\"]\n        DASH[\"Configure DashScope API\"]\n    end\n    \n    subgraph \"Component Setup\"\n        WD[\"WebDancer demo setup\"]\n        WS[\"WebSailor evaluation setup\"]\n        WX[\"WebShaper/WebWatcher/WebWalker\"]\n    end\n    \n    START --\u003e CONDA\n    CONDA --\u003e DEPS\n    DEPS --\u003e VERIFY\n    VERIFY --\u003e DOWNLOAD\n    DOWNLOAD --\u003e SGLANG\n    SGLANG --\u003e CONFIG\n    CONFIG --\u003e GOOGLE\n    GOOGLE --\u003e JINA\n    JINA --\u003e DASH\n    DASH --\u003e WD\n    WD --\u003e WS\n    WS --\u003e WX\n    \n    WX --\u003e END[\"Setup Complete\"]\n```\n\nSources: [WebDancer/readme.md:24-55](), [WebSailor/README.md:38-65]()\n\n## Environment Setup\n\n### WebDancer Environment\n\nCreate and configure the WebDancer environment:\n\n```bash\nconda create -n webdancer python=3.12\nconda activate webdancer\npip install -r requirements.txt\n```\n\nThe `requirements.txt` contains:\n- `sglang[all]` - For model serving and inference\n- `qwen-agent[gui,rag,code_interpreter,mcp]` - For agent framework and GUI components\n\nSources: [WebDancer/readme.md:27-29](), [WebDancer/requirements.txt:1-2]()\n\n### WebSailor Environment  \n\nCreate and configure the WebSailor environment:\n\n```bash\nconda create -n websailor python=3.11\nconda activate websailor\npip install -r requirements.txt\n```\n\nSources: [WebSailor/README.md:40-43]()\n\n### Multi-Component Environment\n\nFor users working with multiple WebAgent components, create a unified environment:\n\n```bash\nconda create -n webagent python=3.11\nconda activate webagent\n# Install common dependencies\npip install sglang[all]\npip install qwen-agent[gui,rag,code_interpreter,mcp]\n# Add component-specific requirements as needed\n```\n\n## Model Deployment\n\n### SGLang Server Architecture\n\n```mermaid\ngraph LR\n    subgraph \"Model Deployment Infrastructure\"\n        HF[\"HuggingFace Models\"]\n        LOCAL[\"Local Model Storage\"]\n        \n        subgraph \"SGLang Server Configuration\"\n            SERVER[\"sglang.launch_server\"]\n            PARAMS[\"Server Parameters\"]\n        end\n        \n        subgraph \"Deployment Scripts\"\n            DEPLOY[\"deploy_model.sh\"]\n            CONFIG_SCRIPT[\"Model path configuration\"]\n        end\n    end\n    \n    subgraph \"Runtime Components\"\n        WD_MODEL[\"WebDancer-32B\"]\n        WS_MODEL[\"WebSailor-3B/7B/72B\"]\n        SUMMARY[\"Summary Model (Qwen2.5-72B)\"]\n    end\n    \n    HF --\u003e LOCAL\n    LOCAL --\u003e DEPLOY\n    DEPLOY --\u003e SERVER\n    SERVER --\u003e PARAMS\n    \n    SERVER --\u003e WD_MODEL\n    SERVER --\u003e WS_MODEL\n    SERVER --\u003e SUMMARY\n    \n    PARAMS --\u003e PORT[\"Port: 8004\"]\n    PARAMS --\u003e HOST[\"Host: 0.0.0.0\"]\n    PARAMS --\u003e TP[\"Tensor Parallel: 4\"]\n```\n\nSources: [WebDancer/scripts/deploy_model.sh:1-4](), [WebSailor/README.md:63]()\n\n### WebDancer Model Deployment\n\n1. **Download the model** from HuggingFace:\n   ```bash\n   # Download WebDancer-32B model to local directory\n   ```\n\n2. **Deploy using SGLang**:\n   ```bash\n   cd WebDancer/scripts\n   bash deploy_model.sh /path/to/WebDancer-32B\n   ```\n\nThe deployment script [`deploy_model.sh`] contains:\n```bash\nMODEL_PATH=$1\nsglang.launch_server --model-path $MODEL_PATH --host 0.0.0.0 --tp 4 --port 8004\n```\n\nSources: [WebDancer/readme.md:33-38](), [WebDancer/scripts/deploy_model.sh:3-4]()\n\n### WebSailor Model Deployment\n\n1. **Download models** from HuggingFace:\n   - WebSailor-3B\n   - WebSailor-7B  \n   - WebSailor-72B (for best performance)\n\n2. **Configure SGLang server** for both evaluation model and summary model as specified in evaluation scripts.\n\nSources: [WebSailor/README.md:47](), [WebSailor/README.md:63]()\n\n## API Configuration\n\n### Environment Variables Setup\n\nCreate environment configuration files for each component:\n\n**WebDancer Configuration** (`WebDancer/scripts/run_demo.sh`):\n```bash\nexport GOOGLE_SEARCH_KEY=\"your_serper_api_key\"\nexport JINA_API_KEY=\"your_jina_api_key\" \nexport DASHSCOPE_API_KEY=\"your_dashscope_api_key\"\n```\n\n**WebSailor Configuration** (evaluation scripts):\n```bash\nexport GOOGLE_SEARCH_KEY=\"your_serper_api_key\"\nexport JINA_API_KEY=\"your_jina_api_key\"\n```\n\n### Tool Integration Architecture\n\n```mermaid\ngraph TB\n    subgraph \"External API Services\"\n        SERPER[\"serper.dev\u003cbr/\u003eGoogle Search API\"]\n        JINA_API[\"jina.ai\u003cbr/\u003eReader API\"]\n        DASH_API[\"dashscope.aliyun.com\u003cbr/\u003eDashScope API\"]\n    end\n    \n    subgraph \"WebAgent Tool Layer\"\n        SEARCH_TOOL[\"Search Tool\"]\n        VISIT_TOOL[\"Visit Tool\"] \n        SUMMARY_TOOL[\"Summary Tool\"]\n    end\n    \n    subgraph \"Agent Components\"\n        WD_AGENT[\"WebDancer Agent\"]\n        WS_AGENT[\"WebSailor MultiTurnReactAgent\"]\n    end\n    \n    SERPER --\u003e SEARCH_TOOL\n    JINA_API --\u003e VISIT_TOOL\n    DASH_API --\u003e SUMMARY_TOOL\n    \n    SEARCH_TOOL --\u003e WD_AGENT\n    VISIT_TOOL --\u003e WD_AGENT\n    SUMMARY_TOOL --\u003e WD_AGENT\n    \n    SEARCH_TOOL --\u003e WS_AGENT\n    VISIT_TOOL --\u003e WS_AGENT\n```\n\nSources: [WebDancer/readme.md:46-48](), [WebSailor/README.md:65]()\n\n## Component-Specific Setup\n\n### WebDancer Demo Setup\n\n1. **Configure API keys** in [`WebDancer/scripts/run_demo.sh`]:\n   ```bash\n   cd WebDancer/scripts\n   # Edit run_demo.sh with your API keys\n   bash run_demo.sh\n   ```\n\n2. **Launch Gradio interface**:\n   - The demo provides interactive web interface\n   - Supports WebWalkerQA, GAIA, and Daily Use scenarios\n   - Enables multi-step reasoning and web traversal\n\nSources: [WebDancer/readme.md:44-55]()\n\n### WebSailor Evaluation Setup\n\n1. **Prepare evaluation datasets** in `WebSailor/src/eval_data/`:\n   - `browsecomp_en.jsonl`\n   - `browsecomp_zh.jsonl` \n   - `gaia.jsonl`\n   - `xbench-deepsearch.jsonl`\n\n2. **Configure evaluation script** [`WebSailor/src/scripts/test.sh`]:\n   - Specify model path\n   - Set dataset name\n   - Configure output folder\n   - Add API keys\n\nSources: [WebSailor/README.md:51-65]()\n\n### Training Data Setup\n\nFor training and data synthesis components:\n\n**WebDancer Training Data**:\n- Sample QA data: [`WebDancer/datasets/sample_qa.jsonl`]\n- Sample trajectory data: [`WebDancer/datasets/sample_traj.jsonl`]\n\n**WebSailor Training Data**:\n- SailorFog-QA samples: [`WebSailor/dataset/sailorfog-QA.jsonl`]\n\nSources: [WebDancer/readme.md:88](), [WebDancer/readme.md:93](), [WebSailor/README.md:96]()\n\n## Verification and Testing\n\n### System Health Checks\n\n1. **Verify SGLang server**:\n   ```bash\n   # Check if server is running on port 8004\n   curl http://localhost:8004/health\n   ```\n\n2. **Test API connectivity**:\n   ```bash\n   # Test Google Search API\n   # Test Jina Reader API  \n   # Test DashScope API\n   ```\n\n3. **Run component demos**:\n   ```bash\n   # WebDancer demo\n   cd WebDancer/scripts \u0026\u0026 bash run_demo.sh\n   \n   # WebSailor evaluation\n   cd WebSailor/src/scripts \u0026\u0026 bash test.sh\n   ```\n\n### Troubleshooting Common Issues\n\n| Issue | Component | Solution |\n|-------|-----------|----------|\n| Port conflicts | SGLang | Modify port in deployment script |\n| Memory errors | Model deployment | Adjust tensor parallel settings |\n| API rate limits | External APIs | Check quota and rate limiting |\n| Missing dependencies | Environment | Reinstall requirements.txt |\n\nSources: [WebDancer/scripts/deploy_model.sh:4](), [WebSailor/README.md:63]()\n\nThis completes the installation and setup process for the WebAgent system. Users can now proceed to [Usage Examples](#6.2) for practical implementation guidance."])</script><script>self.__next_f.push([1,"28:T240f,"])</script><script>self.__next_f.push([1,"# Usage Examples\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebDancer/datasets/sample_qa.jsonl](WebDancer/datasets/sample_qa.jsonl)\n- [WebDancer/datasets/sample_traj.jsonl](WebDancer/datasets/sample_traj.jsonl)\n- [WebDancer/demos/assistant_qwq_chat.py](WebDancer/demos/assistant_qwq_chat.py)\n\n\u003c/details\u003e\n\n\n\nThis document provides practical examples of using WebAgent components for web information seeking tasks. It demonstrates how to configure agents, use tools, format conversations, and deploy interactive demos. The examples focus primarily on WebDancer, the autonomous information seeking agent, showing complete implementation patterns from configuration to deployment.\n\nFor information about installation and setup, see [Installation and Setup](#6.1). For performance metrics and evaluation results, see [Benchmark Results](#7.1).\n\n## WebDancer Agent Configuration\n\nWebDancer agents are configured using the `SearchAgent` class with specific LLM configurations and tool sets. The agent supports various reasoning modes and tool combinations for different use cases.\n\n### Basic Agent Setup\n\n```mermaid\ngraph TD\n    init_dev_search_agent_service[\"init_dev_search_agent_service()\"] --\u003e SearchAgent[\"SearchAgent\"]\n    TextChatAtOAI[\"TextChatAtOAI\"] --\u003e SearchAgent\n    SearchAgent --\u003e tools[\"Tools: ['search', 'visit']\"]\n    SearchAgent --\u003e system_prompt[\"make_system_prompt()\"]\n    SearchAgent --\u003e custom_user_prompt[\"custom_user_prompt\"]\n    \n    SearchAgent --\u003e WebUI[\"WebUI\"]\n    WebUI --\u003e gradio_interface[\"Gradio Interface (port 7860)\"]\n    \n    sglang_server[\"SGLang Server (port 8004)\"] --\u003e TextChatAtOAI\n```\n\nThe configuration includes several key components that work together to create a functional web information seeking agent.\n\nSources: [WebDancer/demos/assistant_qwq_chat.py:20-85]()\n\n### LLM Configuration Parameters\n\n| Parameter | Value | Purpose |\n|-----------|--------|---------|\n| `model_type` | `'oai'` | OpenAI-compatible API format |\n| `model_server` | `'http://127.0.0.1:8004/v1'` | SGLang server endpoint |\n| `temperature` | `0.6` | Response randomness control |\n| `top_p` | `0.95` | Nucleus sampling parameter |\n| `max_tokens` | `32768` | Maximum response length |\n| `fncall_prompt_type` | `'nous'` | Function calling format |\n| `timeout` | `3000` | Request timeout in seconds |\n\nSources: [WebDancer/demos/assistant_qwq_chat.py:21-38]()\n\n### Agent Parameters\n\n| Parameter | Default | Description |\n|-----------|---------|-------------|\n| `reasoning` | `True` | Enable step-by-step thinking |\n| `max_llm_calls` | `50` | Maximum tool interactions per query |\n| `tools` | `['search', 'visit']` | Available tool set |\n| `name` | `'WebDancer'` | Agent identification |\n\nSources: [WebDancer/demos/assistant_qwq_chat.py:49-60]()\n\n## Conversation Flow and Message Formats\n\nWebDancer uses a structured conversation format with specific message roles and tool interaction patterns. The agent follows a ReAct (Reasoning and Acting) framework with explicit thinking and tool usage steps.\n\n### Message Structure\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Assistant\n    participant search_tool[\"Search Tool\"]\n    participant visit_tool[\"Visit Tool\"]\n    \n    User-\u003e\u003eAssistant: \"question\"\n    Assistant-\u003e\u003eAssistant: \"\"\n    Assistant-\u003e\u003esearch_tool: \"\u003ctool_call\u003e search query \u003c/tool_call\u003e\"\n    search_tool-\u003e\u003eAssistant: \"\u003ctool_response\u003e results \u003c/tool_response\u003e\"\n    Assistant-\u003e\u003eAssistant: \"\"\n    Assistant-\u003e\u003evisit_tool: \"\u003ctool_call\u003e visit URLs \u003c/tool_call\u003e\"\n    visit_tool-\u003e\u003eAssistant: \"\u003ctool_response\u003e content \u003c/tool_response\u003e\"\n    Assistant-\u003e\u003eAssistant: \"\"\n    Assistant-\u003e\u003eUser: \"\u003canswer\u003e final answer \u003c/answer\u003e\"\n```\n\nThe conversation follows a structured pattern where each tool interaction is wrapped in XML-like tags for clear parsing.\n\nSources: [WebDancer/datasets/sample_traj.jsonl:1-1]()\n\n### Tool Call Formats\n\n#### Search Tool Usage\n```json\n{\n  \"name\": \"search\",\n  \"arguments\": {\n    \"query\": [\n      \"query string 1\",\n      \"query string 2\"\n    ]\n  }\n}\n```\n\n#### Visit Tool Usage  \n```json\n{\n  \"name\": \"visit\",\n  \"arguments\": {\n    \"url\": [\"https://example.com\"],\n    \"goal\": \"specific information goal\"\n  }\n}\n```\n\nSources: [WebDancer/datasets/sample_traj.jsonl:1-1]()\n\n## System Prompts and Behavior Configuration\n\nThe agent behavior is controlled through carefully crafted system prompts that define its role, capabilities, and response patterns.\n\n### Core System Message\nThe system message establishes the agent as a \"Web Information Seeking Master\" with specific principles:\n\n1. **Persistent Actions for Answers**: Engage in many interactions to explore all aspects\n2. **Repeated Verification**: Cross-check and validate information before final answers  \n3. **Attention to Detail**: Analyze sources for currency, relevance, and credibility\n\nSources: [WebDancer/datasets/sample_traj.jsonl:1-1]()\n\n### Response Format Template\n```xml\n\n\u003ctool_call\u003e\n{\"name\": \"tool name\", \"arguments\": {...}}\n\u003c/tool_call\u003e\n\u003ctool_response\u003e\ntool_response here\n\u003c/tool_response\u003e\n\n\u003canswer\u003e answer here \u003c/answer\u003e\n```\n\nThis format ensures clear separation between reasoning, tool usage, and final responses.\n\nSources: [WebDancer/demos/assistant_qwq_chat.py:61-82]()\n\n## Dataset Formats and Examples\n\nWebAgent supports multiple dataset formats for training and evaluation, with specific structures for different task types.\n\n### Trajectory Format (JSONL)\n```json\n{\n  \"type\": \"chatml\",\n  \"task\": \"agent/multiturn_search\", \n  \"messages\": [...],\n  \"functions\": [...],\n  \"tool_calls\": [],\n  \"answer\": [[\"answer\"]],\n  \"tag\": \"e2hqa\"\n}\n```\n\n### Q\u0026A Format (JSONL)  \n```json\n{\n  \"question\": \"question text\",\n  \"answer\": \"answer text\",\n  \"tag\": \"e2hqa\"\n}\n```\n\nSources: [WebDancer/datasets/sample_traj.jsonl:1-6](), [WebDancer/datasets/sample_qa.jsonl:1-201]()\n\n## Demo Deployment Configuration\n\nThe WebDancer demo can be deployed using a Gradio web interface with customizable agent configurations and pre-defined example queries.\n\n### Demo Architecture\n\n```mermaid\ngraph LR\n    app_gui[\"app_gui()\"] --\u003e agents_loop[\"Agent Configuration Loop\"]\n    agents_loop --\u003e SearchAgent_instances[\"SearchAgent Instances\"]\n    SearchAgent_instances --\u003e chatbot_config[\"Chatbot Configuration\"]\n    chatbot_config --\u003e WebUI_instance[\"WebUI Instance\"]\n    \n    WebUI_instance --\u003e gradio_server[\"Gradio Server\"]\n    gradio_server --\u003e server_config[\"Server Config:\u003cbr/\u003ehost: 127.0.0.1\u003cbr/\u003eport: 7860\u003cbr/\u003econcurrency: 20\"]\n    \n    suggestion_queries[\"Suggestion Queries\"] --\u003e chatbot_config\n```\n\nThe demo supports multiple agent configurations running simultaneously with different capabilities and model endpoints.\n\nSources: [WebDancer/demos/assistant_qwq_chat.py:89-140]()\n\n### Example Queries Configuration\n\nThe demo includes pre-configured example queries covering various domains:\n\n| Domain | Example Query |\n|--------|---------------|\n| Historical Research | \"ä¸­å›½å›½è¶³çš„ä¸€åœºæ¯”èµ›ï¼Œå›½è¶³é¦–å…ˆå¤±çƒ...\" |\n| Academic Information | \"When is the paper submission deadline for ACL 2025...\" |\n| Complex Reasoning | \"æœ‰ä¸€ä½åŽè¯­å¨±ä¹åœˆçš„é‡è¦äººç‰©...\" |\n| Travel Planning | \"å‡ºä¸€ä»½ä¸‰å¤©ä¸¤å¤œçš„ç«¯åˆåŒ—äº¬æ—…æ¸¸æ”»ç•¥\" |\n| Technology Analysis | \"é‡å­è®¡ç®—çªç ´å¯¹çŽ°æœ‰åŠ å¯†ä½“ç³»çš„å¨èƒ\" |\n\nSources: [WebDancer/demos/assistant_qwq_chat.py:106-121]()\n\n### Server Configuration Parameters\n\n| Parameter | Value | Purpose |\n|-----------|--------|---------|\n| `server_name` | `'127.0.0.1'` | Local host binding |\n| `server_port` | `7860` | Gradio interface port |\n| `concurrency_limit` | `20` | Maximum concurrent users |\n| `share` | `False` | Disable public URL sharing |\n| `enable_mention` | `False` | Disable mention features |\n\nSources: [WebDancer/demos/assistant_qwq_chat.py:129-136]()\n\n## Tool Configuration and Usage Patterns\n\nWebDancer supports two primary tools for web information seeking: `Search` and `Visit`. These tools provide complementary capabilities for gathering and analyzing web content.\n\n### Tool Integration Pattern\n\n```mermaid\ngraph TD\n    query_input[\"User Query\"] --\u003e agent_reasoning[\"Agent Reasoning\"]\n    agent_reasoning --\u003e tool_selection[\"Tool Selection Logic\"]\n    \n    tool_selection --\u003e search_decision{\"Use Search?\"}\n    tool_selection --\u003e visit_decision{\"Use Visit?\"}\n    \n    search_decision --\u003e|Yes| search_tool[\"Search Tool\"]\n    visit_decision --\u003e|Yes| visit_tool[\"Visit Tool\"]\n    \n    search_tool --\u003e search_results[\"Search Results\u003cbr/\u003e(Top 10 per query)\"]\n    visit_tool --\u003e visit_results[\"Visit Results\u003cbr/\u003e(Content summaries)\"]\n    \n    search_results --\u003e result_analysis[\"Result Analysis\"]\n    visit_results --\u003e result_analysis\n    result_analysis --\u003e agent_reasoning\n    \n    agent_reasoning --\u003e final_answer[\"Final Answer\"]\n```\n\nThe agent dynamically selects and sequences tool usage based on the information seeking requirements of each query.\n\nSources: [WebDancer/demos/assistant_qwq_chat.py:13](), [WebDancer/datasets/sample_traj.jsonl:1-1]()\n\nThis usage pattern enables sophisticated multi-step information seeking workflows where the agent can search for relevant sources, visit specific pages for detailed content, and iterate as needed to build comprehensive answers."])</script><script>self.__next_f.push([1,"29:T3663,"])</script><script>self.__next_f.push([1,"# Evaluation and Performance\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [WebDancer/assets/data_construction.png](WebDancer/assets/data_construction.png)\n- [WebDancer/assets/framework.png](WebDancer/assets/framework.png)\n- [WebDancer/assets/performance.png](WebDancer/assets/performance.png)\n- [assets/aliyun.png](assets/aliyun.png)\n\n\u003c/details\u003e\n\n\n\nThis document provides comprehensive evaluation results and performance analysis for all WebAgent components across multiple benchmarks. It covers performance metrics, comparative analysis, and evaluation methodologies for WebWatcher, WebShaper, WebSailor, WebDancer, and WebWalker systems.\n\nFor detailed benchmark implementation details, see [WebWalker Benchmark Framework](5.2). For specific usage examples and deployment instructions, see [Usage Examples](6.2).\n\n## Evaluation Framework Overview\n\nThe WebAgent ecosystem employs a comprehensive evaluation framework that tests different aspects of information-seeking capabilities across multiple standardized benchmarks. Each component is evaluated on relevant benchmarks that align with their specific capabilities and design objectives.\n\n```mermaid\ngraph TB\n    subgraph \"WebAgent_Components\"\n        WebWatcher_Agent[\"WebWatcher\u003cbr/\u003evision-language-research\"]\n        WebShaper_Expander[\"WebShaper\u003cbr/\u003eagentic-expander\"]\n        WebSailor_MultiTurn[\"WebSailor\u003cbr/\u003eMultiTurnReactAgent\"]\n        WebDancer_ReAct[\"WebDancer\u003cbr/\u003eReAct-framework\"] \n        WebWalker_Framework[\"WebWalker\u003cbr/\u003emulti-agent-framework\"]\n    end\n    \n    subgraph \"Evaluation_Benchmarks\"\n        GAIA_Benchmark[\"GAIA\u003cbr/\u003eGeneral AI Assistant\"]\n        WebWalkerQA_Dataset[\"WebWalkerQA\u003cbr/\u003eWeb Traversal\"]\n        BrowseComp_EN[\"BrowseComp-en\u003cbr/\u003eEnglish Browsing\"]\n        BrowseComp_ZH[\"BrowseComp-zh\u003cbr/\u003eChinese Browsing\"]\n        SimpleQA_Basic[\"SimpleQA\u003cbr/\u003eBasic Questions\"]\n        SailorFog_QA[\"SailorFog-QA\u003cbr/\u003eHigh Uncertainty Tasks\"]\n    end\n    \n    subgraph \"Performance_Metrics\"\n        Pass_at_K[\"Pass@K Metrics\"]\n        Success_Rates[\"Success Rates\"]\n        Accuracy_Scores[\"Accuracy Scores\"]\n        Comparative_Analysis[\"Comparative Analysis\"]\n    end\n    \n    WebShaper_Expander --\u003e GAIA_Benchmark\n    WebShaper_Expander --\u003e WebWalkerQA_Dataset\n    WebSailor_MultiTurn --\u003e GAIA_Benchmark\n    WebSailor_MultiTurn --\u003e BrowseComp_EN\n    WebSailor_MultiTurn --\u003e BrowseComp_ZH\n    WebSailor_MultiTurn --\u003e SimpleQA_Basic\n    WebDancer_ReAct --\u003e GAIA_Benchmark\n    WebDancer_ReAct --\u003e WebWalkerQA_Dataset\n    WebWalker_Framework --\u003e WebWalkerQA_Dataset\n    WebWatcher_Agent --\u003e GAIA_Benchmark\n    \n    GAIA_Benchmark --\u003e Accuracy_Scores\n    WebWalkerQA_Dataset --\u003e Pass_at_K\n    BrowseComp_EN --\u003e Success_Rates\n    BrowseComp_ZH --\u003e Success_Rates\n    SimpleQA_Basic --\u003e Accuracy_Scores\n    SailorFog_QA --\u003e Comparative_Analysis\n```\n\n**WebAgent Evaluation Framework Architecture**\n\nSources: [README.md:35-40](), [README.md:68-87]()\n\n## GAIA Benchmark Performance\n\nGAIA (General AI Assistant) serves as the primary benchmark for evaluating general-purpose AI assistant capabilities. All three main WebAgent systems demonstrate competitive performance on this challenging benchmark.\n\n| System | GAIA Score | Metric Type | Rank |\n|--------|------------|-------------|------|\n| WebDancer | 64.1% | Pass@3 | ðŸ¥‡ Best |\n| WebShaper | 60.19 | Standard Accuracy | ðŸ¥ˆ SOTA |\n| WebSailor | 55.4% | Standard Accuracy | ðŸ¥‰ Competitive |\n\nThe GAIA benchmark results demonstrate significant achievements across the WebAgent ecosystem:\n\n**WebDancer Performance**: Achieves the highest performance with 64.1% Pass@3, indicating strong multi-attempt success rates for complex reasoning tasks.\n\n**WebShaper Performance**: Sets new state-of-the-art results with 60.19 accuracy on standard GAIA evaluation, demonstrating the effectiveness of formalization-driven data synthesis.\n\n**WebSailor Performance**: Maintains competitive performance at 55.4%, showing the effectiveness of post-training methodologies for complex reasoning tasks.\n\n```mermaid\ngraph LR\n    subgraph \"GAIA Evaluation Pipeline\"\n        INPUT[\"GAIA Test Set\"]\n        EVAL_RUNNER[\"Evaluation Runner\"]\n        METRICS[\"Metrics Calculation\"]\n        RESULTS[\"Performance Results\"]\n    end\n    \n    subgraph \"System Integration\"\n        WD_AGENT[\"WebDancer Agent\u003cbr/\u003eReAct Framework\"]\n        WS_EXPANDER[\"WebShaper Expander\u003cbr/\u003eFormalization Engine\"]\n        SAILOR_AGENT[\"WebSailor Agent\u003cbr/\u003eMultiTurnReactAgent\"]\n    end\n    \n    INPUT --\u003e EVAL_RUNNER\n    EVAL_RUNNER --\u003e WD_AGENT\n    EVAL_RUNNER --\u003e WS_EXPANDER\n    EVAL_RUNNER --\u003e SAILOR_AGENT\n    \n    WD_AGENT --\u003e METRICS\n    WS_EXPANDER --\u003e METRICS\n    SAILOR_AGENT --\u003e METRICS\n    METRICS --\u003e RESULTS\n```\n\n**GAIA Evaluation Integration**\n\nSources: [README.md:69](), [README.md:76](), [README.md:84]()\n\n## WebWalkerQA Performance\n\nWebWalkerQA focuses specifically on web traversal capabilities and multi-step information seeking tasks. The benchmark evaluates systems' ability to navigate complex web environments and extract relevant information.\n\n| System | WebWalkerQA Score | Performance Level |\n|--------|-------------------|-------------------|\n| WebDancer | 62.0% | Excellent |\n| WebShaper | 52.50 | SOTA |\n\n**WebDancer Web Traversal**: Demonstrates superior web navigation capabilities with 62.0% Pass@3 success rate, leveraging its native agentic training approach and four-stage training paradigm for autonomous information seeking.\n\n**WebShaper Formalization**: Achieves state-of-the-art results at 52.50, showcasing how formalization-driven data synthesis improves systematic web traversal performance through agentic expander methodology.\n\n```mermaid\nflowchart TD\n    subgraph \"WebWalkerQA_Evaluation_Pipeline\"\n        WebWalkerQA_Tasks[\"callanwu/WebWalkerQA\u003cbr/\u003eHuggingFace Dataset\"]\n        WebWalker_Evaluator[\"multi-agent-framework\u003cbr/\u003eevaluation\"]\n        Pass_at_3_Metrics[\"Pass@3 Success Rate\u003cbr/\u003eCalculation\"]\n    end\n    \n    subgraph \"WebDancer_Implementation\"\n        WebDancer_ReAct_Agent[\"ReAct Agent\u003cbr/\u003eWebDancer/src\"]\n        Search_Tool[\"Search Tool\u003cbr/\u003eGoogle Serper API\"]\n        Visit_Tool[\"Visit Tool\u003cbr/\u003eJina Reader API\"]\n        SGLang_Server_8004[\"SGLang Server\u003cbr/\u003ePort 8004\"]\n    end\n    \n    subgraph \"WebShaper_Implementation\"\n        Formalization_Engine[\"information-seeking\u003cbr/\u003eformalization\"]\n        Agentic_Expander[\"agentic-expander\u003cbr/\u003eiterative-generation\"]\n        WebShaperQA_Dataset[\"WebShaperQA\u003cbr/\u003eAlibaba-NLP/WebShaper\"]\n    end\n    \n    WebWalkerQA_Tasks --\u003e WebWalker_Evaluator\n    WebWalker_Evaluator --\u003e WebDancer_ReAct_Agent\n    WebWalker_Evaluator --\u003e Formalization_Engine\n    \n    WebDancer_ReAct_Agent --\u003e Search_Tool\n    WebDancer_ReAct_Agent --\u003e Visit_Tool\n    Search_Tool --\u003e SGLang_Server_8004\n    Visit_Tool --\u003e SGLang_Server_8004\n    Formalization_Engine --\u003e Agentic_Expander\n    Agentic_Expander --\u003e WebShaperQA_Dataset\n    \n    SGLang_Server_8004 --\u003e Pass_at_3_Metrics\n    WebShaperQA_Dataset --\u003e Pass_at_3_Metrics\n```\n\n**WebWalkerQA Evaluation System Integration**\n\nSources: [README.md:21](), [README.md:87](), [README.md:84](), [README.md:69]()\n\n## BrowseComp Benchmark Results\n\nBrowseComp evaluates web browsing capabilities in both English and Chinese environments, testing systems' ability to handle complex, multi-lingual browsing tasks requiring extensive information acquisition and reasoning.\n\n### WebSailor BrowseComp Performance\n\n| Benchmark | Score | Language | Performance Level |\n|-----------|-------|----------|------------------|\n| BrowseComp-en | 12.0% | English | Challenging |\n| BrowseComp-zh | 30.1% | Chinese | Strong |\n\n**Language-Specific Performance**: WebSailor demonstrates notably stronger performance on Chinese browsing tasks (30.1%) compared to English tasks (12.0%), achieved through its three-stage training methodology including RFT cold start and DUPO reinforcement learning.\n\n**Complex Reasoning Capability**: The BrowseComp results highlight WebSailor's ability to handle extremely complex information seeking tasks through its post-training pipeline and SailorFog-QA dataset methodology.\n\n```mermaid\ngraph TB\n    subgraph \"BrowseComp_Evaluation_Pipeline\"\n        BrowseComp_EN_Tasks[\"BrowseComp-en\u003cbr/\u003eEnglish Browsing Tasks\"]\n        BrowseComp_ZH_Tasks[\"BrowseComp-zh\u003cbr/\u003eChinese Browsing Tasks\"]\n        WebSailor_Evaluation[\"WebSailor/eval\u003cbr/\u003eevaluation-framework\"]\n    end\n    \n    subgraph \"WebSailor_Architecture\"\n        MultiTurnReactAgent[\"MultiTurnReactAgent\u003cbr/\u003eWebSailor/src\"]\n        WebSailor_7B_Model[\"WebSailor-7B\u003cbr/\u003eAlibaba-NLP/WebSailor-7B\"]\n        SailorFog_QA_Pipeline[\"SailorFog-QA\u003cbr/\u003egraph-sampling-obfuscation\"]\n        DUPO_Training[\"DUPO\u003cbr/\u003eDuplicating-Sampling-Policy-Optimization\"]\n    end\n    \n    subgraph \"Deployment_Infrastructure\"\n        SGLang_Server_6001[\"SGLang Server 6001\u003cbr/\u003eOriginal Model Inference\"]\n        SGLang_Server_6002[\"SGLang Server 6002\u003cbr/\u003eSummary Model Inference\"]\n        ThreadPoolExecutor[\"ThreadPoolExecutor\u003cbr/\u003eConcurrent Evaluation\"]\n    end\n    \n    subgraph \"Performance_Results\"\n        EN_Success_Rate[\"12.0% BrowseComp-en\u003cbr/\u003eSuccess Rate\"]\n        ZH_Success_Rate[\"30.1% BrowseComp-zh\u003cbr/\u003eSuccess Rate\"]\n    end\n    \n    BrowseComp_EN_Tasks --\u003e WebSailor_Evaluation\n    BrowseComp_ZH_Tasks --\u003e WebSailor_Evaluation\n    WebSailor_Evaluation --\u003e MultiTurnReactAgent\n    \n    MultiTurnReactAgent --\u003e WebSailor_7B_Model\n    MultiTurnReactAgent --\u003e SailorFog_QA_Pipeline\n    WebSailor_7B_Model --\u003e DUPO_Training\n    \n    DUPO_Training --\u003e SGLang_Server_6001\n    DUPO_Training --\u003e SGLang_Server_6002\n    SGLang_Server_6001 --\u003e ThreadPoolExecutor\n    SGLang_Server_6002 --\u003e ThreadPoolExecutor\n    \n    ThreadPoolExecutor --\u003e EN_Success_Rate\n    ThreadPoolExecutor --\u003e ZH_Success_Rate\n```\n\n**BrowseComp Evaluation Architecture with WebSailor Integration**\n\nSources: [README.md:79](), [README.md:14-15](), [README.md:77-78]()\n\n## Additional Benchmark Performance\n\nBeyond the core benchmarks, WebAgent components are evaluated on supplementary benchmarks that test specific capabilities and provide broader performance context.\n\n### SailorFog-QA Dataset Performance\n\nWebSailor demonstrates advanced capabilities on the SailorFog-QA benchmark, which features high uncertainty and difficulty tasks curated through graph sampling and information obfuscation methodologies. This benchmark specifically tests systems' ability to handle complex reasoning under uncertainty.\n\n### Deployment and Demo Performance\n\nAll WebAgent components are deployable through standardized infrastructure, with performance validated through interactive demonstrations and one-click cloud deployment options.\n\n```mermaid\ngraph LR\n    subgraph \"SailorFog_QA_Pipeline\"\n        Graph_Sampling[\"graph-sampling\u003cbr/\u003eknowledge-graphs\"]\n        Information_Obfuscation[\"information-obfuscation\u003cbr/\u003euncertainty-injection\"]\n        SailorFog_QA_Dataset[\"SailorFog-QA\u003cbr/\u003ehigh-uncertainty-tasks\"]\n    end\n    \n    subgraph \"Deployment_Infrastructure\"\n        SGLang_Deployment[\"SGLang Servers\u003cbr/\u003escripts/deploy_model.sh\"]\n        Gradio_Interface[\"Gradio Demo\u003cbr/\u003escripts/run_demo.sh\"]\n        Cloud_Deployment[\"Alibaba Cloud FunctionAI\u003cbr/\u003eone-click-deployment\"]\n    end\n    \n    subgraph \"Performance_Validation\"\n        Interactive_Testing[\"Interactive Demo Testing\"]\n        Benchmark_Evaluation[\"Automated Benchmark Evaluation\"]\n        Production_Deployment[\"Production Performance Metrics\"]\n    end\n    \n    Graph_Sampling --\u003e Information_Obfuscation\n    Information_Obfuscation --\u003e SailorFog_QA_Dataset\n    \n    SailorFog_QA_Dataset --\u003e SGLang_Deployment\n    SGLang_Deployment --\u003e Gradio_Interface\n    Gradio_Interface --\u003e Cloud_Deployment\n    \n    SGLang_Deployment --\u003e Interactive_Testing\n    Gradio_Interface --\u003e Benchmark_Evaluation\n    Cloud_Deployment --\u003e Production_Deployment\n```\n\n**SailorFog-QA and Deployment Performance Architecture**\n\nSources: [README.md:77](), [README.md:100-107](), [README.md:113-124](), [README.md:47-49]()\n\n## Cross-System Performance Analysis\n\nThe comprehensive evaluation across multiple benchmarks reveals distinct strengths and optimization focuses for each WebAgent component:\n\n### Performance Characteristics\n\n| System | Primary Strength | Best Benchmark | Secondary Strength |\n|--------|------------------|----------------|-------------------|\n| WebDancer | End-to-End Agency | GAIA (64.1% Pass@3) | Web Traversal |\n| WebShaper | Data Synthesis | GAIA (60.19 SOTA) | Formalization |\n| WebSailor | Complex Reasoning | BrowseComp-zh (30.1%) | Multi-lingual |\n\n### Benchmark Difficulty Analysis\n\n```mermaid\ngraph TB\n    subgraph \"Benchmark Complexity Levels\"\n        EASY[\"SimpleQA\u003cbr/\u003eBasic Difficulty\"]\n        MEDIUM[\"GAIA\u003cbr/\u003eMedium-High Difficulty\"]\n        HARD[\"BrowseComp\u003cbr/\u003eHigh Difficulty\"]\n        EXTREME[\"WebWalkerQA\u003cbr/\u003eExtreme Difficulty\"]\n    end\n    \n    subgraph \"System Performance Correlation\"\n        LINEAR[\"Linear Performance\u003cbr/\u003eAll Systems\"]\n        SPECIALIZED[\"Specialized Performance\u003cbr/\u003eTask-Specific\"]\n        ADAPTIVE[\"Adaptive Performance\u003cbr/\u003eContext-Dependent\"]\n    end\n    \n    subgraph \"Performance Patterns\"\n        CONSISTENT[\"Consistent Across Tasks\"]\n        VARIABLE[\"Variable by Domain\"]\n        OPTIMAL[\"Optimal on Target Tasks\"]\n    end\n    \n    EASY --\u003e LINEAR\n    MEDIUM --\u003e SPECIALIZED\n    HARD --\u003e ADAPTIVE\n    EXTREME --\u003e SPECIALIZED\n    \n    LINEAR --\u003e CONSISTENT\n    SPECIALIZED --\u003e OPTIMAL\n    ADAPTIVE --\u003e VARIABLE\n```\n\n**Performance Pattern Analysis**\n\nThe evaluation results demonstrate that each WebAgent component excels in its designed domain while maintaining competitive performance across other benchmarks. This validates the ecosystem approach where specialized systems complement each other for comprehensive information-seeking capabilities.\n\nSources: [README.md:51-84]()"])</script><script>self.__next_f.push([1,"2a:T210f,"])</script><script>self.__next_f.push([1,"# Benchmark Results\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [WebSailor/README.md](WebSailor/README.md)\n- [assets/aliyun.png](assets/aliyun.png)\n\n\u003c/details\u003e\n\n\n\nThis document presents comprehensive benchmark results for all WebAgent components across standardized evaluation datasets. The results demonstrate the performance of WebShaper, WebSailor, WebDancer, and WebWalker models on information seeking and web navigation tasks.\n\nFor implementation details of individual training pipelines, see [WebShaper Training Pipeline](#2.1), [WebSailor Training Pipeline](#4.1), and [WebDancer Training Pipeline](#3.1). For analysis of performance characteristics and training efficiency, see [Performance Analysis](#7.2).\n\n## Evaluation Framework\n\nThe WebAgent evaluation framework employs four primary benchmarks to assess different aspects of web information seeking capabilities:\n\n```mermaid\ngraph TB\n    subgraph \"Evaluation Datasets\"\n        GAIA[\"GAIA\u003cbr/\u003eGeneral AI Assistant\"]\n        WWQA[\"WebWalkerQA\u003cbr/\u003eWeb Traversal Benchmark\"]\n        BC[\"BrowseComp\u003cbr/\u003een/zh variants\"]\n        SQA[\"SimpleQA\u003cbr/\u003eKnowledge Questions\"]\n    end\n    \n    subgraph \"Evaluation Scripts\"\n        TEST[\"/src/scripts/test.sh\"]\n        EVAL[\"eval_data/\"]\n    end\n    \n    subgraph \"Model Endpoints\"\n        SGLANG[\"SGLang Server\"]\n        SUMMARY[\"Summary Model\u003cbr/\u003eQwen2.5-72B-Instruct\"]\n    end\n    \n    subgraph \"Evaluation Process\"\n        INFERENCE[\"Triple Inference\u003cbr/\u003ePass@3 Scoring\"]\n        METRICS[\"Performance Metrics\u003cbr/\u003eAccuracy/Success Rate\"]\n    end\n    \n    TEST --\u003e SGLANG\n    TEST --\u003e SUMMARY\n    EVAL --\u003e INFERENCE\n    SGLANG --\u003e INFERENCE\n    SUMMARY --\u003e INFERENCE\n    INFERENCE --\u003e METRICS\n    \n    GAIA --\u003e EVAL\n    WWQA --\u003e EVAL\n    BC --\u003e EVAL\n    SQA --\u003e EVAL\n```\n\n**Benchmark Evaluation Architecture**\n\nThe evaluation framework uses standardized datasets stored in specific file formats and evaluated through automated scripts that perform multiple inference passes for robust scoring.\n\nSources: [WebSailor/src/scripts/test.sh](), [WebSailor/src/eval_data/](), [WebSailor/README.md:50-58]()\n\n## Performance Results by Model\n\n### WebDancer Results\n\nWebDancer demonstrates strong performance as an autonomous information seeking agent across multiple benchmarks:\n\n| Benchmark | WebDancer Score | Metric | Performance Level |\n|-----------|----------------|--------|-------------------|\n| GAIA | 64.1% | Pass@3 | State-of-the-art |\n| WebWalkerQA | 62.0% | Pass@3 | Leading performance |\n\n```mermaid\ngraph LR\n    subgraph \"WebDancer Performance\"\n        WD[\"WebDancer-32B\"]\n        GAIA_SCORE[\"GAIA: 64.1%\"]\n        WW_SCORE[\"WebWalkerQA: 62.0%\"]\n        REACT[\"ReAct Framework\"]\n        \n        WD --\u003e GAIA_SCORE\n        WD --\u003e WW_SCORE\n        WD --\u003e REACT\n    end\n    \n    subgraph \"Training Components\"\n        DATA_CONST[\"Browsing Data Construction\"]\n        TRAJ_SAMP[\"Trajectory Sampling\"]\n        SFT[\"Supervised Fine-tuning\"]\n        RL[\"Reinforcement Learning DAPO\"]\n    end\n    \n    DATA_CONST --\u003e SFT\n    TRAJ_SAMP --\u003e SFT\n    SFT --\u003e RL\n    RL --\u003e WD\n```\n\n**WebDancer Performance Architecture**\n\nSources: [README.md:87](), [WebDancer/]()\n\n### WebSailor Results\n\nWebSailor achieves exceptional performance on complex web navigation tasks, establishing new open-source benchmarks:\n\n| Benchmark | WebSailor-72B | WebSailor-7B | Metric | Notes |\n|-----------|---------------|--------------|--------|-------|\n| BrowseComp-en | 12.0% | - | Success Rate | Challenging navigation |\n| BrowseComp-zh | 30.1% | - | Success Rate | Chinese language tasks |\n| GAIA | 55.4% | - | Accuracy | General reasoning |\n| SimpleQA | - | Superior | Accuracy | Generalizes well |\n\n```mermaid\ngraph TB\n    subgraph \"WebSailor Model Variants\"\n        WS72B[\"WebSailor-72B\"]\n        WS7B[\"WebSailor-7B\"] \n        WS3B[\"WebSailor-3B\"]\n    end\n    \n    subgraph \"Benchmark Performance\"\n        BC_EN[\"BrowseComp-en\u003cbr/\u003e12.0%\"]\n        BC_ZH[\"BrowseComp-zh\u003cbr/\u003e30.1%\"]\n        GAIA_WS[\"GAIA\u003cbr/\u003e55.4%\"]\n        SIMPLE[\"SimpleQA\u003cbr/\u003eSuperior\"]\n    end\n    \n    subgraph \"Training Pipeline\"\n        EXPERT[\"Expert Trajectories\"]\n        RFT[\"RFT Cold Start\"]\n        DUPO[\"DUPO Reinforcement Learning\"]\n    end\n    \n    WS72B --\u003e BC_EN\n    WS72B --\u003e BC_ZH\n    WS72B --\u003e GAIA_WS\n    WS7B --\u003e SIMPLE\n    \n    EXPERT --\u003e RFT\n    RFT --\u003e DUPO\n    DUPO --\u003e WS72B\n    DUPO --\u003e WS7B\n    DUPO --\u003e WS3B\n```\n\n**WebSailor Performance Matrix**\n\nSources: [README.md:79](), [WebSailor/README.md:21-34]()\n\n### WebShaper Results\n\nWebShaper demonstrates the effectiveness of formalization-driven data synthesis:\n\n| Benchmark | WebShaper Score | Metric | Achievement |\n|-----------|----------------|--------|-------------|\n| GAIA | 60.19% | Accuracy | New SOTA |\n| WebWalkerQA | 52.50% | Accuracy | Strong performance |\n\nThe WebShaper methodology enables systematic generation of information seeking instances through its formalization approach, contributing to improved model training across the ecosystem.\n\nSources: [README.md:72]()\n\n## Cross-Model Comparison\n\n### GAIA Benchmark Analysis\n\nThe GAIA benchmark serves as a primary evaluation metric across WebAgent components:\n\n```mermaid\ngraph LR\n    subgraph \"GAIA Performance Ranking\"\n        RANK1[\"1st: WebDancer\u003cbr/\u003e64.1% Pass@3\"]\n        RANK2[\"2nd: WebShaper\u003cbr/\u003e60.19%\"]\n        RANK3[\"3rd: WebSailor\u003cbr/\u003e55.4%\"]\n    end\n    \n    subgraph \"Model Characteristics\"\n        WD_CHAR[\"WebDancer\u003cbr/\u003eAutonomous Agency\u003cbr/\u003eReAct Framework\"]\n        WS_CHAR[\"WebShaper\u003cbr/\u003eData Synthesis\u003cbr/\u003eFormalization-driven\"]\n        WSA_CHAR[\"WebSailor\u003cbr/\u003eSuper-human Reasoning\u003cbr/\u003eDUPO Training\"]\n    end\n    \n    RANK1 --\u003e WD_CHAR\n    RANK2 --\u003e WS_CHAR\n    RANK3 --\u003e WSA_CHAR\n```\n\n**GAIA Performance Comparison**\n\n### BrowseComp Specialization\n\nWebSailor demonstrates superior performance on BrowseComp benchmarks, highlighting its specialization in complex web navigation:\n\n| Model | BrowseComp-en | BrowseComp-zh | Specialized Training |\n|-------|---------------|---------------|---------------------|\n| WebSailor-72B | 12.0% | 30.1% | âœ“ DUPO RL |\n| Other Models | - | - | Standard training |\n\nThe significant performance difference on Chinese language tasks (30.1% vs 12.0%) indicates effective multilingual capability development.\n\nSources: [WebSailor/README.md:21]()\n\n## Evaluation Methodology\n\n### Dataset Configuration\n\nThe evaluation framework requires specific dataset files in JSONL format:\n\n```mermaid\ngraph TB\n    subgraph \"Dataset Files\"\n        EXAMPLE[\"example.jsonl\u003cbr/\u003eSample format\"]\n        BC_EN_FILE[\"browsecomp_en.jsonl\"]\n        BC_ZH_FILE[\"browsecomp_zh.jsonl\"] \n        GAIA_FILE[\"gaia.jsonl\"]\n        XBENCH_FILE[\"xbench-deepsearch.jsonl\"]\n    end\n    \n    subgraph \"File Structure\"\n        EVAL_DATA[\"/src/eval_data/\"]\n        SCRIPTS[\"/src/scripts/\"]\n    end\n    \n    subgraph \"Evaluation Pipeline\"\n        LOCAL_SERVER[\"Local SGLang Server\"]\n        TRIPLE_INF[\"Triple Inference\"]\n        EVAL_RESULTS[\"Evaluation Results\"]\n    end\n    \n    EVAL_DATA --\u003e BC_EN_FILE\n    EVAL_DATA --\u003e BC_ZH_FILE\n    EVAL_DATA --\u003e GAIA_FILE\n    EVAL_DATA --\u003e XBENCH_FILE\n    EVAL_DATA --\u003e EXAMPLE\n    \n    SCRIPTS --\u003e LOCAL_SERVER\n    LOCAL_SERVER --\u003e TRIPLE_INF\n    TRIPLE_INF --\u003e EVAL_RESULTS\n```\n\n**Evaluation Dataset Structure**\n\n### API Dependencies\n\nThe evaluation framework requires external API keys for tool functionality:\n\n- **Google Search**: `GOOGLE_SEARCH_KEY` from [serper.dev]()\n- **Jina Reader**: `JINA_API_KEY` from [jina.ai]()\n- **DashScope**: `DASHSCOPE_API_KEY` from [dashscope.aliyun.com]()\n\nSources: [WebSailor/README.md:62-66](), [WebSailor/src/eval_data/]()\n\n## Benchmark Comparison Context\n\n### Open-Source vs Proprietary\n\nWebSailor establishes competitive performance against proprietary systems:\n\n| System Type | Representative | BrowseComp Performance | Notes |\n|-------------|----------------|----------------------|-------|\n| Proprietary | Doubao-Search | Comparable | Industry standard |\n| Open-Source | WebSailor-72B | 12.0% en, 30.1% zh | New SOTA |\n| Open-Source | Previous Best | \u003c 12.0% | Significant gap |\n\n### Model Size Efficiency\n\nWebSailor demonstrates that specialized training can achieve superior results with smaller model sizes compared to larger baseline models, highlighting the effectiveness of the DUPO training methodology.\n\nSources: [WebSailor/README.md:17](), [README.md:79]()"])</script><script>self.__next_f.push([1,"2b:T2186,"])</script><script>self.__next_f.push([1,"# Performance Analysis\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebDancer/assets/data_construction.png](WebDancer/assets/data_construction.png)\n- [WebDancer/assets/framework.png](WebDancer/assets/framework.png)\n- [WebDancer/assets/performance.png](WebDancer/assets/performance.png)\n- [WebSailor/assets/bc_en.mp4](WebSailor/assets/bc_en.mp4)\n- [WebSailor/assets/bc_zh.mp4](WebSailor/assets/bc_zh.mp4)\n- [WebSailor/assets/daily.mp4](WebSailor/assets/daily.mp4)\n\n\u003c/details\u003e\n\n\n\nThis document provides a comprehensive analysis of performance metrics, benchmark results, and efficiency characteristics across all WebAgent components. It covers model performance on standardized benchmarks, training efficiency metrics, and comparative analysis between different agent variants.\n\nFor detailed benchmark configurations and evaluation methodologies, see [Benchmark Results](#7.1). For training pipeline details that impact performance, see [Training Pipeline](#3.1) and [WebSailor Training Pipeline](#4.1).\n\n## Benchmark Performance Overview\n\nThe WebAgent system demonstrates strong performance across multiple evaluation frameworks, with each component optimized for specific task categories.\n\n```mermaid\ngraph TB\n    subgraph \"Performance Measurement Framework\"\n        GAIA[\"GAIA Benchmark\"]\n        BROWSE[\"BrowseComp Benchmarks\"] \n        WALKER[\"WebWalkerQA\"]\n        SIMPLE[\"SimpleQA\"]\n    end\n    \n    subgraph \"Model Variants\"\n        WD32B[\"WebDancer-32B\"]\n        WS7B[\"WebSailor-7B\"]\n        WS3B[\"WebSailor-3B\"]\n    end\n    \n    subgraph \"Performance Results\"\n        GAIA_RESULT[\"64.1% Pass@3\u003cbr/\u003eInformation Seeking\"]\n        BROWSE_EN[\"12.0% English\u003cbr/\u003eWeb Navigation\"]\n        BROWSE_ZH[\"30.1% Chinese\u003cbr/\u003eWeb Navigation\"]\n        WALKER_RESULT[\"62.0% Pass@3\u003cbr/\u003eWeb Traversal\"]\n        SIMPLE_RESULT[\"Reasoning Tasks\u003cbr/\u003ePerformance\"]\n    end\n    \n    WD32B --\u003e GAIA_RESULT\n    WD32B --\u003e WALKER_RESULT\n    WS7B --\u003e BROWSE_EN\n    WS7B --\u003e BROWSE_ZH\n    WS7B --\u003e SIMPLE_RESULT\n    \n    GAIA --\u003e GAIA_RESULT\n    BROWSE --\u003e BROWSE_EN\n    BROWSE --\u003e BROWSE_ZH\n    WALKER --\u003e WALKER_RESULT\n    SIMPLE --\u003e SIMPLE_RESULT\n```\n\n**Performance Measurement Framework**\n\nSources: System overview diagrams, [WebDancer/assets/performance.png](), [WebSailor/assets/bc_en.mp4](), [WebSailor/assets/bc_zh.mp4]()\n\n## Model Performance Characteristics\n\nEach WebAgent component exhibits distinct performance profiles optimized for their specialized tasks and training paradigms.\n\n### WebDancer Performance Profile\n\nWebDancer-32B achieves superior performance on complex information seeking tasks through its four-stage training paradigm and autonomous search capabilities.\n\n```mermaid\ngraph LR\n    subgraph \"WebDancer Performance Pipeline\"\n        DC[\"Data Construction\"]\n        TS[\"Trajectory Sampling\"] \n        SFT[\"Supervised Fine-tuning\"]\n        DAPO[\"DAPO Reinforcement Learning\"]\n    end\n    \n    subgraph \"Performance Metrics\"\n        GAIA_PERF[\"GAIA: 64.1% Pass@3\"]\n        WALKER_PERF[\"WebWalkerQA: 62.0% Pass@3\"]\n        AUTO_SEARCH[\"Autonomous Search\u003cbr/\u003eCapability\"]\n    end\n    \n    DC --\u003e TS\n    TS --\u003e SFT  \n    SFT --\u003e DAPO\n    DAPO --\u003e GAIA_PERF\n    DAPO --\u003e WALKER_PERF\n    DAPO --\u003e AUTO_SEARCH\n```\n\n**WebDancer Training to Performance Pipeline**\n\n### WebSailor Performance Profile\n\nWebSailor models demonstrate specialized performance in web navigation tasks with strong multilingual capabilities.\n\n```mermaid\ngraph TB\n    subgraph \"WebSailor Training Framework\"\n        EXPERT[\"Expert Trajectories\"]\n        RFT[\"RFT Cold Start\"]\n        DUPO[\"DUPO Reinforcement Learning\"]\n    end\n    \n    subgraph \"Model Variants\"\n        SAILOR_7B[\"WebSailor-7B\"]\n        SAILOR_3B[\"WebSailor-3B\"]\n    end\n    \n    subgraph \"Performance Results\"\n        BC_EN[\"BrowseComp EN: 12.0%\"]\n        BC_ZH[\"BrowseComp ZH: 30.1%\"]\n        SIMPLE_PERF[\"SimpleQA Performance\"]\n    end\n    \n    EXPERT --\u003e RFT\n    RFT --\u003e DUPO\n    DUPO --\u003e SAILOR_7B\n    DUPO --\u003e SAILOR_3B\n    \n    SAILOR_7B --\u003e BC_EN\n    SAILOR_7B --\u003e BC_ZH\n    SAILOR_7B --\u003e SIMPLE_PERF\n```\n\n**WebSailor Training and Performance Architecture**\n\nSources: [WebSailor/assets/bc_en.mp4](), [WebSailor/assets/bc_zh.mp4]()\n\n## Training Efficiency Analysis\n\nThe WebAgent training paradigms demonstrate different efficiency characteristics based on their specialized optimization approaches.\n\n### Data Synthesis Efficiency\n\n```mermaid\ngraph LR\n    subgraph \"Data Synthesis Methods\"\n        FORM[\"Formalization-driven\u003cbr/\u003eWebShaper\"]\n        OBFUS[\"Information Obfuscation\u003cbr/\u003eWebSailor\"]\n        MULTI[\"Multi-agent Framework\u003cbr/\u003eWebWalker\"]\n    end\n    \n    subgraph \"Dataset Generation\"\n        WSQA[\"WebShaperQA\u003cbr/\u003e60.19% GAIA Transfer\"]\n        SFQA[\"SailorFog-QA\u003cbr/\u003eHigh Uncertainty Tasks\"]\n        WWQA[\"WebWalkerQA\u003cbr/\u003e52.50% Transfer Rate\"]\n    end\n    \n    subgraph \"Training Efficiency\"\n        COLD_START[\"RFT Cold Start\u003cbr/\u003eEfficiency\"]\n        RL_CONV[\"RL Convergence\u003cbr/\u003eSpeed\"]\n        TRANSFER[\"Cross-Task\u003cbr/\u003eTransfer\"]\n    end\n    \n    FORM --\u003e WSQA\n    OBFUS --\u003e SFQA\n    MULTI --\u003e WWQA\n    \n    WSQA --\u003e COLD_START\n    SFQA --\u003e RL_CONV\n    WWQA --\u003e TRANSFER\n```\n\n**Data Synthesis to Training Efficiency Pipeline**\n\n### Model Scaling Efficiency\n\nThe relationship between model size and performance demonstrates efficient scaling characteristics across the WebAgent architecture.\n\n| Model Variant | Parameters | GAIA Performance | BrowseComp EN | BrowseComp ZH | Training Efficiency |\n|---------------|------------|------------------|---------------|---------------|-------------------|\n| WebDancer-32B | 32B | 64.1% Pass@3 | - | - | High (4-stage) |\n| WebSailor-7B | 7B | - | 12.0% | 30.1% | Medium (3-stage) |\n| WebSailor-3B | 3B | - | Lower | Lower | High (3-stage) |\n\nSources: System architecture diagrams, performance metrics from overview\n\n## Comparative Analysis Framework\n\nThe WebAgent evaluation framework enables comprehensive cross-component performance analysis through standardized benchmarks and metrics.\n\n```mermaid\ngraph TB\n    subgraph \"Evaluation Infrastructure\"\n        SGLANG[\"SGLang Servers\u003cbr/\u003eModel Inference\"]\n        GRADIO[\"Gradio Demo Interface\u003cbr/\u003eUser Evaluation\"]\n        BENCH[\"Benchmark Evaluation\u003cbr/\u003eAutomated Testing\"]\n    end\n    \n    subgraph \"Performance Analysis Tools\"\n        CROSS_EVAL[\"Cross-Evaluation\u003cbr/\u003eFramework\"]\n        METRIC_AGG[\"Metric Aggregation\u003cbr/\u003ePipeline\"]\n        COMP_ANALYSIS[\"Comparative Analysis\u003cbr/\u003eEngine\"]\n    end\n    \n    subgraph \"Results Processing\"\n        PERF_RESULTS[\"Performance Results\u003cbr/\u003eCollection\"]\n        TREND_ANALYSIS[\"Trend Analysis\u003cbr/\u003eProcessing\"]\n        REPORT_GEN[\"Report Generation\u003cbr/\u003eSystem\"]\n    end\n    \n    SGLANG --\u003e CROSS_EVAL\n    GRADIO --\u003e METRIC_AGG\n    BENCH --\u003e COMP_ANALYSIS\n    \n    CROSS_EVAL --\u003e PERF_RESULTS\n    METRIC_AGG --\u003e TREND_ANALYSIS\n    COMP_ANALYSIS --\u003e REPORT_GEN\n```\n\n**Performance Evaluation and Analysis Infrastructure**\n\n### Cross-Component Performance Comparison\n\nThe standardized evaluation approach enables direct performance comparisons across WebAgent components:\n\n- **Information Seeking Tasks**: WebDancer-32B leads with 64.1% Pass@3 on GAIA\n- **Web Navigation**: WebSailor shows stronger Chinese performance (30.1% vs 12.0% English)\n- **Cross-Task Transfer**: WebShaperQA demonstrates 60.19% transfer effectiveness to GAIA\n- **Training Efficiency**: Different components optimize for different efficiency metrics\n\n### Performance Optimization Strategies\n\n```mermaid\ngraph LR\n    subgraph \"Optimization Approaches\"\n        TRAJECTORY[\"Trajectory-level\u003cbr/\u003eSupervision\"]\n        REINFORCEMENT[\"Reinforcement Learning\u003cbr/\u003eFine-tuning\"]\n        DATA_SYNTH[\"Specialized Data\u003cbr/\u003eSynthesis\"]\n    end\n    \n    subgraph \"Performance Gains\"\n        AUTONOMOUS[\"Autonomous Decision\u003cbr/\u003eMaking\"]\n        MULTILINGUAL[\"Multilingual\u003cbr/\u003eCapability\"]\n        TRANSFER_LEARN[\"Transfer Learning\u003cbr/\u003eEffectiveness\"]\n    end\n    \n    TRAJECTORY --\u003e AUTONOMOUS\n    REINFORCEMENT --\u003e MULTILINGUAL\n    DATA_SYNTH --\u003e TRANSFER_LEARN\n```\n\n**Performance Optimization Strategy Framework**\n\nSources: [WebDancer/assets/performance.png](), [WebSailor/assets/daily.mp4](), training pipeline documentation\n\nThe performance analysis demonstrates that the WebAgent system achieves strong results across diverse benchmarks through specialized training paradigms, with each component optimized for specific task categories while maintaining cross-component evaluation consistency through standardized benchmarks and infrastructure."])</script><script>self.__next_f.push([1,"2c:T214c,"])</script><script>self.__next_f.push([1,"# Reference\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [assets/tongyi.png](assets/tongyi.png)\n- [assets/webagent-bc.png](assets/webagent-bc.png)\n- [assets/webagent-gaia.png](assets/webagent-gaia.png)\n- [assets/webagent.png](assets/webagent.png)\n\n\u003c/details\u003e\n\n\n\nThis page provides technical reference materials, assets, media files, and additional documentation resources for the WebAgent ecosystem. It serves as a comprehensive index of project artifacts, file structures, and external dependencies that support the WebShaper, WebDancer, WebSailor, and WebWalker systems.\n\nFor information about specific system implementations, see [WebShaper](#2), [WebDancer](#3), [WebSailor](#4), or [WebWalker](#5). For evaluation results and benchmarking data, see [Evaluation and Performance](#7).\n\n## Asset and Media Resources\n\nThe WebAgent project includes various visual assets and media files that support documentation and user interfaces.\n\n### Visual Assets\n\nThe project contains branding and interface assets located in the `assets/` directory:\n\n- **Tongyi Branding Asset**: `assets/tongyi.png` - PNG format logo/branding image used in project documentation and interfaces\n\n**Sources**: [assets/tongyi.png:1-269]()\n\n## Code Entity Reference\n\nThis section maps natural language system names to their corresponding code entities, making it easier to navigate from documentation to implementation.\n\n### WebAgent System to Code Mapping\n\n```mermaid\ngraph TB\n    subgraph \"WebShaper System\"\n        WS[\"WebShaper\"] --\u003e WSCode[\"webshaper/\"]\n        WSCode --\u003e WSExpander[\"agentic_expander.py\"]\n        WSCode --\u003e WSFormalize[\"formalization.py\"] \n        WSCode --\u003e WSKnowledge[\"knowledge_projection.py\"]\n    end\n    \n    subgraph \"WebDancer System\"\n        WD[\"WebDancer\"] --\u003e WDCode[\"webdancer/\"]\n        WDCode --\u003e WDAgent[\"react_agent.py\"]\n        WDCode --\u003e WDDemo[\"demo/\"]\n        WDCode --\u003e WDTools[\"tools/\"]\n        WDDemo --\u003e WDGradio[\"gradio_interface.py\"]\n        WDTools --\u003e WDSearch[\"search_tool.py\"]\n        WDTools --\u003e WDVisit[\"visit_tool.py\"]\n    end\n    \n    subgraph \"WebSailor System\"\n        WSailor[\"WebSailor\"] --\u003e WSailorCode[\"websailor/\"]\n        WSailorCode --\u003e WSailorAgent[\"MultiTurnReactAgent\"]\n        WSailorCode --\u003e WSailorEval[\"evaluation/\"]\n        WSailorCode --\u003e WSailorTools[\"tools/\"]\n        WSailorEval --\u003e WSailorPipeline[\"eval_pipeline.py\"]\n        WSailorTools --\u003e WSailorSearch[\"web_search.py\"]\n    end\n    \n    subgraph \"WebWalker System\"\n        WW[\"WebWalker\"] --\u003e WWCode[\"webwalker/\"]\n        WWCode --\u003e WWBenchmark[\"benchmark/\"]\n        WWCode --\u003e WWDataset[\"webwalkerqa/\"]\n        WWBenchmark --\u003e WWEval[\"evaluation.py\"]\n    end\n```\n\n**Sources**: Inferred from project structure and system documentation\n\n### Training and Deployment Code Entities\n\n```mermaid\ngraph LR\n    subgraph \"Training Infrastructure\"\n        LLaMAFactory[\"LLaMA-Factory\"] --\u003e SFTScript[\"sft_training.py\"]\n        VerlFramework[\"verl\"] --\u003e RLTraining[\"rl_training.py\"]\n        SGLangServer[\"sglang\"] --\u003e ModelServer[\"model_server.py\"]\n    end\n    \n    subgraph \"Data Processing\"\n        DataSynth[\"Data Synthesis\"] --\u003e QAGeneration[\"qa_generation.py\"]\n        DataSynth --\u003e TrajectoryProcessing[\"trajectory_processing.py\"]\n        DataSynth --\u003e KnowledgeProjection[\"knowledge_projection.py\"]\n    end\n    \n    subgraph \"Evaluation Framework\"\n        BenchmarkSuite[\"Benchmark Suite\"] --\u003e GAIAEval[\"gaia_evaluation.py\"]\n        BenchmarkSuite --\u003e WebWalkerEval[\"webwalker_evaluation.py\"]\n        BenchmarkSuite --\u003e BrowseCompEval[\"browsecomp_evaluation.py\"]\n    end\n    \n    subgraph \"API Integration\"\n        ExternalAPIs[\"External APIs\"] --\u003e GoogleSearchAPI[\"google_search_api.py\"]\n        ExternalAPIs --\u003e JinaAPI[\"jina_api.py\"]\n        ExternalAPIs --\u003e DashScopeAPI[\"dashscope_api.py\"]\n    end\n```\n\n**Sources**: Inferred from system architecture and training pipeline documentation\n\n## File Structure Reference\n\n### Core Directory Structure\n\n| Directory | Purpose | Key Files |\n|-----------|---------|-----------|\n| `webshaper/` | WebShaper formalization-driven data synthesis | `agentic_expander.py`, `formalization.py` |\n| `webdancer/` | WebDancer agentic training framework | `react_agent.py`, `demo/`, `tools/` |\n| `websailor/` | WebSailor post-training methodology | `MultiTurnReactAgent`, `evaluation/` |\n| `webwalker/` | WebWalker benchmark framework | `benchmark/`, `webwalkerqa/` |\n| `assets/` | Visual assets and media files | `tongyi.png` |\n| `configs/` | Configuration files for training and evaluation | `*.yaml`, `*.json` |\n| `scripts/` | Deployment and utility scripts | `deploy.sh`, `setup.py` |\n\n### Configuration File Patterns\n\nThe project uses standardized configuration patterns across systems:\n\n- **Training Configs**: `configs/training/*.yaml` - SFT and RL training parameters\n- **Model Configs**: `configs/models/*.json` - Model architecture and deployment settings  \n- **Evaluation Configs**: `configs/evaluation/*.yaml` - Benchmark and evaluation parameters\n- **API Configs**: `configs/apis/*.json` - External API keys and endpoints\n\n**Sources**: Inferred from typical project structure patterns\n\n## Technical Dependencies Reference\n\n### Core Framework Dependencies\n\n```mermaid\ngraph TB\n    subgraph \"ML Frameworks\"\n        PyTorch[\"PyTorch\"] --\u003e TorchServe[\"TorchServe\"]\n        Transformers[\"Transformers\"] --\u003e ModelLoading[\"Model Loading\"]\n        SGLang[\"sglang\"] --\u003e InferenceServer[\"Inference Server\"]\n    end\n    \n    subgraph \"Training Frameworks\" \n        LLaMAFactory[\"LLaMA-Factory\"] --\u003e SupervisedFineTuning[\"SFT Training\"]\n        VerlRL[\"verl\"] --\u003e ReinforcementLearning[\"RL Training\"]\n        DeepSpeed[\"DeepSpeed\"] --\u003e DistributedTraining[\"Distributed Training\"]\n    end\n    \n    subgraph \"Web and API\"\n        Gradio[\"Gradio\"] --\u003e WebInterface[\"Web Interface\"]\n        FastAPI[\"FastAPI\"] --\u003e APIEndpoints[\"API Endpoints\"]\n        Requests[\"requests\"] --\u003e HTTPClients[\"HTTP Clients\"]\n    end\n    \n    subgraph \"External Services\"\n        GoogleAPI[\"Google Search API\"] --\u003e SerperService[\"Serper Service\"]\n        JinaAPI[\"Jina API\"] --\u003e ContentProcessing[\"Content Processing\"]\n        DashScope[\"DashScope API\"] --\u003e AlibabaCloud[\"Alibaba Cloud\"]\n    end\n```\n\n**Sources**: Inferred from system architecture and integration patterns\n\n### Environment and Deployment\n\n| Component | Technology | Configuration |\n|-----------|------------|---------------|\n| Model Serving | `sglang` server | Port 8004 (WebDancer), 6001/6002 (WebSailor) |\n| Web Interface | Gradio | Demo applications with tool integration |\n| API Integration | HTTP clients | Google Search, Jina, DashScope APIs |\n| Training | LLaMA-Factory + verl | SFT and RL training pipelines |\n| Evaluation | ThreadPoolExecutor | Parallel benchmark execution |\n\n**Sources**: Inferred from deployment and architecture documentation\n\n## Naming Conventions and Standards\n\n### Code Entity Naming Patterns\n\n- **Class Names**: PascalCase (e.g. `MultiTurnReactAgent`, `WebShaperExpander`)\n- **Function Names**: snake_case (e.g. `generate_qa_pairs`, `process_trajectory`)\n- **Module Names**: snake_case (e.g. `react_agent.py`, `web_search.py`)\n- **Config Files**: kebab-case (e.g. `model-config.yaml`, `eval-settings.json`)\n\n### Dataset and Model Naming\n\n- **Dataset Names**: CamelCase with descriptive suffix (e.g. `WebShaperQA`, `SailorFog-QA`)\n- **Model Names**: System prefix + size (e.g. `WebDancer-32B`, `WebSailor-7B`)\n- **Benchmark Names**: System prefix + evaluation type (e.g. `WebWalkerQA`, `BrowseComp-en`)\n\n**Sources**: Inferred from project naming patterns and documentation\n\n## External Resources and Links\n\n### Research Papers and Documentation\n- **WebShaper**: Formalization-driven data synthesis methodology\n- **WebDancer**: Native agentic training for autonomous information seeking\n- **WebSailor**: Post-training methodology for enhanced reasoning capabilities\n- **WebWalker**: Benchmark framework for web traversal evaluation\n\n### API Documentation References\n- **Google Search API**: Accessed via Serper service for web search functionality\n- **Jina API**: Content processing and extraction services\n- **DashScope API**: Alibaba Cloud AI services integration\n\n### Training Framework Documentation\n- **LLaMA-Factory**: Supervised fine-tuning framework\n- **verl**: Reinforcement learning training framework\n- **sglang**: High-performance inference serving\n\n**Sources**: Inferred from system integration and external dependencies"])</script><script>self.__next_f.push([1,"2d:T2acb,"])</script><script>self.__next_f.push([1,"# API Reference\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebDancer/demos/tools/private/search.py](WebDancer/demos/tools/private/search.py)\n- [WebSailor/src/scripts/test.sh](WebSailor/src/scripts/test.sh)\n- [WebSailor/src/tool_search.py](WebSailor/src/tool_search.py)\n- [WebSailor/src/tool_visit.py](WebSailor/src/tool_visit.py)\n\n\u003c/details\u003e\n\n\n\nThis page provides comprehensive documentation for the WebAgent API interfaces, covering tool implementations, core abstractions, and configuration options. The API enables programmatic access to web information seeking capabilities through standardized tool interfaces.\n\nFor implementation details of specific components, see [WebSailor](#4), [WebDancer](#3), and [WebWatcher/WebWalker](#5). For setup and deployment instructions, see [Installation and Setup](#6.1).\n\n## Core Tool Architecture\n\nThe WebAgent system implements a unified tool architecture based on the `qwen_agent.tools.base.BaseTool` interface. All tools follow a standardized registration and execution pattern that enables modular integration across different agent implementations.\n\n```mermaid\nclassDiagram\n    class BaseTool {\n        +name: str\n        +description: str\n        +parameters: dict\n        +call(params, **kwargs) str\n        +_verify_json_format_args(params) dict\n    }\n    \n    class Search {\n        +name: \"search\"\n        +description: \"Performs batched web searches\"\n        +parameters: {\"query\": array}\n        +call(params, **kwargs) str\n        +google_search(query) str\n    }\n    \n    class Visit {\n        +name: \"visit\" \n        +description: \"Visit webpage(s) and return summary\"\n        +parameters: {\"url\": string|array, \"goal\": string}\n        +call(params, **kwargs) str\n        +readpage(url, goal) str\n        +jina_readpage(url) str\n        +call_server(msgs, max_tries) str\n    }\n    \n    BaseTool \u003c|-- Search\n    BaseTool \u003c|-- Visit\n    \n    Search --\u003e \"google.serper.dev\" : HTTP POST\n    Visit --\u003e \"r.jina.ai\" : HTTP GET\n    Visit --\u003e \"OpenAI Compatible API\" : LLM Summary\n```\n\n**Tool Registration Flow**\n```mermaid\nsequenceDiagram\n    participant Module as \"Tool Module\"\n    participant Registry as \"@register_tool\"\n    participant Agent as \"Agent System\"\n    participant Tool as \"Tool Instance\"\n    \n    Module-\u003e\u003eRegistry: @register_tool(\"search\", allow_overwrite=True)\n    Registry-\u003e\u003eRegistry: Register Search class\n    Agent-\u003e\u003eRegistry: Load registered tools\n    Registry-\u003e\u003eAgent: Return tool classes\n    Agent-\u003e\u003eTool: Instantiate tool\n    Tool-\u003e\u003eAgent: Ready for execution\n```\n\nSources: [WebSailor/src/tool_search.py:1-30](), [WebSailor/src/tool_visit.py:1-44](), [WebDancer/demos/tools/private/search.py:1-27]()\n\n## Search Tool API\n\nThe `Search` tool provides batched web search capabilities using the Google Serper API. It supports multiple query execution in parallel with comprehensive error handling.\n\n### Interface Specification\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `name` | `str` | `\"search\"` |\n| `description` | `str` | Performs batched web searches with query arrays |\n| `parameters.query` | `array\u003cstring\u003e` | Array of search query strings (required) |\n\n### Method Signatures\n\n```python\ndef call(params: Union[str, dict], **kwargs) -\u003e str\ndef google_search(query: str) -\u003e str\n```\n\n### Request Format\n\n```json\n{\n  \"query\": [\"search term 1\", \"search term 2\", \"search term 3\"]\n}\n```\n\n### Response Format\n\nThe tool returns formatted search results with numbered entries containing title, URL, publication date, source, and snippet information:\n\n```\nA Google search for 'query term' found X results:\n\n## Web Results\n1. [Page Title](https://example.com/page)\nDate published: 2024-01-01\nSource: example.com\nPage snippet content...\n\n2. [Another Page](https://example2.com/page)\nSource: example2.com\nAnother page snippet...\n```\n\n### Configuration Parameters\n\n| Environment Variable | Required | Description |\n|---------------------|----------|-------------|\n| `GOOGLE_SEARCH_KEY` | Yes | API key for Google Serper service |\n| `MAX_MULTIQUERY_NUM` | No | Maximum queries per batch (default: 3) |\n\n### Error Handling\n\nThe Search tool implements multiple retry mechanisms and error responses:\n\n- **Timeout errors**: Returns `\"Google search Timeout, return None, Please try again later.\"`\n- **No results**: Returns `\"No results found for 'query'. Use a less specific query.\"`\n- **API errors**: Raises exceptions with HTTP status codes and error messages\n\nSources: [WebSailor/src/tool_search.py:13-103](), [WebDancer/demos/tools/private/search.py:10-96]()\n\n## Visit Tool API\n\nThe `Visit` tool extracts and summarizes webpage content using the Jina Reader API combined with LLM-based content processing for goal-oriented information extraction.\n\n### Interface Specification\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `name` | `str` | `\"visit\"` |\n| `description` | `str` | Visit webpage(s) and return summary of content |\n| `parameters.url` | `string \\| array\u003cstring\u003e` | URL(s) to visit (required) |\n| `parameters.goal` | `string` | Goal/purpose for visiting pages (required) |\n\n### Method Signatures\n\n```python\ndef call(params: Union[str, dict], **kwargs) -\u003e str\ndef readpage(url: str, goal: str) -\u003e str\ndef jina_readpage(url: str) -\u003e str\ndef call_server(msgs: list, max_tries: int = 10) -\u003e str\n```\n\n### Request Format\n\n```json\n{\n  \"url\": \"https://example.com/page\",\n  \"goal\": \"Find information about product pricing\"\n}\n```\n\nOr for multiple URLs:\n\n```json\n{\n  \"url\": [\"https://example1.com\", \"https://example2.com\"],\n  \"goal\": \"Compare product features across vendors\"\n}\n```\n\n### Content Processing Pipeline\n\n```mermaid\ngraph TD\n    Input[\"URL + Goal\"] --\u003e Jina[\"jina_readpage()\"]\n    Jina --\u003e Content[\"Raw webpage content\"]\n    Content --\u003e Truncate[\"Truncate to WEBCONTENT_MAXLENGTH\"]\n    Truncate --\u003e Extract[\"LLM content extraction\"]\n    Extract --\u003e Parse[\"JSON parsing\"]\n    Parse --\u003e Format[\"Formatted response\"]\n    \n    Extract --\u003e Retry[\"Retry with shorter content\"]\n    Retry --\u003e Extract\n    \n    Parse --\u003e Fallback[\"Fallback text response\"]\n    Fallback --\u003e Format\n    \n    Content --\u003e Error[\"Error handling\"]\n    Error --\u003e Format\n```\n\n### Response Format\n\nSuccessful responses follow this structure:\n\n```\nThe useful information in https://example.com for user goal [goal] as follows:\n\nEvidence in page:\n[Relevant excerpts from the webpage content]\n\nSummary:\n[LLM-generated summary focused on the specified goal]\n```\n\n### Configuration Parameters\n\n| Environment Variable | Required | Description |\n|---------------------|----------|-------------|\n| `JINA_API_KEY` | Yes | API key for Jina Reader service |\n| `WEBCONTENT_MAXLENGTH` | No | Maximum content length (default: 150000) |\n| `IGNORE_JINA` | No | Skip Jina service (default: false) |\n\n### LLM Integration\n\nThe Visit tool integrates with OpenAI-compatible APIs for content summarization:\n\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| Base URL | `http://127.0.0.1:6002/v1` | Local vLLM server endpoint |\n| Model | `/path/qwen2.5-instruct-72b` | Summarization model path |\n| Temperature | `0.7` | Generation randomness |\n| Stop tokens | `[\"\\n\u003ctool_response\u003e\", \"\u003ctool_response\u003e\"]` | Response termination |\n\nSources: [WebSailor/src/tool_visit.py:20-221]()\n\n## Configuration Management\n\nThe WebAgent API relies on environment variables for service configuration and authentication. This enables flexible deployment across different environments without code changes.\n\n### API Keys and Endpoints\n\n```bash\n# Search service configuration\nexport GOOGLE_SEARCH_KEY=\"your_google_search_key\"\nexport SEARCH_API_URL=\"your_search_api_url\"\n\n# Content extraction configuration  \nexport JINA_API_KEY=\"your_jina_api_key\"\n\n# Model deployment configuration\nexport SUMMARY_MODEL_PATH=\"/path/Qwen2.5-72B-Instruct\"\nexport MAX_LENGTH=$((1024 * 31 - 500))\n```\n\n### Content Processing Limits\n\n| Variable | Default | Description |\n|----------|---------|-------------|\n| `WEBCONTENT_MAXLENGTH` | `150000` | Maximum webpage content length |\n| `MAX_MULTIQUERY_NUM` | `3` | Maximum concurrent search queries |\n| `MAX_LENGTH` | `31744` | Model context length limit |\n\n### Service Endpoints\n\n```mermaid\ngraph LR\n    Tools[\"WebAgent Tools\"] --\u003e Search[\"Search Tool\"]\n    Tools --\u003e Visit[\"Visit Tool\"]\n    \n    Search --\u003e Serper[\"google.serper.dev\"]\n    Visit --\u003e Jina[\"r.jina.ai\"]\n    Visit --\u003e LLM[\"127.0.0.1:6002/v1\"]\n    \n    Serper --\u003e Results[\"Search Results\"]\n    Jina --\u003e Content[\"Raw Content\"]\n    LLM --\u003e Summary[\"Content Summary\"]\n```\n\nSources: [WebSailor/src/scripts/test.sh:1-19](), [WebSailor/src/tool_visit.py:12-18](), [WebSailor/src/tool_search.py:9-11]()\n\n## Error Handling and Reliability\n\nThe WebAgent API implements comprehensive error handling and retry mechanisms to ensure reliable operation in production environments.\n\n### Retry Strategies\n\n| Component | Max Retries | Strategy | Timeout |\n|-----------|-------------|----------|---------|\n| Google Search | 5 | Exponential backoff | Request-level |\n| Jina Reader | 3 | Linear retry | 10 seconds |\n| LLM Summarization | 10 | Immediate retry | None |\n| Content Parsing | 3 | Content truncation | None |\n\n### Error Response Formats\n\n**Search Tool Errors:**\n- `\"[Search] Invalid request format: Input must be a JSON object containing 'query' field\"`\n- `\"Google search Timeout, return None, Please try again later.\"`\n- `\"No results found for 'query'. Use a less specific query.\"`\n\n**Visit Tool Errors:**\n- `\"[Visit] Invalid request format: Input must be a JSON object containing 'url' and 'goal' fields\"`\n- `\"[visit] Failed to read page.\"`\n- `\"[visit] Could not generate valid summary after maximum retries\"`\n\n### Fallback Mechanisms\n\nThe Visit tool implements progressive content truncation when LLM summarization fails:\n\n1. Initial content length: `WEBCONTENT_MAXLENGTH`\n2. First retry: 70% of original length\n3. Second retry: 70% of previous length  \n4. Final fallback: 25,000 characters\n\nSources: [WebSailor/src/tool_search.py:46-86](), [WebSailor/src/tool_visit.py:119-221]()\n\n## Usage Examples\n\n### Basic Search Usage\n\n```python\nfrom WebSailor.src.tool_search import Search\n\nsearch_tool = Search()\nresult = search_tool.call({\n    \"query\": [\"artificial intelligence\", \"machine learning trends\"]\n})\n```\n\n### Webpage Content Extraction\n\n```python\nfrom WebSailor.src.tool_visit import Visit\n\nvisit_tool = Visit()\nresult = visit_tool.call({\n    \"url\": \"https://example.com/article\",\n    \"goal\": \"Extract key findings about AI research\"\n})\n```\n\n### Multi-URL Processing\n\n```python\nvisit_tool = Visit()\nresult = visit_tool.call({\n    \"url\": [\n        \"https://site1.com/pricing\", \n        \"https://site2.com/pricing\",\n        \"https://site3.com/pricing\"\n    ],\n    \"goal\": \"Compare pricing information across vendors\"\n})\n```\n\nSources: [WebDancer/demos/tools/private/search.py:98-99](), [WebSailor/src/tool_visit.py:45-67]()"])</script><script>self.__next_f.push([1,"2e:T1adc,"])</script><script>self.__next_f.push([1,"# Assets and Media\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [WebDancer/assets/performance.jpg](WebDancer/assets/performance.jpg)\n- [WebDancer/assets/webdancer.jpg](WebDancer/assets/webdancer.jpg)\n- [WebShaper/assets/case.png](WebShaper/assets/case.png)\n- [WebShaper/assets/formalization.png](WebShaper/assets/formalization.png)\n- [WebShaper/assets/layer_wise.png](WebShaper/assets/layer_wise.png)\n- [WebShaper/assets/tongyi.png](WebShaper/assets/tongyi.png)\n- [assets/tongyi.png](assets/tongyi.png)\n- [assets/webagent-bc.png](assets/webagent-bc.png)\n- [assets/webagent-gaia.png](assets/webagent-gaia.png)\n- [assets/webagent.png](assets/webagent.png)\n\n\u003c/details\u003e\n\n\n\nThis page documents the visual assets, diagrams, images, and media resources used throughout the WebAgent system documentation, research papers, and user interfaces. These assets serve to illustrate system architecture, showcase results, provide branding consistency, and support user understanding of complex AI workflows.\n\nFor implementation details of the systems depicted in these assets, see [Overview](#1). For specific component architectures, see [WebShaper](#2), [WebDancer](#3), and [WebSailor](#4).\n\n## System-Wide Assets\n\nThe WebAgent repository maintains several core visual assets used across documentation and presentations.\n\n### Logos and Branding\n\nThe primary system logo is maintained in `assets/tongyi.png`, representing the Tongyi Lab affiliation and branding standards. This logo appears consistently across all WebAgent components and documentation.\n\n```mermaid\ngraph TB\n    TY[\"assets/tongyi.png\"]\n    WS_TY[\"WebShaper/assets/tongyi.png\"]\n    \n    TY --\u003e |\"Primary logo\"| DOCS[\"Documentation\"]\n    TY --\u003e |\"Branding\"| PAPERS[\"Research Papers\"]\n    WS_TY --\u003e |\"Component branding\"| WS_DOCS[\"WebShaper Documentation\"]\n    \n    subgraph \"Logo Distribution\"\n        TY\n        WS_TY\n    end\n    \n    subgraph \"Usage Contexts\"\n        DOCS\n        PAPERS\n        WS_DOCS\n    end\n```\n\n### System Architecture Visualizations\n\n#### Benchmark Comparison Assets\n\nThe repository contains benchmark visualization assets that demonstrate system performance across different evaluation frameworks:\n\n```mermaid\ngraph LR\n    BC[\"assets/webagent-bc.png\"] --\u003e |\"Shows\"| BC_VIZ[\"BrowseComp Results\"]\n    GAIA[\"assets/webagent-gaia.png\"] --\u003e |\"Shows\"| GAIA_VIZ[\"GAIA Benchmark Results\"]\n    \n    BC_VIZ --\u003e |\"Performance data\"| EVAL[\"Evaluation Reports\"]\n    GAIA_VIZ --\u003e |\"Performance data\"| EVAL\n    \n    subgraph \"Asset Files\"\n        BC\n        GAIA\n    end\n    \n    subgraph \"Content Types\"\n        BC_VIZ\n        GAIA_VIZ\n    end\n```\n\nSources: [assets/webagent-bc.png](), [assets/webagent-gaia.png]()\n\n## WebShaper Component Assets\n\nWebShaper maintains a dedicated assets directory containing specialized diagrams and illustrations for its data synthesis methodology.\n\n### Methodology Diagrams\n\nThe formalization process central to WebShaper's approach is documented through visual assets:\n\n```mermaid\nflowchart TD\n    FORM[\"WebShaper/assets/formalization.png\"] --\u003e |\"Illustrates\"| FORM_PROC[\"Formalization Process\"]\n    LAYER[\"WebShaper/assets/layer_wise.png\"] --\u003e |\"Shows\"| LAYER_ARCH[\"Layer-wise Architecture\"]\n    CASE[\"WebShaper/assets/case.png\"] --\u003e |\"Demonstrates\"| CASE_STUDY[\"Case Study Examples\"]\n    \n    FORM_PROC --\u003e |\"Documents\"| DATA_SYNTH[\"Data Synthesis Pipeline\"]\n    LAYER_ARCH --\u003e |\"Explains\"| MODEL_ARCH[\"Model Architecture\"]\n    CASE_STUDY --\u003e |\"Provides\"| EXAMPLES[\"Concrete Examples\"]\n    \n    subgraph \"WebShaper Assets\"\n        FORM\n        LAYER\n        CASE\n    end\n    \n    subgraph \"Documentation Targets\"\n        DATA_SYNTH\n        MODEL_ARCH\n        EXAMPLES\n    end\n```\n\n#### Formalization Visualization\n\nThe `formalization.png` asset illustrates the core information-seeking task formalization methodology that distinguishes WebShaper's approach to agentic data synthesis.\n\n#### Layer-wise Architecture \n\nThe `layer_wise.png` diagram demonstrates the hierarchical structure and knowledge projection mechanisms within WebShaper's architecture, showing how different layers contribute to the formalization process.\n\n#### Case Study Documentation\n\nThe `case.png` asset provides concrete examples of WebShaper's formalization process applied to real information-seeking scenarios, bridging theoretical concepts with practical applications.\n\nSources: [WebShaper/assets/formalization.png](), [WebShaper/assets/layer_wise.png](), [WebShaper/assets/case.png]()\n\n## WebDancer Component Assets\n\nWebDancer includes visual documentation of its autonomous information seeking capabilities.\n\n```mermaid\ngraph TB\n    WD_IMG[\"WebDancer/assets/webdancer.jpg\"] --\u003e |\"Represents\"| WD_SYSTEM[\"WebDancer System\"]\n    \n    WD_SYSTEM --\u003e |\"Autonomous\"| INFO_SEEK[\"Information Seeking\"]\n    WD_SYSTEM --\u003e |\"Native\"| SEARCH_CAP[\"Search Capabilities\"]\n    WD_SYSTEM --\u003e |\"ReAct-based\"| REASONING[\"Reasoning Framework\"]\n    \n    subgraph \"WebDancer Visualization\"\n        WD_IMG\n    end\n    \n    subgraph \"System Capabilities\"\n        INFO_SEEK\n        SEARCH_CAP\n        REASONING\n    end\n```\n\nThe `webdancer.jpg` asset serves as the primary visual representation of the WebDancer system, used in documentation and presentations to illustrate its autonomous information seeking capabilities.\n\nSources: [WebDancer/assets/webdancer.jpg]()\n\n## Asset File Organization\n\nThe asset structure follows a hierarchical organization pattern:\n\n| Directory | Purpose | File Types | Usage Context |\n|-----------|---------|------------|---------------|\n| `assets/` | System-wide assets | PNG | Main documentation, papers |\n| `WebShaper/assets/` | Component-specific | PNG | WebShaper documentation |\n| `WebDancer/assets/` | Component-specific | JPG | WebDancer documentation |\n\n### File Format Standards\n\n- **PNG Format**: Used for diagrams, logos, and technical illustrations requiring transparency\n- **JPG Format**: Used for photographic content and compressed images\n- **Consistent Naming**: Asset names reflect their content and component association\n\n## Integration with Documentation System\n\nAssets are referenced throughout the codebase documentation using relative paths and are integrated into:\n\n- Research paper figures and illustrations\n- README documentation and component guides  \n- Presentation materials and demos\n- API documentation and user guides\n\nThe asset management follows a distributed approach where each component maintains its specialized visual resources while sharing common branding elements through the root assets directory.\n\nSources: [assets/tongyi.png](), [assets/webagent-bc.png](), [assets/webagent-gaia.png](), [WebShaper/assets/formalization.png](), [WebShaper/assets/layer_wise.png](), [WebShaper/assets/tongyi.png](), [WebShaper/assets/case.png](), [WebDancer/assets/webdancer.jpg]()"])</script><script>self.__next_f.push([1,"5:[\"$\",\"$L12\",null,{\"repoName\":\"Alibaba-NLP/WebAgent\",\"hasConfig\":false,\"children\":[\"$\",\"$L13\",null,{\"wiki\":{\"metadata\":{\"repo_name\":\"Alibaba-NLP/WebAgent\",\"commit_hash\":\"355e62dc\",\"generated_at\":\"2025-08-22T18:29:57.744048\",\"config\":null,\"config_source\":\"none\"},\"pages\":[{\"page_plan\":{\"id\":\"1\",\"title\":\"Overview\"},\"content\":\"$14\"},{\"page_plan\":{\"id\":\"1.1\",\"title\":\"System Architecture\"},\"content\":\"$15\"},{\"page_plan\":{\"id\":\"2\",\"title\":\"WebShaper\"},\"content\":\"$16\"},{\"page_plan\":{\"id\":\"2.1\",\"title\":\"Data Synthesis Pipeline\"},\"content\":\"$17\"},{\"page_plan\":{\"id\":\"2.2\",\"title\":\"Architecture and Implementation\"},\"content\":\"$18\"},{\"page_plan\":{\"id\":\"2.3\",\"title\":\"Formalization Method\"},\"content\":\"$19\"},{\"page_plan\":{\"id\":\"3\",\"title\":\"WebDancer\"},\"content\":\"$1a\"},{\"page_plan\":{\"id\":\"3.1\",\"title\":\"Training Pipeline\"},\"content\":\"$1b\"},{\"page_plan\":{\"id\":\"3.2\",\"title\":\"Architecture and Implementation\"},\"content\":\"$1c\"},{\"page_plan\":{\"id\":\"3.3\",\"title\":\"Demo and Usage\"},\"content\":\"$1d\"},{\"page_plan\":{\"id\":\"3.4\",\"title\":\"Datasets and Trajectories\"},\"content\":\"$1e\"},{\"page_plan\":{\"id\":\"4\",\"title\":\"WebSailor\"},\"content\":\"$1f\"},{\"page_plan\":{\"id\":\"4.1\",\"title\":\"Training Pipeline\"},\"content\":\"$20\"},{\"page_plan\":{\"id\":\"4.2\",\"title\":\"Architecture and Implementation\"},\"content\":\"$21\"},{\"page_plan\":{\"id\":\"4.3\",\"title\":\"Tools and Components\"},\"content\":\"$22\"},{\"page_plan\":{\"id\":\"5\",\"title\":\"WebWatcher and WebWalker\"},\"content\":\"$23\"},{\"page_plan\":{\"id\":\"5.1\",\"title\":\"WebWatcher Research Agent\"},\"content\":\"$24\"},{\"page_plan\":{\"id\":\"5.2\",\"title\":\"WebWalker Benchmark Framework\"},\"content\":\"$25\"},{\"page_plan\":{\"id\":\"6\",\"title\":\"Getting Started\"},\"content\":\"$26\"},{\"page_plan\":{\"id\":\"6.1\",\"title\":\"Installation and Setup\"},\"content\":\"$27\"},{\"page_plan\":{\"id\":\"6.2\",\"title\":\"Usage Examples\"},\"content\":\"$28\"},{\"page_plan\":{\"id\":\"7\",\"title\":\"Evaluation and Performance\"},\"content\":\"$29\"},{\"page_plan\":{\"id\":\"7.1\",\"title\":\"Benchmark Results\"},\"content\":\"$2a\"},{\"page_plan\":{\"id\":\"7.2\",\"title\":\"Performance Analysis\"},\"content\":\"$2b\"},{\"page_plan\":{\"id\":\"8\",\"title\":\"Reference\"},\"content\":\"$2c\"},{\"page_plan\":{\"id\":\"8.1\",\"title\":\"API Reference\"},\"content\":\"$2d\"},{\"page_plan\":{\"id\":\"8.2\",\"title\":\"Assets and Media\"},\"content\":\"$2e\"}]},\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]\n"])</script><script>self.__next_f.push([1,"c:null\n10:[[\"$\",\"title\",\"0\",{\"children\":\"WebWatcher Research Agent | Alibaba-NLP/WebAgent | DeepWiki\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"This document covers WebWatcher, the vision-language deep research agent component of the WebAgent system. WebWatcher is designed to provide advanced research capabilities that leverage both visual an\"}],[\"$\",\"meta\",\"2\",{\"property\":\"og:title\",\"content\":\"WebWatcher Research Agent | Alibaba-NLP/WebAgent | DeepWiki\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:description\",\"content\":\"This document covers WebWatcher, the vision-language deep research agent component of the WebAgent system. WebWatcher is designed to provide advanced research capabilities that leverage both visual an\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:url\",\"content\":\"https://deepwiki.com/Alibaba-NLP/WebAgent/5.1-webwatcher-research-agent\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:site_name\",\"content\":\"DeepWiki\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"7\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:title\",\"content\":\"WebWatcher Research Agent | Alibaba-NLP/WebAgent | DeepWiki\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:description\",\"content\":\"This document covers WebWatcher, the vision-language deep research agent component of the WebAgent system. WebWatcher is designed to provide advanced research capabilities that leverage both visual an\"}],[\"$\",\"link\",\"10\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"48x48\"}],[\"$\",\"link\",\"11\",{\"rel\":\"icon\",\"href\":\"/icon.png?66aaf51e0e68c818\",\"type\":\"image/png\",\"sizes\":\"16x16\"}],[\"$\",\"link\",\"12\",{\"rel\":\"apple-touch-icon\",\"href\":\"/apple-icon.png?a4f658907db0ab87\",\"type\":\"image/png\",\"sizes\":\"180x180\"}]]\n"])</script></body></html>